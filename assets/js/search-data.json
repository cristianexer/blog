{
  
    
        "post0": {
            "title": "My CUDA installing instructions for Windows",
            "content": "In this notebook I will show how I have managed to get CUDA working on my device ( you might need to download different version of the packages based on the GPU you have) . I already had the latest version of Anaconda installed, so I am not going to go through that, but if you are installing it make sure you check the option to add Anaconda to the PATH environment variable and run the conda init into your terminal with the right option accorindg to your type of terminal [bash, powershell, command prompt] . CUDA . Before installing CUDA is highly recommended/essential to install Visual Studio Community Edition (Not Visual Studio Code). It is not required to install any other additional workload/packages if you are not planning to use Visual Studio as your main IDE. . My GPU is an Nvidia RTX 3090 and this enabled me to install the latest version of CUDA toolkit 11.3, however, as mentioned before, you need to check the architecture your GPU is based on and download the CUDA toolkit version acording to that. You can find here in the CUDA Toolkit Archive all the CUDA relsease . After installing CUDA, we need to install cuDNN . NVIDIA CUDA Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. It provides highly tuned implementations of routines arising frequently in DNN applications. . Annoyingly, you have to create an Nvidia account if you want to download it from the official website Moving forward, after the cuDNN is downloaded, we can extract the files and folders from the archive so that we can have a folder called tools and add all the extracted files in there so the bin folder would have the next path . C: tools cuda bin . Then, we have to add this C: tools cuda bin to the System PATH environment variable in windows. . Follow this steps if you have not added the path to the bin folder to the environment. . Hit windows Key | Search for Environment variables then click Environment Variables on the window that have openend | In the System Variables find the PATH variable and Hit Edit | Then hit new in the new window that have openend and paste the path to the bin folder C: tools cuda bin | . Now you will have to reboot your PC, and hopefully all going to work just fine . Tensorflow . Now we can create a new environment for tensorflow and pip install tensorflow . conda create --name tf2 python=3.8 conda activate tf2 pip install tf-nightly-gpu . Then for the testing purpose . import tensorflow as tf . INFO:tensorflow:Enabling eager execution INFO:tensorflow:Enabling v2 tensorshape INFO:tensorflow:Enabling resource variables INFO:tensorflow:Enabling tensor equality INFO:tensorflow:Enabling control flow v2 . print(tf.test.is_built_with_cuda()) . True . assert tf.test.is_built_with_cuda() . with tf.device(&#39;/GPU:0&#39;): a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name=&#39;a&#39;) b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name=&#39;b&#39;) c = tf.matmul(a, b) c . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[22., 28.], [49., 64.]], dtype=float32)&gt; . sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) . Device mapping: /job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0b:00.0, compute capability: 8.6 . tf.test.gpu_device_name() . &#39;/device:GPU:0&#39; . Pytorch . conda create --name ptorch python=3.8 conda activate ptorch conda install pytorch -c conda-forge -c pytorch . Again let&#39;s test it . import torch . torch.cuda.is_available() . True . torch.cuda.get_device_name(device=None) . &#39;NVIDIA GeForce RTX 3090&#39; . torch.cuda.current_device() . 0 . torch.cuda.device_count() . 1 .",
            "url": "https://cristianexer.github.io/blog/2021/04/17/CUDA-Install-Commands.html",
            "relUrl": "/2021/04/17/CUDA-Install-Commands.html",
            "date": " • Apr 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Facebook 2019 533M Leaked Data",
            "content": "import os import pandas as pd import numpy as np from IPython.core.display import display, HTML import plotly.graph_objects as go from plotly.subplots import make_subplots import matplotlib.pyplot as plt import seaborn as sns import pycountry plt.style.use(&#39;seaborn&#39;) . path_to_data = &#39;../../Downloads/Facebook Leak [2019][533M Records][106 Countries]&#39; . USA . usa = pd.read_feather(os.path.join(path_to_data,&#39;USA/usa.feather&#39;)) usa.columns . Index([&#39;phone_no&#39;, &#39;user_id&#39;, &#39;fname&#39;, &#39;lname&#39;, &#39;gender&#39;, &#39;city1&#39;, &#39;city2&#39;, &#39;relationship&#39;, &#39;company&#39;, &#39;seen&#39;, &#39;email&#39;, &#39;unknown&#39;], dtype=&#39;object&#39;) . get all the states from US . states = [x.name for x in pycountry.subdivisions.lookup(&#39;US&#39;)] state2code = {x.name:x.code.split(&#39;-&#39;)[-1] for x in pycountry.subdivisions.lookup(&#39;US&#39;)} . loop through states and if we find a state in the given address then return that state . def addr_search(addr,lst): for c in lst: if addr != None and c in addr: return c return addr . next we will use the previous function and the state2code dictionary to get the state and state code form the address we have . usa[&#39;state_city1&#39;] = usa.city1.apply(lambda x: addr_search(x,states)) usa[&#39;state_city1_code&#39;] = usa[&#39;state_city1&#39;].apply(lambda x: state2code.get(x,np.nan)) usa[&#39;state_city2&#39;] = usa.city2.apply(lambda x: addr_search(x,states)) usa[&#39;state_city2_code&#39;] = usa[&#39;state_city2&#39;].apply(lambda x: state2code.get(x,np.nan)) . then we want to see where is the largest amount of leaked accounts by state . city_1_summary = usa[&#39;state_city1_code&#39;].value_counts().div(len(usa)) city_2_summary = usa[&#39;state_city2_code&#39;].value_counts().div(len(usa)) . now we can make some nice plots.. . Percentage of leaked accounts by relationship . usa.relationship.value_counts().plot.pie(autopct=&#39;%1.0f%%&#39;,title=&#39;Percentage of leaked accounts by relationship&#39;,figsize=(8,8),explode=(0.1,0.1,0.1,0,0,0,0,0,0,0,0)).legend(bbox_to_anchor=(1.3,-0.12),ncol=3); . Top 20 Companies by leaked accounts . usa.company.value_counts()[:20].sort_values().plot.barh().legend(bbox_to_anchor=(1,1)); . Percentage of missing values in email addresses . usa.email.isnull().value_counts().rename({True:&#39;Missing&#39;,False:&#39;Found&#39;}).plot.pie(autopct=&#39;%1.0f%%&#39;,title=&#39;Percentage of missing email addresses&#39;); plt.ylabel(&#39;&#39;); . Percentage of missing values in phone number . usa.phone_no.isnull().value_counts().rename({True:&#39;Missing&#39;,False:&#39;Found&#39;}).plot.pie(autopct=&#39;%1.0f%%&#39;,title=&#39;Percentage of missing phone number&#39;); plt.ylabel(&#39;&#39;); . UK . uk = pd.read_feather(os.path.join(path_to_data,&#39;uk/uk.feather&#39;)) uk.columns . Index([&#39;phone_no&#39;, &#39;user_id&#39;, &#39;fname&#39;, &#39;lname&#39;, &#39;gender&#39;, &#39;city1&#39;, &#39;city2&#39;, &#39;relationship&#39;, &#39;company&#39;, &#39;seen&#39;, &#39;email&#39;, &#39;unknown&#39;], dtype=&#39;object&#39;) . def addr_search_uk(addr,uk_loc_dict): for k,v in uk_loc_dict.items(): if addr != None and k in addr: return k return np.nan . uk_loc = pd.read_html(&#39;https://simple.wikipedia.org/wiki/ISO_3166-2:GB&#39;)[0] uk_loc[&#39;Location&#39;] = uk_loc[&#39;Location&#39;].str.split(&#39;(&#39;,expand=True)[0].str.strip() uk_loc = uk_loc.set_index(&#39;Location&#39;)[&#39;Code&#39;].to_dict() . uk[&#39;county_city1&#39;] = uk.city1.apply(lambda x: addr_search_uk(x,uk_loc)) uk[&#39;county_city2&#39;] = uk.city2.apply(lambda x: addr_search_uk(x,uk_loc)) . uk_city1_summary = uk[&#39;county_city1&#39;].value_counts().div(len(uk)).mul(100) uk_city2_summary = uk[&#39;county_city2&#39;].value_counts().div(len(uk)).mul(100) . Percentage of accounts leaked by cities and counties . top_n = 30 fig,ax = plt.subplots(1,2,figsize=(10,10)) uk_city1_summary.sort_values()[:top_n].sort_values().plot.barh(ax=ax[0],title=&#39;City 1&#39;) uk_city2_summary.sort_values()[:top_n].sort_values().plot.barh(ax=ax[1],title=&#39;City 2&#39;) plt.tight_layout() . Percentage of leaked accounts by relationship . uk.relationship.value_counts().plot.pie(autopct=&#39;%1.0f%%&#39;,title=&#39;Percentage of leaked accounts by relationship&#39;,figsize=(8,8),explode=(0.1,0.1,0.1,0,0,0,0,0,0,0,0)).legend(bbox_to_anchor=(1.3,-0.12),ncol=3); plt.ylabel(&#39;&#39;); . Top 20 Companies by leaked accounts . uk.company.value_counts()[:20].sort_values().plot.barh().legend(bbox_to_anchor=(1,1)); . Percentage of missing values in email addresses . uk.email.isnull().value_counts().rename({True:&#39;Missing&#39;,False:&#39;Found&#39;}).plot.pie(autopct=&#39;%1.0f%%&#39;,title=&#39;Percentage of missing email addresses&#39;); plt.ylabel(&#39;&#39;); . Percentage of missing values in phone number . uk.phone_no.isnull().value_counts().rename({True:&#39;Missing&#39;,False:&#39;Found&#39;}).plot.pie(autopct=&#39;%1.0f%%&#39;,title=&#39;Percentage of missing phone number&#39;); plt.ylabel(&#39;&#39;); . Conclusions . Not many email addresses found in the data, however there is no missing phone number | Using the user_id we can easily access the user profile https://facebook.com/{user_id} | .",
            "url": "https://cristianexer.github.io/blog/2021/04/05/FB-Leaked-Data.html",
            "relUrl": "/2021/04/05/FB-Leaked-Data.html",
            "date": " • Apr 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Endless Quests",
            "content": ". In this notebook we will try to use the new GPT-Neo to generate endless quests for a game using dummy data . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . First, let&#39;s get the model loaded and see how it works . try: from transformers import pipeline, set_seed except ModuleNotFoundError: !pip install git+https://github.com/huggingface/transformers.git from transformers import pipeline, set_seed . GPT-2 . GPT-2 . GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. . More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens. . This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt. . generator = pipeline(&#39;text-generation&#39;, model=&#39;gpt2&#39;) . . set_seed(42) . text = generator(&quot;I would like to have some pasta for dinner&quot;, min_length=200)[0][&#39;generated_text&#39;] print(text) . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . I would like to have some pasta for dinner today.  I think it was our night before, right?  We were in the middle of watching the Super Bowl  before my friend went into a bad mood.  She left the . GPT-Neo . EleutherAI/gpt-neo-1.3B . GPT-Neo 1.3B is a transformer model designed using EleutherAI&#39;s replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model. . neo = pipeline(&#39;text-generation&#39;, model=&#39;EleutherAI/gpt-neo-1.3B&#39;) . . set_seed(42) . text = neo(&quot;I would like to have some pasta for dinner&quot;,do_sample=True, min_length=100)[0][&#39;generated_text&#39;] print(text) . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . I would like to have some pasta for dinner next night, and I was wondering what you guys think about adding some mushrooms as well. I would rather have the mushroom and the pasta together so I am thinking it sounds like a good combination. I have . Let&#39;s try to generate quests using the Pokemon dataset . . You can get the dataset from here . pokemons = pd.read_csv(&#39;https://gist.githubusercontent.com/armgilles/194bcff35001e7eb53a2a8b441e8b2c6/raw/92200bc0a673d5ce2110aaad4544ed6c4010f687/pokemon.csv&#39;) pokemons[:3] . # Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary . 0 1 | Bulbasaur | Grass | Poison | 318 | 45 | 49 | 49 | 65 | 65 | 45 | 1 | False | . 1 2 | Ivysaur | Grass | Poison | 405 | 60 | 62 | 63 | 80 | 80 | 60 | 1 | False | . 2 3 | Venusaur | Grass | Poison | 525 | 80 | 82 | 83 | 100 | 100 | 80 | 1 | False | . sample = neo(&quot;Bulbasaur&quot;,do_sample=True, min_length=100,pad_token_id=50256)[0][&#39;generated_text&#39;] print(sample) . Bulbasaur&#39;s popularity as a playable character in both the Nintendo 3DS and Wii U versions of the game was boosted with the appearance of a new, fully colored, 3DS XL version, in which the blue/gray color scheme returns with a . def generate_quest(model,pokemon,min_length=100): quest_body = np.random.choice([ &#39;{pokemon} has been spawned in {location} go and {action}&#39;, &#39;Prepare yourself for going to {location} and {action} {pokemon}&#39;, &#39;{action} few {pokemon}, you can usually find them in the {location} area&#39; ]) elements = { &#39;location&#39; : np.random.choice([&#39;kyoto&#39;,&#39;asakusa&#39;,&#39;tokyo&#39;,&#39;hiroshima&#39;,&#39;nagasaki&#39;]), &#39;action&#39; : np.random.choice([&#39;catch&#39;,&#39;kill&#39;,&#39;save&#39;]), &#39;pokemon&#39;: pokemon } random_quest_body = quest_body.format(**elements) return model(random_quest_body, do_sample=True,min_length=min_length,pad_token_id=50256)[0][&#39;generated_text&#39;] . Let&#39;s pick up a random pokemon name and generate a random quest . rand_pokemon = pokemons.sample(1)[&#39;Name&#39;].values[0] print(&#39;Pokemon:&#39;,rand_pokemon,&#39; n&#39;) generated_quest = generate_quest(model=neo,pokemon=rand_pokemon,min_length=200) print(generated_quest) . Pokemon: Hydreigon Hydreigon has been spawned in asakusa go and catch a look at its official website www.yoshizuma-hikari-haikai-haikai.co.jp or you could click here and look at the . for _ in range(10): rand_pokemon = pokemons.sample(1)[&#39;Name&#39;].values[0] print(&#39;&#39;*10) print(&#39;Pokemon:&#39;,rand_pokemon,&#39; n&#39;) generated_quest = generate_quest(model=neo,pokemon=rand_pokemon,min_length=200) print(generated_quest) print(&#39;&#39;*10,&#39; n&#39;) . Pokemon: Magmar catch few Magmar, you can usually find them in the nagasaki area of the west end of the town. you can check the map of the nagasaki area to get there. Downtown Nagasaki: The town Pokemon: Bibarel Bibarel has been spawned in kyoto go and kill the other and this is a very good reason. It is important to note that bibarel originated in the ube state and there are no longer in ube, only Pokemon: Corsola Corsola has been spawned in hiroshima go and catch up with the newest news. The new news of 2019 was officially announced by the group&#39;s leader Shigeo Takahashi by way of Twitter. The members of Corsola made Pokemon: Sealeo Sealeo has been spawned in kyoto go and catch up on what we could do about tatata kyoto lol. So with all the hype about the war of the yuji ken and the kenm Pokemon: Palpitoad Prepare yourself for going to nagasaki and kill Palpitoad. This might seem boring, but it will be something you&#39;ll remember for a long time. This is a game by the creators of Tomb Raider and it&#39;s only the last I Pokemon: Swellow save few Swellow, you can usually find them in the nagasaki area of Tokyo, the most popular restaurant here with their dishes are the sushi, which is amazing, there are also many other restaurants that serve food and offer different dishes including some Pokemon: DarmanitanZen Mode kill few DarmanitanZen Mode, you can usually find them in the kyoto area - In L.2 you can find &#34;Toxic Shaft&#34; and &#34;Toxic Wind&#34; along with the main DarmanitanZen Mode, Pokemon: Octillery kill few Octillery, you can usually find them in the tokyo area. I always went to the main shrine to get some food and water, but every time I went there, the shrine was closed. I think she probably closed the shrine Pokemon: Mandibuzz Mandibuzz has been spawned in tokyo go and save the world with this wonderful, and slightly addictive platforming platform game. After going to prison and getting sentenced to death, a young man called Mitsuyoshi Nagase comes home Pokemon: Slakoth Prepare yourself for going to asakusa and save Slakoth and his crew. The final act starts here for me… You can save those of you who want to. I will say this: your loss and my gain . Conclusions . Those GPT models are awesome | The Neo version takes way more time than version 2 | I think this might be a nice way to get inspiration for quest creation after more iterations and improvements in the quest body | .",
            "url": "https://cristianexer.github.io/blog/2021/04/01/Endless-Quests.html",
            "relUrl": "/2021/04/01/Endless-Quests.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Probability Distributions",
            "content": ". Project . Aim: Get familiar with probability distibutions and how they work . Objectives: . Get Familiar with common probability distributions Bernoulli Distribution | Uniform Distribution | Binomial Distribution | Normal Distribution | Gamma Distribution | Poisson Distribution | Exponential Distribution | Tweedie Distribution | . | Highlight the main differences between Tweedie Distribution based model and the traditinal Frequency-Severity models | . Imports . import pandas as pd import numpy as np import scipy.stats as ss import matplotlib.pyplot as plt import seaborn as sns sns.set_style(&#39;whitegrid&#39;) . Distributions . A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values . Bernoulli Distribution . $f(k;p) =pk + (1-p)(1-k)$ . $p$ = probability | $k$ = possible outcomes | $f$ = probability mass function | . A Bernoulli event is one for which the probability the event occurs is p and the probability the event does not occur is $1-p$; i.e., the event is has two possible outcomes (usually viewed as success or failure) occurring with probability $p$ and $1-p$, respectively. A Bernoulli trial is an instantiation of a Bernoulli event. So long as the probability of success or failure remains the same from trial to trial (i.e., each trial is independent of the others), a sequence of Bernoulli trials is called a Bernoulli process. Among other conclusions that could be reached, this means that for n trials, the probability of n successes is $p^n$. . A Bernoulli distribution is the pair of probabilities of a Bernoulli event . random_bernoulli = ss.bernoulli(p=0.5).rvs(size=10) random_bernoulli.mean() . 0.6 . plt.hist(random_bernoulli,label=&#39;Original Population&#39;,alpha=.7); plt.hist(ss.bernoulli(p=random_bernoulli.mean()).rvs(size=10),label=&#39;Random Samples with Bernoulli&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . Binomial Distribution . $P(k;n) = frac{n!}{k!(n-k)!} p^k(1-p)^{(n-k)}$ . The binomial distribution is a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions. The underlying assumptions of the binomial distribution are that there is only one outcome for each trial, that each trial has the same probability of success, and that each trial is mutually exclusive, or independent of each other. . random_binomial = np.random.binomial(1,p=.5,size=10) random_binomial.mean() . 0.4 . plt.hist(random_binomial,label=&#39;Original Population&#39;,alpha=.7); plt.hist(np.random.binomial(1,p=random_binomial.mean(),size=10),label=&#39;Random Samples with Binomial&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . What is the difference between Bernoulli Distribution and Binomial Distribution . The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n. For example, if a manufactured item is defective with probability p, then the binomial distribution represents the number of successes and failures in a lot of n items. In particular, sampling from this distribution gives a count of the number of defective items in a sample lot. . Poisson Distribution . Let&#39;s review Probability Mass Function(PMF) . In probability and statistics, a probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete density function. . A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability. . The Poisson distribution describes the probability to find exactly x events in a given length of time if the events occur independently at a constant rate. . In addition, the Poisson distribution can be obtained as an approximation of a binomial distribution when the number of trials $n$ of the latter distribution is large, success probability $p$ is small, and $np$ is a finite number. . The PMF of the Poisson distribution is given by $P(X = x) = frac{e^{-λ}λ^x}{x!}, x=0,1,... infty$ where λ is a positive number. . Both the mean and variance of the Poisson distribution are equal to λ. . The maximum likelihood estimate of λ from a sample from the Poisson distribution is the sample mean. . x = np.random.randint(5,100,size=100) x.mean() . 57.03 . random_poisson = np.random.poisson(x,size=100) random_poisson.mean() . 57.72 . plt.hist(x,label=&#39;Original Population&#39;,alpha=.7); plt.hist(random_poisson,label=&#39;Random Samples with Poisson&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . Uniform Distribution . Before this let&#39;s review the Probability Density Function(PDF) . We capture the notion of being close to a number with a probability density function which is often denoted by $ρ(x)$ . While the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would equal one sample compared to the other sample. . The probability density function is $p(x) = frac{1}{b - a}$ . . In statistics, a type of probability distribution in which all outcomes are equally likely. A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same. . This distribution is defined by two parameters, $a$ and $b$: . $a$ is the minimum. | $b$ is the maximum. | . low = 5 # min - also known as loc in the context of uniform distribution high = 10 # max - also know as scale in the context of uniform distribution random_uniform = np.random.uniform(low,high,size=100) . plt.hist(random_uniform,label=&#39;Original Population&#39;,alpha=.7); plt.hist(np.random.uniform(random_uniform.min(),random_uniform.max(),size=100),label=&#39;Random Samples with Uniform&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . Normal Distribution . $f(x)= { frac{1}{ sigma sqrt{2 pi}}}e^{- { frac {1}{2}} ( frac {x- mu}{ sigma})^2}$ . Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve. . The normal distribution model is motivated by the Central Limit Theorem. This theory states that averages calculated from independent, identically distributed random variables have approximately normal distributions, regardless of the type of distribution from which the variables are sampled (provided it has finite variance). Normal distribution is sometimes confused with symmetrical distribution. . The normal distribution has two parameters: . $mu$ - mean | $sigma$ - standard deviation | . random_vals = np.random.randint(1,100,size=10) random_vals . array([40, 69, 21, 58, 60, 85, 95, 93, 78, 91]) . mu = random_vals.mean() sigma = random_vals.std() random_normal = np.random.normal(mu,sigma,size=10) . plt.hist(random_vals,label=&#39;Original Population&#39;,alpha=.7); plt.hist(random_normal,label=&#39;Random Samples with Normal&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . Gamma Distribution . where $G(a)$ is the Gamma function, and the parameters a and b are both positive, i.e. $a &gt; 0$ and $b$ &gt; 0$ . $a$ (alpha) is known as the shape parameter, while $b$ (beta) is referred to as the scale parameter. | $b$ has the effect of stretching or compressing the range of the Gamma distribution. | . A Gamma distribution with $b = 1$ is known as the standard Gamma distribution. . The Gamma distribution represents a family of shapes. As suggested by its name, a controls the shape of the family of distributions. The fundamental shapes are characterized by the following values of a: . Case I (a &lt; 1) . When a &lt; 1, the Gamma distribution is exponentially shaped and asymptotic to both the vertical and horizontal axes. | . | Case II (a = 1) . A Gamma distribution with shape parameter a = 1 and scale parameter b is the same as an exponential distribution of scale parameter (or mean) b. | . | Case III (a &gt; 1) . When a is greater than one, the Gamma distribution assumes a mounded (unimodal), but skewed shape. The skewness reduces as the value of a increases. | . | . The Gamma distribution is sometimes called the Erlang distribution, when its shape parameter a is an integer. . x = np.random.gamma(500,size=100) m = x.mean() v = x.var() m,v . (497.73306638358423, 506.7061887509693) . a = m**2/v # calculate alpha parameter - also known as shape b = m/v # calculate beta parameter - also known as scale . random_gamma = np.random.gamma(a,1/b,size=100) random_gamma.mean() . 497.19854925829094 . plt.hist(x,label=&#39;Original Population&#39;,alpha=.7); plt.hist(random_gamma,label=&#39;Random Samples with Gamma&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . Exponential Distribution . $ f(x; frac{1}{ lambda}) = frac{1}{ lambda} exp(- frac{x}{ lambda})$ . The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete. . Some Applications: . How much time will elapse before an earthquake occurs in a given region? | How long do we need to wait until a customer enters our shop? | How long will it take before a call center receives the next phone call? | How long will a piece of machinery work without breaking down? | . x = np.random.exponential(size=100) x.mean() . 0.9534300085400815 . random_exponential = np.random.exponential(x.mean(),size=100) random_exponential.mean() . 0.8035637714108852 . plt.hist(x,label=&#39;Original Population&#39;,alpha=.7); plt.hist(random_exponential,label=&#39;Random Samples with Exponential&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=2); . What is the relationship between the Poisson Distribution and Exponential Distribution . A classical example of a random variable having a Poisson distribution is the number of phone calls received by a call center. If the time elapsed between two successive phone calls has an exponential distribution and it is independent of the time of arrival of the previous calls, then the total number of calls received in one hour has a Poisson distribution. . . The concept is illustrated by the plot above, where the number of phone calls received is plotted as a function of time. The graph of the function makes an upward jump each time a phone call arrives. The time elapsed between two successive phone calls is equal to the length of each horizontal segment and it has an exponential distribution. The number of calls received in 60 minutes is equal to the length of the segment highlighted by the vertical curly brace and it has a Poisson distribution. . The Poisson distribution deals with the number of occurrences in a fixed period of time, and the exponential distribution deals with the time between occurrences of successive events as time flows by continuously. . Tweedie Distribution . The Tweedie distribution is a special case of an exponential distribution. . The probability density function cannot be evaluated directly. Instead special algorithms must be created to calculate the density. . This family of distributions has the following characteristics: . a mean of $E(Y) = μ$ | a variance of $Var(Y) = φ μp$. | . The $p$ in the variance function is an additional shape parameter for the distribution. $p$ is sometimes written in terms of the shape parameter $α: p = (α – 2) / (α -1)$ . Some familiar distributions are special cases of the Tweedie distribution: . $p = 0$ : Normal distribution | $p = 1$: Poisson distribution | $1 &lt; p &lt; 2$: Compound Poisson/gamma distribution | $p = 2$ gamma distribution | $2 &lt; p$ &lt; 3 Positive stable distributions | $p = 3$: Inverse Gaussian distribution / Wald distribution | $p &gt; 3$: Positive stable distributions | $p = ∞$ Extreme stable distributions | . Some Applications: . modeling claims in the insurance industry | medical/genomic testing | anywhere else there is a mixture of zeros and non-negative data points. | . try: import tweedie as tw except ImportError: !pip install tweedie import tweedie as tw . x = np.random.gamma(1e3,size=100) * np.random.binomial(1,p=.3,size=100) . plt.hist(x) plt.title(&#39;Original Population&#39;); . mu = x.mean() sigma = x.std() mu,sigma . (170.5586714756651, 377.0265213604315) . random_tweedie_poisson = tw.tweedie(p=1.01,mu=mu,phi=sigma).rvs(100) random_tweedie_compound = tw.tweedie(p=1.2,mu=mu,phi=sigma).rvs(100) random_tweedie_gamma = tw.tweedie(p=1.79,mu=mu,phi=sigma).rvs(100) . random_tweedie_poisson.mean() . 190.23724143818134 . random_tweedie_compound.mean() . 187.05772479414344 . random_tweedie_gamma.mean() . 76.04517167156773 . plt.hist(x,label=&#39;Original Population&#39;,alpha=.7); plt.hist(random_tweedie_poisson,label=&#39;Random Samples with Tweedie Poisson&#39;,alpha=.7); plt.hist(random_tweedie_compound,label=&#39;Random Samples with Tweedie Compund&#39;,alpha=.7); plt.hist(random_tweedie_gamma,label=&#39;Random Samples with Tweedie Gamma&#39;,alpha=.7); plt.xlabel(&#39;Value&#39;) plt.ylabel(&#39;Frequency&#39;); plt.legend(bbox_to_anchor=(1, -0.128),ncol=1); . Tweedie vs Frequency-Severity . Auto Claims Data . . Dataset . VARIABLE NAME DEFINITION THEORETICAL EFFECT . INDEX | Identification Variable (do not use) | None | . TARGET FLAG | Was Car in a crash? 1=YES 0=NO | None | . TARGET AMT | If car was in a crash, what was the cost | None | . AGE | Age of Driver | Very young people tend to be risky. Maybe very old people also. | . BLUEBOOK | Value of Vehicle | Unknown effect on probability of collision, but probably effect the payout if there is a crash | . CAR AGE | Vehicle Age | Unknown effect on probability of collision, but probably effect the payout if there is a crash | . CAR TYPE | Type of Car | Unknown effect on probability of collision, but probably effect the payout if there is a crash | . CAR USE | Vehicle Use | Commercial vehicles are driven more, so might increase probability of collision | . CLM FREQ | # Claims (Past 5 Years) | The more claims you filed in the past, the more you are likely to file in the future | . EDUCATION | Max Education Level | Unknown effect, but in theory more educated people tend to drive more safely | . HOMEKIDS | # Children at Home | Unknown effect | . HOME VAL | Home Value | In theory, home owners tend to drive more responsibly | . INCOME | Income | In theory, rich people tend to get into fewer crashes | . JOB | Job Category | In theory, white collar jobs tend to be safer | . KIDSDRIV | # Driving Children | When teenagers drive your car, you are more likely to get into crashes | . MSTATUS | Marital Status | In theory, married people drive more safely | . MVR PTS | Motor Vehicle Record Points | If you get lots of traffic tickets, you tend to get into more crashes | . OLDCLAIM | Total Claims (Past 5 Years) | If your total payout over the past five years was high, this suggests future payouts will be high | . PARENT1 | Single Parent | Unknown effect | . RED CAR | A Red Car | Urban legend says that red cars (especially red sports cars) are more risky. Is that true? | . REVOKED | License Revoked (Past 7 Years) | If your license was revoked in the past 7 years, you probably are a more risky driver. | . SEX | Gender | Urban legend says that women have less crashes then men. Is that true? | . TIF | Time in Force | People who have been customers for a long time are usually more safe. | . TRAVTIME | Distance to Work | Long drives to work usually suggest greater risk | . URBANICITY | Home/Work Area | Unknown | . YOJ | Years on Job | People who stay at a job for a long time are usually more safe | . Load Data and Clean Data . df = pd.read_csv(&#39;/content/drive/MyDrive/Datasets/car_insurance_claim.csv&#39;) # make columns lowercase df.columns = df.columns.str.lower() # drop useless columns df = df.drop([&#39;kidsdriv&#39;,&#39;parent1&#39;,&#39;revoked&#39;,&#39;mvr_pts&#39;,&#39;travtime&#39;,&#39;id&#39;,&#39;birth&#39;],axis=1) # clean money amounts df[[&#39;home_val&#39;,&#39;bluebook&#39;,&#39;oldclaim&#39;,&#39;clm_amt&#39;,&#39;income&#39;]] = df[[&#39;home_val&#39;,&#39;bluebook&#39;,&#39;oldclaim&#39;,&#39;clm_amt&#39;,&#39;income&#39;]].apply(lambda x: x.str.replace(&#39;$&#39;,&#39;&#39;).str.replace(&#39;,&#39;,&#39;&#39;)).astype(float) # clean values from columns to_clean = [&#39;education&#39;,&#39;occupation&#39;,&#39;mstatus&#39;,&#39;gender&#39;,&#39;car_type&#39;] for col in to_clean: df[col] = df[col].str.replace(&#39;z_&#39;,&#39;&#39;).str.replace(&#39;&lt;&#39;,&#39;&#39;) df[&#39;urbanicity&#39;] = df[&#39;urbanicity&#39;].str.split(&#39;/ &#39;,expand=True)[1] to_clean = [&#39;mstatus&#39;,&#39;red_car&#39;] for col in to_clean: df[col] = df[col].str.lower().replace({ &#39;yes&#39;: True, &#39;no&#39;: False}).astype(int) df = df.drop([&#39;car_age&#39;,&#39;occupation&#39;,&#39;home_val&#39;,&#39;income&#39;,&#39;yoj&#39;],axis=1).dropna() df[:3] . age homekids mstatus gender education car_use bluebook tif car_type red_car oldclaim clm_freq clm_amt claim_flag urbanicity . 0 60.0 | 0 | 0 | M | PhD | Private | 14230.0 | 11 | Minivan | 1 | 4461.0 | 2 | 0.0 | 0 | Urban | . 1 43.0 | 0 | 0 | M | High School | Commercial | 14940.0 | 1 | Minivan | 1 | 0.0 | 0 | 0.0 | 0 | Urban | . 2 48.0 | 0 | 0 | M | Bachelors | Private | 21970.0 | 1 | Van | 1 | 0.0 | 0 | 0.0 | 0 | Urban | . Select Features and Targets . features = [&#39;age&#39;,&#39;gender&#39;,&#39;car_type&#39;,&#39;red_car&#39;,&#39;tif&#39;,&#39;education&#39;,&#39;car_use&#39;,&#39;bluebook&#39;,&#39;oldclaim&#39;,&#39;urbanicity&#39;] binary_target = &#39;claim_flag&#39; severity_target = &#39;clm_amt&#39; frequency_target = &#39;clm_freq&#39; . Modeling imports . import xgboost as xgb import patsy from sklearn.model_selection import train_test_split from sklearn import metrics . def clean_col_names(df): df.columns = df.columns.str.replace(&#39;[T.&#39;,&#39;_&#39;,regex=False).str.replace(&#39;]&#39;,&#39;&#39;,regex=False).str.replace(&#39; &#39;,&#39;_&#39;,regex=False).str.lower() return df . Train test split data . train, test = train_test_split(df,random_state=42, test_size=0.33,stratify=df[binary_target]) # select only claims train_sev = train.loc[train[severity_target].gt(0)] # create design matrix pats = lambda data: patsy.dmatrix(&#39;+&#39;.join(data.columns.tolist()),data,return_type=&#39;dataframe&#39;) # apply design matrix train_mx = pats(train[features]) train_mx_sev = patsy.build_design_matrices([train_mx.design_info],train_sev,return_type=&#39;dataframe&#39;)[0] test_mx = patsy.build_design_matrices([train_mx.design_info],test,return_type=&#39;dataframe&#39;)[0] # clean columns train_mx = clean_col_names(train_mx) train_mx_sev = clean_col_names(train_mx_sev) test_mx = clean_col_names(test_mx) . Frequency-Severity . Tree based XGBoost Models . binary_model = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;, eval_metric=&#39;auc&#39;, n_estimators=100, use_label_encoder=False).fit(train_mx, train[binary_target]) # gamma model severity_model = xgb.XGBRegressor(objective=&#39;reg:gamma&#39;, eval_metric=&#39;gamma-nloglik&#39;, n_estimators=100).fit(train_mx_sev, train_sev[severity_target]) # poisson model frequency_model = xgb.XGBRegressor(objective=&#39;count:poisson&#39;, eval_metric=&#39;poisson-nloglik&#39;, n_estimators=100).fit(train_mx_sev, train_sev[frequency_target]) . Calculate expected Claims . expected_claims = (severity_model.predict(test_mx) * frequency_model.predict(test_mx)) * binary_model.predict_proba(test_mx)[:,1] . Tweedie . var_power = 1.5 tweedie_model = xgb.XGBRegressor(objective=&#39;reg:tweedie&#39;, eval_metric=f&#39;tweedie-nloglik@{var_power}&#39;, n_estimators=100, tweedie_variance_power=var_power).fit(train_mx, train[severity_target]) tweedie_preds = tweedie_model.predict(test_mx) . Evaluation . def lorenz_curve(y_true, y_pred, exposure): y_true, y_pred = np.asarray(y_true), np.asarray(y_pred) exposure = np.asarray(exposure) # order samples by increasing predicted risk: ranking = np.argsort(y_pred) ranked_exposure = exposure[ranking] ranked_pure_premium = y_true[ranking] cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure) cumulated_claim_amount /= cumulated_claim_amount[-1] cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount)) return cumulated_samples, cumulated_claim_amount fig, ax = plt.subplots(1,2,figsize=(10, 5)) for label, y_pred in [(&quot;Frequency-Severity Model&quot;, expected_claims), (&quot;Tweedie Model&quot;, tweedie_preds)]: ordered_samples, cum_claims = lorenz_curve(test[severity_target], y_pred, test[severity_target].index) gini = 1 - 2 * metrics.auc(ordered_samples, cum_claims) ax.ravel()[0].plot(ordered_samples, cum_claims, linestyle=&quot;-&quot;, label=label + &quot; (Gini index: {:.3f})&quot;.format(gini)) ordered_samples, cum_claims = lorenz_curve(test[severity_target], test[severity_target], test[severity_target].index) gini = 1 - 2 * metrics.auc(ordered_samples, cum_claims) ax.ravel()[0].plot(ordered_samples, cum_claims, linestyle=&quot;-.&quot;, color=&quot;gray&quot;,label=&quot;Actual (Gini index: {:.3f})&quot;.format(gini)) ax.ravel()[0].plot([0, 1], [0, 1], linestyle=&quot;--&quot;, color=&quot;black&quot;) ax.ravel()[0].set(title=&quot;Lorenz Curves&quot;,xlabel=(&#39;Fraction of policyholders n&#39;&#39;(ordered by index of test dataframe)&#39;),ylabel=&#39;Fraction of total claim amount&#39;) ax.ravel()[0].legend(loc=&quot;upper left&quot;) ax.ravel()[1].hist(test[severity_target],label=&#39;Actual Claims&#39;) ax.ravel()[1].hist(expected_claims,label=&#39;Frequency-Severity Model&#39;) ax.ravel()[1].hist(tweedie_preds,label=&#39;Tweedie Model&#39;) ax.ravel()[1].legend(); ax.ravel()[1].set_title(&#39;Claims Distribution&#39;); ax.ravel()[1].set_xlabel(&#39;Claim Amount&#39;); ax.ravel()[1].set_ylabel(&#39;Frequency&#39;); plt.tight_layout(); . The Gini coefficient measures the inequality among values of a frequency distribution (for example, levels of income). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of one (or 100%) expresses maximal inequality among values (e.g., for a large number of people where only one person has all the income or consumption and all others have none, the Gini coefficient will be nearly one). . Conclusions . With the right parameters calculated from a variable we can use the probability distributions functions to draw samples from the same distribution . Is important to know how and when to use each distribution when needed based on the target variable and the problem requirements | . | To aproximate the real distribution you need to draw a large number of samples (kind of obvious but good to have it written down) . | Frequency - Severity Model: . Pros: Ability to improve and analyze models individually Extend more frequency based models while keeping severity simple | . | . | Cons: Highly increased complexity for hyperparameters tunning and feature selection | . | . | Tweedie Model: . Pros: Easier to apply tuning techniques for hyperparameters and feature selection since is just one model | . | Cons: Even though we can choose the ratio between Poisson - Gamma might be hard to maximize the performance from each component of the compound model | . | . | We do not have a good metric to meassure how good are our models in solving the insurance expected loss prediction other than the lorezn curve . | . Follow Up Questions . can we use PDF to infer the distribution of a variable ? | . References . Empirical Distributions . | Gamma Distribution in a RocTopple Probabilistic Analysis . | Poisson Distribution . | tweedie-vs-poisson-gamma . | .",
            "url": "https://cristianexer.github.io/blog/2021/02/23/Probability-Distributions.html",
            "relUrl": "/2021/02/23/Probability-Distributions.html",
            "date": " • Feb 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Can we find any sentiments on Wall Street ?",
            "content": ". Imports . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.style.use(&#39;seaborn&#39;) . About Data . Dataset . There are two channels of data provided in this dataset:&gt;News data: I crawled historical news headlines from Reddit WorldNews Channel (/r/worldnews). They are ranked by reddit users&#39; votes, and only the top 25 headlines are considered for a single date.(Range: 2008-06-08 to 2016-07-01) Stock data: Dow Jones Industrial Average (DJIA) is used to &quot;prove the concept&quot;. (Range: 2008-08-08 to 2016-07-01) I provided three data files in .csv format: RedditNews.csv:two columnsThe first column is the &quot;date&quot;, and second column is the &quot;news headlines&quot;. All news are ranked from top to bottom based on how hot they are. Hence, there are 25 lines for each date. DJIA_table.csv: Downloaded directly from Yahoo Finance: check out the web page for more info. CombinedNewsDJIA.csv: To make things easier for my students, I provide this combined dataset with 27 columns. The first column is &quot;Date&quot;, the second is &quot;Label&quot;, and the following ones are news headlines ranging from &quot;Top1&quot; to &quot;Top25&quot;. . df = pd.read_csv(&#39;data/Combined_news_DJIA.csv&#39;) df[:2] . Date Label Top1 Top2 Top3 Top4 Top5 Top6 Top7 Top8 ... Top16 Top17 Top18 Top19 Top20 Top21 Top22 Top23 Top24 Top25 . 0 2008-08-08 | 0 | b&quot;Georgia &#39;downs two Russian warplanes&#39; as cou... | b&#39;BREAKING: Musharraf to be impeached.&#39; | b&#39;Russia Today: Columns of troops roll into So... | b&#39;Russian tanks are moving towards the capital... | b&quot;Afghan children raped with &#39;impunity,&#39; U.N. ... | b&#39;150 Russian tanks have entered South Ossetia... | b&quot;Breaking: Georgia invades South Ossetia, Rus... | b&quot;The &#39;enemy combatent&#39; trials are nothing but... | ... | b&#39;Georgia Invades South Ossetia - if Russia ge... | b&#39;Al-Qaeda Faces Islamist Backlash&#39; | b&#39;Condoleezza Rice: &quot;The US would not act to p... | b&#39;This is a busy day: The European Union has ... | b&quot;Georgia will withdraw 1,000 soldiers from Ir... | b&#39;Why the Pentagon Thinks Attacking Iran is a ... | b&#39;Caucasus in crisis: Georgia invades South Os... | b&#39;Indian shoe manufactory - And again in a se... | b&#39;Visitors Suffering from Mental Illnesses Ban... | b&quot;No Help for Mexico&#39;s Kidnapping Surge&quot; | . 1 2008-08-11 | 1 | b&#39;Why wont America and Nato help us? If they w... | b&#39;Bush puts foot down on Georgian conflict&#39; | b&quot;Jewish Georgian minister: Thanks to Israeli ... | b&#39;Georgian army flees in disarray as Russians ... | b&quot;Olympic opening ceremony fireworks &#39;faked&#39;&quot; | b&#39;What were the Mossad with fraudulent New Zea... | b&#39;Russia angered by Israeli military sale to G... | b&#39;An American citizen living in S.Ossetia blam... | ... | b&#39;Israel and the US behind the Georgian aggres... | b&#39;&quot;Do not believe TV, neither Russian nor Geor... | b&#39;Riots are still going on in Montreal (Canada... | b&#39;China to overtake US as largest manufacturer&#39; | b&#39;War in South Ossetia [PICS]&#39; | b&#39;Israeli Physicians Group Condemns State Tort... | b&#39; Russia has just beaten the United States ov... | b&#39;Perhaps *the* question about the Georgia - R... | b&#39;Russia is so much better at war&#39; | b&quot;So this is what it&#39;s come to: trading sex fo... | . 2 rows × 27 columns . df.Label.value_counts().rename(index={1:&#39;Next Day Stock Raise&#39;,0:&#39;No Raise&#39;}).plot.bar(rot=0,figsize=(5,3)); . features = [f&#39;Top{x+1}&#39; for x in range(25)] target = &#39;Label&#39; . Vader on Wall Street . VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It is fully open-sourced under the [MIT License] (we sincerely appreciate all attributions and readily accept most contributions, but please don&#39;t hold us liable). . import nltk from nltk.sentiment.vader import SentimentIntensityAnalyzer nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to [nltk_data] /Users/cristianexer/nltk_data... [nltk_data] Package vader_lexicon is already up-to-date! . True . sid = SentimentIntensityAnalyzer() . %%time copy_df = df.copy() copy_df[:1] . CPU times: user 1.08 ms, sys: 425 µs, total: 1.51 ms Wall time: 1.02 ms . Date Label Top1 Top2 Top3 Top4 Top5 Top6 Top7 Top8 ... Top16 Top17 Top18 Top19 Top20 Top21 Top22 Top23 Top24 Top25 . 0 2008-08-08 | 0 | b&quot;Georgia &#39;downs two Russian warplanes&#39; as cou... | b&#39;BREAKING: Musharraf to be impeached.&#39; | b&#39;Russia Today: Columns of troops roll into So... | b&#39;Russian tanks are moving towards the capital... | b&quot;Afghan children raped with &#39;impunity,&#39; U.N. ... | b&#39;150 Russian tanks have entered South Ossetia... | b&quot;Breaking: Georgia invades South Ossetia, Rus... | b&quot;The &#39;enemy combatent&#39; trials are nothing but... | ... | b&#39;Georgia Invades South Ossetia - if Russia ge... | b&#39;Al-Qaeda Faces Islamist Backlash&#39; | b&#39;Condoleezza Rice: &quot;The US would not act to p... | b&#39;This is a busy day: The European Union has ... | b&quot;Georgia will withdraw 1,000 soldiers from Ir... | b&#39;Why the Pentagon Thinks Attacking Iran is a ... | b&#39;Caucasus in crisis: Georgia invades South Os... | b&#39;Indian shoe manufactory - And again in a se... | b&#39;Visitors Suffering from Mental Illnesses Ban... | b&quot;No Help for Mexico&#39;s Kidnapping Surge&quot; | . 1 rows × 27 columns . Extract Sentiments from each news title . copy_df[features] = copy_df[features].applymap(lambda x: sid.polarity_scores(x) if type(x) == str else {}) copy_df[:1] . Date Label Top1 Top2 Top3 Top4 Top5 Top6 Top7 Top8 ... Top16 Top17 Top18 Top19 Top20 Top21 Top22 Top23 Top24 Top25 . 0 2008-08-08 | 0 | {&#39;neg&#39;: 0.262, &#39;neu&#39;: 0.738, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0, &#39;compound... | {&#39;neg&#39;: 0.172, &#39;neu&#39;: 0.828, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.247, &#39;neu&#39;: 0.753, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.424, &#39;neu&#39;: 0.576, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0, &#39;compound... | {&#39;neg&#39;: 0.149, &#39;neu&#39;: 0.851, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.107, &#39;neu&#39;: 0.79, &#39;pos&#39;: 0.103, &#39;com... | ... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0, &#39;compound... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 1.0, &#39;pos&#39;: 0.0, &#39;compound... | {&#39;neg&#39;: 0.078, &#39;neu&#39;: 0.819, &#39;pos&#39;: 0.103, &#39;co... | {&#39;neg&#39;: 0.092, &#39;neu&#39;: 0.78, &#39;pos&#39;: 0.128, &#39;com... | {&#39;neg&#39;: 0.112, &#39;neu&#39;: 0.773, &#39;pos&#39;: 0.116, &#39;co... | {&#39;neg&#39;: 0.351, &#39;neu&#39;: 0.649, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.406, &#39;neu&#39;: 0.594, &#39;pos&#39;: 0.0, &#39;comp... | {&#39;neg&#39;: 0.14, &#39;neu&#39;: 0.86, &#39;pos&#39;: 0.0, &#39;compou... | {&#39;neg&#39;: 0.65, &#39;neu&#39;: 0.35, &#39;pos&#39;: 0.0, &#39;compou... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.649, &#39;pos&#39;: 0.351, &#39;comp... | . 1 rows × 27 columns . Some steps to format the data . copy_df = copy_df.set_index(&#39;Date&#39;)[features].unstack().reset_index() copy_df[:3] . level_0 Date 0 . 0 Top1 | 2008-08-08 | {&#39;neg&#39;: 0.262, &#39;neu&#39;: 0.738, &#39;pos&#39;: 0.0, &#39;comp... | . 1 Top1 | 2008-08-11 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.668, &#39;pos&#39;: 0.332, &#39;comp... | . 2 Top1 | 2008-08-12 | {&#39;neg&#39;: 0.169, &#39;neu&#39;: 0.656, &#39;pos&#39;: 0.175, &#39;co... | . sentiments = pd.DataFrame.from_dict(copy_df[0].values.tolist()) sentiments[:3] . neg neu pos compound . 0 0.262 | 0.738 | 0.000 | -0.5994 | . 1 0.000 | 0.668 | 0.332 | 0.8156 | . 2 0.169 | 0.656 | 0.175 | 0.0258 | . copy_df[sentiments.columns] = sentiments copy_df = copy_df.drop([0],axis=1).rename(columns={&#39;level_0&#39;:&#39;news_label&#39;}) copy_df[:3] . news_label Date neg neu pos compound . 0 Top1 | 2008-08-08 | 0.262 | 0.738 | 0.000 | -0.5994 | . 1 Top1 | 2008-08-11 | 0.000 | 0.668 | 0.332 | 0.8156 | . 2 Top1 | 2008-08-12 | 0.169 | 0.656 | 0.175 | 0.0258 | . clean_df = copy_df.groupby(&#39;Date&#39;,as_index=False).agg({x:[&#39;mean&#39;,&#39;sum&#39;] for x in sentiments.columns}).copy() clean_df.columns = [ &#39;_&#39;.join(x) if x[1] != &#39;&#39; else x[0].lower() for x in clean_df.columns] clean_df = clean_df.merge(df[[&#39;Date&#39;,&#39;Label&#39;]],left_on=&#39;date&#39;,right_on=&#39;Date&#39;,how=&#39;left&#39;).drop([&#39;Date&#39;],axis=1) clean_df[&#39;date&#39;] = pd.to_datetime(clean_df[&#39;date&#39;] ) clean_df[:3] . date neg_mean neg_sum neu_mean neu_sum pos_mean pos_sum compound_mean compound_sum Label . 0 2008-08-08 | 0.19284 | 4.821 | 0.76920 | 19.230 | 0.03800 | 0.950 | -0.309440 | -7.7360 | 0 | . 1 2008-08-11 | 0.15028 | 3.757 | 0.78260 | 19.565 | 0.06708 | 1.677 | -0.120740 | -3.0185 | 1 | . 2 2008-08-12 | 0.15712 | 3.928 | 0.78496 | 19.624 | 0.05788 | 1.447 | -0.217556 | -5.4389 | 0 | . Now let&#39;s do some data enrichment for those sentiments . import pandas_datareader.data as web . min_date,max_date = clean_df.date.min(),clean_df.date.max() min_date,max_date . (Timestamp(&#39;2008-08-08 00:00:00&#39;), Timestamp(&#39;2016-07-01 00:00:00&#39;)) . DJI stock prices from the same period of time . stock = web.DataReader(&#39;^DJI&#39;, &#39;stooq&#39;,start=min_date,end=max_date).reset_index() stock.columns = stock.columns.str.lower() stock[:3] . date open high low close volume . 0 2016-07-01 | 17924.24 | 18002.38 | 17916.91 | 17949.37 | 82167191 | . 1 2016-06-30 | 17712.76 | 17930.61 | 17711.80 | 17929.99 | 133078223 | . 2 2016-06-29 | 17456.02 | 17704.51 | 17456.02 | 17694.68 | 106343184 | . Merged the clean data with the stock prices data . enriched_df = clean_df.merge(stock,on=&#39;date&#39;,how=&#39;left&#39;).sort_values(by=&#39;date&#39;).copy() enriched_df[:3] . date neg_mean neg_sum neu_mean neu_sum pos_mean pos_sum compound_mean compound_sum Label open high low close volume . 0 2008-08-08 | 0.19284 | 4.821 | 0.76920 | 19.230 | 0.03800 | 0.950 | -0.309440 | -7.7360 | 0 | 11432.1 | 11760.0 | 11388.0 | 11734.3 | 212842817 | . 1 2008-08-11 | 0.15028 | 3.757 | 0.78260 | 19.565 | 0.06708 | 1.677 | -0.120740 | -3.0185 | 1 | 11729.7 | 11867.1 | 11675.5 | 11782.3 | 183186104 | . 2 2008-08-12 | 0.15712 | 3.928 | 0.78496 | 19.624 | 0.05788 | 1.447 | -0.217556 | -5.4389 | 0 | 11781.7 | 11782.3 | 11601.5 | 11642.5 | 173686814 | . Features Correlation . plt.figure(figsize=(16,7)) corr = enriched_df.corr(method=&#39;spearman&#39;) sns.heatmap(corr[corr&gt;0],annot=True,fmt=&#39;.2f&#39;,cmap=&#39;Blues&#39;); . Close Price and Compund Mean hilighted by given Label . sns.scatterplot(x=enriched_df[&#39;close&#39;],y=enriched_df[&#39;compound_mean&#39;],hue=enriched_df[&#39;Label&#39;],alpha=.5); . Now let&#39;s use an XGBoost model to see what is driving prices changes . import xgboost as xgb from sklearn.model_selection import GridSearchCV from sklearn import metrics . We well use the compound mean as feature to capture the news influence in our model . features = [&#39;compound_mean&#39;,&#39;open&#39;,&#39;high&#39;,&#39;low&#39;] target = &#39;close&#39; . XGBoost Regressor . params = {&#39;eval_metric&#39;: &#39;rmse&#39;, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 100, &#39;objective&#39;: &#39;reg:gamma&#39;, &#39;use_label_encoder&#39;: False} reg = xgb.XGBRegressor(**params) reg.fit(enriched_df[features],enriched_df[target]) . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, eval_metric=&#39;rmse&#39;, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=5, min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, objective=&#39;reg:gamma&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None) . Let&#39;s look at one of our trees . positive sentiment: compound score &gt;= 0.05 | neutral sentiment: (compound score &gt; -0.05) and (compound score &lt; 0.05) | negative sentiment: compound score &lt;= -0.05 | . fig,ax = plt.subplots(1,1,figsize=(35,10)) xgb.plot_tree(reg,num_trees=50,ax=ax); . Feature Importance . xgb.plot_importance(reg); . Feature Impact . import shap . expl = shap.TreeExplainer(reg) . shap_values = expl.shap_values(enriched_df[features],enriched_df[target]) . shap.summary_plot(shap_values,enriched_df[features]) .",
            "url": "https://cristianexer.github.io/blog/2021/02/14/Stock-Market-Sentiments.html",
            "relUrl": "/2021/02/14/Stock-Market-Sentiments.html",
            "date": " • Feb 14, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Autoencoders and Fraud Detection",
            "content": ". Imports . import pandas as pd import numpy as np from sklearn import preprocessing import matplotlib.pyplot as plt import seaborn as sns from sklearn import metrics from sklearn.model_selection import train_test_split from keras.layers import Input, Dense from keras.models import Model, Sequential from keras import regularizers import xgboost as xgb plt.style.use(&#39;seaborn&#39;) . Data . The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. . It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are &#39;Time&#39; and &#39;Amount&#39;. Feature &#39;Time&#39; contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature &#39;Amount&#39; is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature &#39;Class&#39; is the response variable and it takes value 1 in case of fraud and 0 otherwise. . df = pd.read_csv(&#39;data/creditcard.csv&#39;) df . Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class . 0 0.0 | -1.359807 | -0.072781 | 2.536347 | 1.378155 | -0.338321 | 0.462388 | 0.239599 | 0.098698 | 0.363787 | ... | -0.018307 | 0.277838 | -0.110474 | 0.066928 | 0.128539 | -0.189115 | 0.133558 | -0.021053 | 149.62 | 0 | . 1 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 | -0.082361 | -0.078803 | 0.085102 | -0.255425 | ... | -0.225775 | -0.638672 | 0.101288 | -0.339846 | 0.167170 | 0.125895 | -0.008983 | 0.014724 | 2.69 | 0 | . 2 1.0 | -1.358354 | -1.340163 | 1.773209 | 0.379780 | -0.503198 | 1.800499 | 0.791461 | 0.247676 | -1.514654 | ... | 0.247998 | 0.771679 | 0.909412 | -0.689281 | -0.327642 | -0.139097 | -0.055353 | -0.059752 | 378.66 | 0 | . 3 1.0 | -0.966272 | -0.185226 | 1.792993 | -0.863291 | -0.010309 | 1.247203 | 0.237609 | 0.377436 | -1.387024 | ... | -0.108300 | 0.005274 | -0.190321 | -1.175575 | 0.647376 | -0.221929 | 0.062723 | 0.061458 | 123.50 | 0 | . 4 2.0 | -1.158233 | 0.877737 | 1.548718 | 0.403034 | -0.407193 | 0.095921 | 0.592941 | -0.270533 | 0.817739 | ... | -0.009431 | 0.798278 | -0.137458 | 0.141267 | -0.206010 | 0.502292 | 0.219422 | 0.215153 | 69.99 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 284802 172786.0 | -11.881118 | 10.071785 | -9.834783 | -2.066656 | -5.364473 | -2.606837 | -4.918215 | 7.305334 | 1.914428 | ... | 0.213454 | 0.111864 | 1.014480 | -0.509348 | 1.436807 | 0.250034 | 0.943651 | 0.823731 | 0.77 | 0 | . 284803 172787.0 | -0.732789 | -0.055080 | 2.035030 | -0.738589 | 0.868229 | 1.058415 | 0.024330 | 0.294869 | 0.584800 | ... | 0.214205 | 0.924384 | 0.012463 | -1.016226 | -0.606624 | -0.395255 | 0.068472 | -0.053527 | 24.79 | 0 | . 284804 172788.0 | 1.919565 | -0.301254 | -3.249640 | -0.557828 | 2.630515 | 3.031260 | -0.296827 | 0.708417 | 0.432454 | ... | 0.232045 | 0.578229 | -0.037501 | 0.640134 | 0.265745 | -0.087371 | 0.004455 | -0.026561 | 67.88 | 0 | . 284805 172788.0 | -0.240440 | 0.530483 | 0.702510 | 0.689799 | -0.377961 | 0.623708 | -0.686180 | 0.679145 | 0.392087 | ... | 0.265245 | 0.800049 | -0.163298 | 0.123205 | -0.569159 | 0.546668 | 0.108821 | 0.104533 | 10.00 | 0 | . 284806 172792.0 | -0.533413 | -0.189733 | 0.703337 | -0.506271 | -0.012546 | -0.649617 | 1.577006 | -0.414650 | 0.486180 | ... | 0.261057 | 0.643078 | 0.376777 | 0.008797 | -0.473649 | -0.818267 | -0.002415 | 0.013649 | 217.00 | 0 | . 284807 rows × 31 columns . EDA . Feature Correlation . corr = df.corr(method=&#39;spearman&#39;) plt.figure(figsize=(12,7)) sns.heatmap(corr[corr &gt; .2],annot=True,fmt=&#39;.2f&#39;,cmap=&#39;Blues&#39;); . Modeling . Select Features for modeling . features = [&#39;Time&#39;,&#39;V1&#39;,&#39;V2&#39;,&#39;V3&#39;,&#39;V4&#39;,&#39;V5&#39;,&#39;V6&#39;,&#39;V7&#39;,&#39;V8&#39;,&#39;V9&#39;,&#39;V10&#39;,&#39;V11&#39;,&#39;V12&#39;,&#39;V13&#39;,&#39;V14&#39;,&#39;V15&#39;,&#39;V16&#39;,&#39;V17&#39;,&#39;V18&#39;,&#39;V19&#39;,&#39;V20&#39;,&#39;V21&#39;,&#39;V22&#39;,&#39;V23&#39;,&#39;V24&#39;,&#39;V25&#39;,&#39;V26&#39;,&#39;V27&#39;,&#39;V28&#39;,&#39;Amount&#39;] target = &#39;Class&#39; . Train Test Split . train, test = train_test_split(df, test_size=0.33,random_state=42,stratify=df[target]) . Autoencoder . Autoencoder is a neural network designed to learn an identity function in an unsupervised way to reconstruct the original input while compressing the data in the process so as to discover a more efficient and compressed representation. . The encoder network essentially accomplishes the dimensionality reduction, just like how we would use Principal Component Analysis (PCA) or Matrix Factorization (MF) for. In addition, the autoencoder is explicitly optimized for the data reconstruction from the code. A good intermediate representation not only can capture latent variables, but also benefits a full decompression process. . Define a generalistic Auto Encoder . def autoencoder(shape,regularizer=regularizers.l1(10e-5)): ## input layer X_train.shape[1] input_layer = Input(shape=(shape,)) ## encoding part encoded = Dense(100, activation=&#39;tanh&#39;, activity_regularizer=regularizer)(input_layer) encoded = Dense(50, activation=&#39;relu&#39;)(encoded) ## decoding part decoded = Dense(50, activation=&#39;tanh&#39;)(encoded) decoded = Dense(100, activation=&#39;tanh&#39;)(decoded) ## output layer output_layer = Dense(shape, activation=&#39;relu&#39;)(decoded) autoencoder = Model(input_layer, output_layer) autoencoder.compile(optimizer=&quot;adadelta&quot;, loss=&quot;mse&quot;) return autoencoder . Pass the shape of your X_train . encoder = autoencoder(train[features].shape[1]) . Keras model Summary . encoder.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 30)] 0 _________________________________________________________________ dense (Dense) (None, 100) 3100 _________________________________________________________________ dense_1 (Dense) (None, 50) 5050 _________________________________________________________________ dense_2 (Dense) (None, 50) 2550 _________________________________________________________________ dense_3 (Dense) (None, 100) 5100 _________________________________________________________________ dense_4 (Dense) (None, 30) 3030 ================================================================= Total params: 18,830 Trainable params: 18,830 Non-trainable params: 0 _________________________________________________________________ . Let&#39;s normalize our data . scaler = preprocessing.MinMaxScaler().fit(train[features].values) . scaled_data = df.copy() scaled_data[features] = scaler.transform(df[features]) fraud, non_fraud = scaled_data.loc[scaled_data[target] == 1],scaled_data.loc[scaled_data[target] == 0] . Now we can fit our Autoencoder . encoder.fit(scaled_data[features], scaled_data[features], epochs = 20, shuffle = True, validation_split = 0.25) . Epoch 1/20 6676/6676 [==============================] - 9s 1ms/step - loss: 0.1457 - val_loss: 0.0821 Epoch 2/20 6676/6676 [==============================] - 6s 845us/step - loss: 0.0772 - val_loss: 0.0821 Epoch 3/20 6676/6676 [==============================] - 6s 867us/step - loss: 0.0770 - val_loss: 0.0818 Epoch 4/20 6676/6676 [==============================] - 5s 790us/step - loss: 0.0769 - val_loss: 0.0814 Epoch 5/20 6676/6676 [==============================] - 7s 976us/step - loss: 0.0768 - val_loss: 0.0810 Epoch 6/20 6676/6676 [==============================] - 5s 783us/step - loss: 0.0712 - val_loss: 0.0603 Epoch 7/20 6676/6676 [==============================] - 5s 722us/step - loss: 0.0559 - val_loss: 0.0599 Epoch 8/20 6676/6676 [==============================] - 5s 717us/step - loss: 0.0558 - val_loss: 0.0596 Epoch 9/20 6676/6676 [==============================] - 5s 713us/step - loss: 0.0557 - val_loss: 0.0593 Epoch 10/20 6676/6676 [==============================] - 5s 710us/step - loss: 0.0556 - val_loss: 0.0590 Epoch 11/20 6676/6676 [==============================] - 5s 801us/step - loss: 0.0555 - val_loss: 0.0586 Epoch 12/20 6676/6676 [==============================] - 5s 768us/step - loss: 0.0554 - val_loss: 0.0584 Epoch 13/20 6676/6676 [==============================] - 5s 748us/step - loss: 0.0553 - val_loss: 0.0581 Epoch 14/20 6676/6676 [==============================] - 8s 1ms/step - loss: 0.0552 - val_loss: 0.0577 Epoch 15/20 6676/6676 [==============================] - 6s 835us/step - loss: 0.0551 - val_loss: 0.0575 Epoch 16/20 6676/6676 [==============================] - 6s 862us/step - loss: 0.0550 - val_loss: 0.0572 Epoch 17/20 6676/6676 [==============================] - 7s 1ms/step - loss: 0.0549 - val_loss: 0.0569 Epoch 18/20 6676/6676 [==============================] - 6s 971us/step - loss: 0.0548 - val_loss: 0.0409 Epoch 19/20 6676/6676 [==============================] - 6s 957us/step - loss: 0.0261 - val_loss: 0.0248 Epoch 20/20 6676/6676 [==============================] - 6s 855us/step - loss: 0.0229 - val_loss: 0.0245 . &lt;tensorflow.python.keras.callbacks.History at 0x7fee661db190&gt; . We extract the hidden layers so then our model can encode given data . hidden_representation = Sequential() hidden_representation.add(encoder.layers[0]) hidden_representation.add(encoder.layers[1]) hidden_representation.add(encoder.layers[2]) . then we encode the two dataframes that contains fraudulent and non fraudulent samples . fraud_hidden = hidden_representation.predict(fraud[features]) non_fraud_hidden = hidden_representation.predict(non_fraud[features]) . finally we ca bring them back together into a dataframe where we can see that we have a higher number of features than the initial one, this being a result of our 3rd layer which has an output shape of 50 . encoded_df = pd.DataFrame(np.append(fraud_hidden, non_fraud_hidden, axis = 0)) encoded_df[target] = np.append(np.ones(fraud_hidden.shape[0]), np.zeros(non_fraud_hidden.shape[0])) encoded_df[target] = encoded_df[target].astype(int) encoded_df[:3] . 0 1 2 3 4 5 6 7 8 9 ... 41 42 43 44 45 46 47 48 49 Class . 0 0.0 | 0.176606 | 0.482658 | 0.149698 | 0.0 | 0.0 | 0.675127 | 0.0 | 0.677855 | 0.018553 | ... | 0.257808 | 0.0 | 0.070143 | 0.015373 | 0.0 | 0.0 | 0.0 | 0.0 | 0.153822 | 1 | . 1 0.0 | 0.141019 | 0.480566 | 0.048828 | 0.0 | 0.0 | 0.617751 | 0.0 | 0.668253 | 0.000000 | ... | 0.260265 | 0.0 | 0.094272 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.217649 | 1 | . 2 0.0 | 0.151974 | 0.425213 | 0.094413 | 0.0 | 0.0 | 0.695415 | 0.0 | 0.605650 | 0.000000 | ... | 0.183696 | 0.0 | 0.106665 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.159188 | 1 | . 3 rows × 51 columns . Classification models . Now that we have encoded data we wan to train an XGBoost model to classify fraudulent accounts. . We will use first the raw data to train the classifier, then in the second part we will use the encoded data. . Before Autoencoding . train the classifier . %%time clf = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;,use_label_encoder=False,eval_metric=&#39;logloss&#39;) clf.fit(train[features],train[target]) . CPU times: user 2min 49s, sys: 896 ms, total: 2min 50s Wall time: 23.3 s . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, eval_metric=&#39;logloss&#39;, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, objective=&#39;binary:logistic&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None) . make predictions on the test data . test = test.copy() test[&#39;y_pred&#39;] = clf.predict(test[features]) . Classification Report . print(metrics.classification_report(test[target],test[&#39;y_pred&#39;])) . precision recall f1-score support 0 1.00 1.00 1.00 93825 1 0.97 0.77 0.86 162 accuracy 1.00 93987 macro avg 0.98 0.89 0.93 93987 weighted avg 1.00 1.00 1.00 93987 . Confusion Matrix . sns.heatmap(metrics.confusion_matrix(test[target],test[&#39;y_pred&#39;]),cmap=&#39;Blues&#39;,annot=True,fmt=&#39;d&#39;, annot_kws={&#39;size&#39;: 16}) plt.xlabel(&#39;Predicted&#39;) plt.ylabel(&#39;Actual&#39;); . After Autoencoding . Train test split the encoded data . encoded_train,encoded_test = train_test_split(encoded_df,test_size=0.33,random_state=42,stratify=encoded_df[target]) . train the classifier . %%time enc_clf = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;,use_label_encoder=False,eval_metric=&#39;logloss&#39;) enc_clf.fit(encoded_train.drop([target],axis=1),encoded_train[target]) . CPU times: user 3min 27s, sys: 1.74 s, total: 3min 29s Wall time: 29.2 s . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, eval_metric=&#39;logloss&#39;, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8, num_parallel_tree=1, objective=&#39;binary:logistic&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, use_label_encoder=False, validate_parameters=1, verbosity=None) . make predictions on the test data . encoded_test = encoded_test.copy() encoded_test[&#39;y_pred&#39;] = enc_clf.predict(encoded_test.drop([target],axis=1)) . Classification Report . print(metrics.classification_report(encoded_test[target],encoded_test[&#39;y_pred&#39;])) . precision recall f1-score support 0 1.00 1.00 1.00 93825 1 0.95 0.67 0.79 162 accuracy 1.00 93987 macro avg 0.97 0.84 0.89 93987 weighted avg 1.00 1.00 1.00 93987 . Confusion Matrix . sns.heatmap(metrics.confusion_matrix(encoded_test[target],encoded_test[&#39;y_pred&#39;]),cmap=&#39;Blues&#39;,annot=True,fmt=&#39;d&#39;, annot_kws={&#39;size&#39;: 16}) plt.xlabel(&#39;Predicted&#39;) plt.ylabel(&#39;Actual&#39;); . Conclusions . Here we have seen an example usage of the autoencoder architecture | Another aspect we can observe is the contrast between the orignal data vs encoded data through the xgboost model. In the same time the loss value from the autoencoder is playing a big role in the difference between the results from the both classifiers. | The current data in this example is not illustrating the full potention of autoencoders since it has already being processed by PCA | . References . Semi Supervised Classification using AutoEncoders | From Autoencoder to Beta-VAE | .",
            "url": "https://cristianexer.github.io/blog/2021/02/13/Cred-Card-Fraud-Detection.html",
            "relUrl": "/2021/02/13/Cred-Card-Fraud-Detection.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Tagging Wikipedia Articles",
            "content": "Imports . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns plt.style.use(&#39;seaborn&#39;) sns.set_style(&#39;whitegrid&#39;) . About Data . Dataset . Context . The growing availability of information in the past decade has allowed internet users to find vast amounts of information online, but this has come with more and more deceptive articles designed to advertise or promote a product or ideology. In addition, hidden sponsored news articles have grown in prevalence in recent years as news organizations have shifted their business strategy to account for developments in technology and content consumption. It is for this reason that having a system in place to detect these deceptive practices is more important than ever. . Content . This dataset consists of articles that were tagged by users as having a &quot;promotional tone&quot; (promotional.csv) and of articles that were tagged as &quot;good articles&quot; (good.csv). . The each promotional article can have multiple labels (quotes from Wikipedia tags): . advert - &quot;This article contains content that is written like an advertisement.&quot; | coi - &quot;A major contributor to this article appears to have a close connection with its subject.&quot; | fanpov - &quot;This article may be written from a fan&#39;s point of view, rather than a neutral point of view.&quot; | pr - &quot;This article reads like a press release or a news article or is largely based on routine coverage or sensationalism.&quot; | resume - &quot;This biographical article is written like a résumé.&quot; | . The &quot;good articles&quot; are articles that were deemed &quot;well written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated.&quot; . df = pd.read_csv(&#39;data/wiki-articles-promo.csv&#39;).reset_index().rename(columns={&#39;index&#39;:&#39;id&#39;}) df . id text advert coi fanpov pr resume url . 0 0 | 1 Litre no Namida 1, lit. 1 Litre of Tears als... | 0 | 0 | 1 | 0 | 0 | https://en.wikipedia.org/wiki/1%20Litre%20no%2... | . 1 1 | 1DayLater was free, web based software that wa... | 1 | 1 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/1DayLater | . 2 2 | 1E is a privately owned IT software and servic... | 1 | 0 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/1E | . 3 3 | 1Malaysia pronounced One Malaysia in English a... | 1 | 0 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/1Malaysia | . 4 4 | The Jerusalem Biennale, as stated on the Bienn... | 1 | 0 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/1st%20Jerusalem%... | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 23832 23832 | ZURICH.MINDS is a non profit foundation set up... | 1 | 0 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/Zurich.minds | . 23833 23833 | zvelo, Inc. or simply zvelo is a privately hel... | 1 | 0 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/Zvelo | . 23834 23834 | Zygote Media Group is a 3D human anatomy conte... | 1 | 1 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/Zygote%20Media%2... | . 23835 23835 | Zylom is a distributor of casual games for PC ... | 1 | 0 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/Zylom | . 23836 23836 | Zynx Health Incorporated is an American corpor... | 1 | 1 | 0 | 0 | 0 | https://en.wikipedia.org/wiki/Zynx%20Health | . 23837 rows × 8 columns . labels = [&#39;advert&#39;,&#39;coi&#39;,&#39;fanpov&#39;,&#39;pr&#39;,&#39;resume&#39;] . Modeling . Pre-Processing . from sklearn.model_selection import train_test_split from sklearn import metrics . train,test = train_test_split(df[[&#39;text&#39;] + labels],test_size=0.33, random_state=42,stratify=df[labels[1:]]) . For ML based models we can use USE to extract the sentence vectors . The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. . The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder. . To learn more about text embeddings, refer to the TensorFlow Embeddings documentation. Our encoder differs from word level embedding models in that we train on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just individual words. Details are available in the paper &quot;Universal Sentence Encoder&quot; . Paper . Pre-Trained Models . . import tensorflow as tf import tensorflow_hub as hub class UniversalSentenceEncoder: def __init__(self, encoder=&#39;universal-sentence-encoder&#39;, version=&#39;4&#39;): self.version = version self.encoder = encoder self.embd = hub.load(f&quot;https://tfhub.dev/google/{encoder}/{version}&quot;,) def embed(self, sentences): return self.embd(sentences) def squized(self, sentences): return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string)))) use = UniversalSentenceEncoder() . Use the USE to get the vectors from the text . %%time train_use = train.copy() train_use[&#39;text_vect&#39;] = use.squized(train_use[&#39;text&#39;].tolist()).tolist() . CPU times: user 5min 38s, sys: 7min 41s, total: 13min 20s Wall time: 16min 45s . %%time test_use = test.copy() test_use[&#39;text_vect&#39;] = use.squized(test_use[&#39;text&#39;].tolist()).tolist() . CPU times: user 3min 5s, sys: 3min 4s, total: 6min 10s Wall time: 6min 20s . Binary Relevance . Transforms a multi-label classification problem with L labels into L single-label separate binary classification problems using the same base classifier provided in the constructor. The prediction output is the union of all per label classifiers . . from skmultilearn.problem_transform import BinaryRelevance from sklearn.ensemble import RandomForestClassifier . Random Forest . %%time classifier = BinaryRelevance( classifier = RandomForestClassifier() ) # train classifier.fit(pd.DataFrame(train_use[&#39;text_vect&#39;].tolist()), train_use[labels]) . CPU times: user 2min 43s, sys: 1.21 s, total: 2min 44s Wall time: 2min 45s . BinaryRelevance(classifier=RandomForestClassifier(), require_dense=[True, True]) . predictions = classifier.predict(pd.DataFrame(test_use[&#39;text_vect&#39;].tolist())) . predictions_proba = classifier.predict_proba(pd.DataFrame(test_use[&#39;text_vect&#39;].tolist())) . print(metrics.classification_report(test_use[labels],predictions,zero_division=0)) . precision recall f1-score support 0 0.83 0.97 0.90 6239 1 0.00 0.00 0.00 707 2 0.92 0.14 0.24 493 3 0.00 0.00 0.00 500 4 0.74 0.16 0.26 726 micro avg 0.83 0.72 0.77 8665 macro avg 0.50 0.25 0.28 8665 weighted avg 0.72 0.72 0.68 8665 samples avg 0.79 0.75 0.77 8665 . XGBoost . import xgboost as xgb . %%time classifier_xgb = BinaryRelevance( classifier = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;,use_label_encoder=False) ) # train classifier_xgb.fit(pd.DataFrame(train_use[&#39;text_vect&#39;].tolist()), train_use[labels]) . [18:22:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [18:23:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [18:23:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [18:24:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [18:25:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. CPU times: user 26min 45s, sys: 5.84 s, total: 26min 51s Wall time: 3min 40s . BinaryRelevance(classifier=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None, colsample_bynode=None, colsample_bytree=None, gamma=None, gpu_id=None, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=None, max_delta_step=None, max_depth=None, min_child_weight=None, missing=nan, monotone_constraints=None, n_estimators=100, n_jobs=None, num_parallel_tree=None, random_state=None, reg_alpha=None, reg_lambda=None, scale_pos_weight=None, subsample=None, tree_method=None, use_label_encoder=False, validate_parameters=None, verbosity=None), require_dense=[True, True]) . xgb_pred = classifier_xgb.predict(pd.DataFrame(test_use[&#39;text_vect&#39;].tolist(),columns=[f&#39;f{x}&#39; for x in range(512)])) . xgb_pred_proba = classifier_xgb.predict_proba(pd.DataFrame(test_use[&#39;text_vect&#39;].tolist(),columns=[f&#39;f{x}&#39; for x in range(512)])) . print(metrics.classification_report(test_use[labels],xgb_pred,zero_division=0)) . precision recall f1-score support 0 0.86 0.94 0.90 6239 1 0.45 0.01 0.01 707 2 0.73 0.29 0.42 493 3 0.00 0.00 0.00 500 4 0.61 0.38 0.47 726 micro avg 0.84 0.73 0.78 8665 macro avg 0.53 0.32 0.36 8665 weighted avg 0.75 0.73 0.71 8665 samples avg 0.79 0.76 0.77 8665 . We can see using Binary Relevance we can easily use diffrent models for multi-label classification. . Now let&#39;s try Transformers . . Not those... . Original package . Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone. . 🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets then share them with the community on our model hub. At the same time, each python module defining an architecture can be used as a standalone and modified to enable quick research experiments. . 🤗 Transformers is backed by the two most popular deep learning libraries, PyTorch and TensorFlow, with a seamless integration between them, allowing you to train your models with one then load it for inference with the other. . . However, we will use the Simple Transformers package, which lets you quickly train and evaluate Transformer models. Keep the big guns for other projects and new PC... . . from simpletransformers.classification import MultiLabelClassificationModel import logging . logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(&quot;transformers&quot;) transformers_logger.setLevel(logging.WARNING) . train[&#39;labels&#39;] = train[labels].values.tolist() test[&#39;labels&#39;] = test[labels].values.tolist() . train_df = train[[&#39;text&#39;,&#39;labels&#39;]].copy() eval_df = test[[&#39;text&#39;,&#39;labels&#39;]].copy() . model = MultiLabelClassificationModel( &quot;roberta&quot;, &quot;roberta-base&quot;, num_labels=len(labels), use_cuda=True, args={&quot;reprocess_input_data&quot;: True, &quot;overwrite_output_dir&quot;: True, &quot;num_train_epochs&quot;: 5}, ) . INFO:filelock:Lock 140598834372336 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock INFO:filelock:Lock 140598834372336 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock INFO:filelock:Lock 140597941182416 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock INFO:filelock:Lock 140597941182416 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultiLabelSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;] - This IS expected if you are initializing RobertaForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForMultiLabelSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. INFO:filelock:Lock 140597903954216 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock INFO:filelock:Lock 140597903954216 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock INFO:filelock:Lock 140597938558176 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock INFO:filelock:Lock 140597938558176 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock . model.train_model(train_df) . INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used. INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_0_15970 /usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/. . (9985, 0.23426581945831798) . result, model_outputs, wrong_predictions = model.eval_model(eval_df) . INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used. INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_0_7867 . result . {&#39;LRAP&#39;: 0.8831122975015083, &#39;eval_loss&#39;: 0.28330832859690536} . preds = pd.DataFrame(model_outputs,columns=labels) preds . advert coi fanpov pr resume . 0 0.963867 | 0.109924 | 0.001567 | 0.044006 | 0.001279 | . 1 0.969727 | 0.089905 | 0.002100 | 0.036438 | 0.001156 | . 2 0.969238 | 0.091858 | 0.002035 | 0.037048 | 0.001165 | . 3 0.972168 | 0.069641 | 0.003004 | 0.031860 | 0.001057 | . 4 0.973145 | 0.062805 | 0.003622 | 0.029816 | 0.001040 | . ... ... | ... | ... | ... | ... | . 7862 0.938965 | 0.149658 | 0.001240 | 0.074341 | 0.001810 | . 7863 0.087708 | 0.089600 | 0.006958 | 0.053314 | 0.870605 | . 7864 0.970215 | 0.038910 | 0.007904 | 0.025513 | 0.001086 | . 7865 0.969238 | 0.035828 | 0.009193 | 0.025421 | 0.001078 | . 7866 0.971191 | 0.041138 | 0.007122 | 0.026810 | 0.001032 | . 7867 rows × 5 columns . print(metrics.classification_report(test[labels],preds.gt(.5).astype(int),zero_division=0)) . precision recall f1-score support 0 0.87 0.93 0.90 6239 1 0.00 0.00 0.00 707 2 0.60 0.43 0.50 493 3 0.30 0.02 0.03 500 4 0.55 0.53 0.54 726 micro avg 0.83 0.74 0.78 8665 macro avg 0.46 0.38 0.39 8665 weighted avg 0.72 0.74 0.72 8665 samples avg 0.81 0.77 0.78 8665 . Conclusions . We have played with those packages to solve the multi-label classificationt task . Binary Relevance Universal Sentence Encoder | Random Forest | XGBoost | . | Transformers | . We could see very intresting results, especially that each one of them works very diffrent from each other. . Personally I think using the XGBoost model with the USE worked the best in terms of accuracy-resources report, also it had a better precision for the label 1 where the other models could not score anything for that label. . All these models can be improved by tunning but this notebooks was mainly designed for learning purposes. .",
            "url": "https://cristianexer.github.io/blog/2021/02/05/Wiki-Articles-Multi-Label.html",
            "relUrl": "/2021/02/05/Wiki-Articles-Multi-Label.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Statistical Testing",
            "content": ". Aim: Acquire knowledge and experience in the area of statistical testing . Objctives: . Research and understand how and when to use core statisitcal testing methods | Ask and Answer questions that can be answered using one or more statistical tests on the dataset found | . Imports . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import scipy.stats as ss sns.set_style(&#39;whitegrid&#39;) plt.style.use(&#39;seaborn&#39;) . About Statistical Testing . Parametric vs Non-Parametric tests . What is the difference between a parametric and a non-parametric test ? . . Parametric tests assume underlying statistical distributions in the data, therefore, several conditions of validity must be met so that the result of a parametric test is reliable. . . Non-parametric tests do not rely on any distribution, thus can be applied even if parametric conditions of validity are not met. . . What is the advantage of using a non-parametric test ? . Non-parametric tests are more robust than parametric tests. In other words, they are valid in a broader range of situations (fewer conditions of validity). . What is the advantage of using a parametric test ? . The advantage of using a parametric test instead of a non-parametric equivalent is that the former will have more statistical power than the latter. . In other words, a parametric test is more able to lead to a rejection of H0. Most of the time, the p-value associated to a parametric test will be lower than the p-value associated to a nonparametric equivalent that is run on the same data. . . T test . . $t$ = Student&#39;s t-test . $m$ = mean . $ mu$ = theoretical value . $s$ = standard deviation . ${n}$ = variable set size . The Student’s t-Test is a statistical hypothesis test for testing whether two samples are expected to have been drawn from the same population. . It is named for the pseudonym “Student” used by William Gosset, who developed the test. . The test works by checking the means from two samples to see if they are significantly different from each other. It does this by calculating the standard error in the difference between means, which can be interpreted to see how likely the difference is, if the two samples have the same mean (the null hypothesis). . Good To Know . Works with small number of samples | If we compare 2 groups, they must have the same distribution | . t = observed difference between sample means / standard error of the difference between the means . Let&#39;s look at this example, we make 2 series where we draw random values from the normal(Gaussian) distribution . data = pd.DataFrame({ &#39;data1&#39;: np.random.normal(size=10), &#39;data2&#39;: np.random.normal(size=10) }) data[:3] . data1 data2 . 0 -0.164235 | 0.489534 | . 1 0.077709 | -0.226522 | . 2 -1.161500 | 0.277101 | . We can see that some of the values from both series overlap, so we can look for to see if there is a relationship by luck . Note: since this part is random next the you run the nb you might get different values . data.plot.hist(alpha=.8,figsize=(5,3)); . One Sample T test . data.describe().T . count mean std min 25% 50% 75% max . data1 10.0 | -0.564590 | 1.260859 | -2.200233 | -1.752491 | -0.32348 | 0.065744 | 1.766438 | . data2 10.0 | -0.035974 | 0.980542 | -1.355678 | -0.494399 | -0.11048 | 0.210083 | 2.282740 | . This is a two-sided test for the null hypothesis that the expected value (mean) of a sample of independent observations a is equal to the given population mean. . H0 = &#39;the population mean is equal to a mean of {}&#39; a = 0.05 hypothesized_population_mean = 1.5 stat, p = ss.ttest_1samp(data[&#39;data1&#39;],hypothesized_population_mean) print(f&#39;Statistic: {stat} nP-Value: {p:.4f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0.format(hypothesized_population_mean)}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0.format(hypothesized_population_mean)}&#39;) . Statistic: -5.1780647040905325 P-Value: 0.0006 Statistically significant / We can trust the statistic Reject H0: the population mean is equal to a mean of 1.5 . Unpaired T test . An unpaired t-test (also known as an independent t-test) is a statistical procedure that compares the averages/means of two independent or unrelated groups to determine if there is a significant difference between the two . H0 = &#39;the means of both populations are equal&#39; a = 0.05 stat, p = ss.ttest_ind(data[&#39;data1&#39;],data[&#39;data2&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: -1.0465644559040281 P-Value: 0.309 Statistically not significant / We cannot trust the statistic Accept H0: the means of both populations are equal . Paired T test . The paired sample t-test, sometimes called the dependent sample t-test, is a statistical procedure used to determine whether the mean difference between two sets of observations is zero. In a paired sample t-test, each subject or entity is measured twice, resulting in pairs of observations. . H0 = &#39;means difference between two sample is 0&#39; a = 0.05 stat, p = ss.ttest_rel(data[&#39;data1&#39;],data[&#39;data2&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: -1.0430434002617406 P-Value: 0.324 Statistically not significant / We cannot trust the statistic Accept H0: means difference between two sample is 0 . ANOVA test . ANOVA determines whether the groups created by the levels of the independent variable are statistically different by calculating whether the means of the treatment levels are different from the overall mean of the dependent variable. . The null hypothesis (H0) of ANOVA is that there is no difference among group means. . The alternate hypothesis (Ha) is that at least one group differs significantly from the overall mean of the dependent variable. . The assumptions of the ANOVA test are the same as the general assumptions for any parametric test: . Independence of observations: the data were collected using statistically-valid methods, and there are no hidden relationships among observations. If your data fail to meet this assumption because you have a confounding variable that you need to control for statistically, use an ANOVA with blocking variables. | Normally-distributed response variable: The values of the dependent variable follow a normal distribution. | Homogeneity of variance: The variation within each group being compared is similar for every group. If the variances are different among the groups, then ANOVA probably isn’t the right fit for the data. | . data.mean() . data1 -0.564590 data2 -0.035974 dtype: float64 . H0 = &#39;two or more groups have the same population mean&#39; a = 0.05 stat, p = ss.f_oneway(data[&#39;data1&#39;],data[&#39;data2&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: 1.0952971603616946 P-Value: 0.309 Statistically not significant / We cannot trust the statistic Accept H0: two or more groups have the same population mean . Linear Regression . It is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable). The variable we are using to predict the other variable&#39;s value is called the independent variable (or sometimes, the predictor variable). . The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (&lt; 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor&#39;s value are related to changes in the response variable. . Conversely, a larger (insignificant) p-value suggests that changes in the predictor are not associated with changes in the response. . Regression coefficients represent the mean change in the response variable for one unit of change in the predictor variable while holding other predictors in the model constant. This statistical control that regression provides is important because it isolates the role of one variable from all of the others in the model. . The key to understanding the coefficients is to think of them as slopes, and they’re often called slope coefficients. . reg_data = pd.DataFrame({ &#39;data1&#39;: np.random.gamma(25,size=20) }) reg_data[&#39;data2&#39;] = reg_data[&#39;data1&#39;].apply(lambda x: x + np.random.randint(1,25)) reg_data[:5] . data1 data2 . 0 21.403142 | 25.403142 | . 1 31.252916 | 35.252916 | . 2 22.933164 | 42.933164 | . 3 20.455040 | 30.455040 | . 4 25.948037 | 28.948037 | . sns.regplot(x=reg_data[&#39;data1&#39;],y=reg_data[&#39;data2&#39;]); . slope, intercept, r, p, se = ss.linregress(reg_data[&#39;data1&#39;], reg_data[&#39;data2&#39;]) print(f&#39;Slope: {slope} nIntercept: {intercept} nP-Value: {p} nr: {r} nse: {se}&#39;) H0 = &#39;changes in the predictor are not associated with changes in the response&#39; a = 0.05 if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Slope: 1.3536514855760986 Intercept: 2.1239248669953383 P-Value: 0.0007791282263931652 r: 0.6890350363826457 se: 0.33558640994910627 Statistically significant / We can trust the statistic Reject H0: changes in the predictor are not associated with changes in the response . Pearson correlation . The Pearson correlation coefficient measures the linear relationship between two datasets. Strictly speaking, Pearson’s correlation requires that each dataset be normally distributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. . Correlations of -1 or +1 imply an exact linear relationship. . Positive correlations imply that as x increases, so does y. . Negative correlations imply that as x increases, y decreases. . The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so. . H0 = &#39;the two variables are uncorrelated&#39; a = 0.05 stat, p = ss.pearsonr(data[&#39;data1&#39;],data[&#39;data2&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: -0.0069778154571835905 P-Value: 0.985 Statistically not significant / We cannot trust the statistic Accept H0: the two variables are uncorrelated . Chi-Square Test . There are two types of chi-square tests. Both use the chi-square statistic and distribution for different purposes: . A chi-square goodness of fit test determines if sample data matches a population . | A chi-square test for independence compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another. . A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. | A very large chi square test statistic means that the data does not fit very well. In other words, there isn’t a relationship. | . | . animals = [&#39;dog&#39;,&#39;cat&#39;,&#39;horse&#39;,&#39;dragon&#39;,&#39;unicorn&#39;] chi_data = pd.DataFrame({x:[np.random.randint(5,25) for _ in range(3)] for x in animals},index=[&#39;village1&#39;,&#39;village2&#39;,&#39;village3&#39;]) chi_data . dog cat horse dragon unicorn . village1 24 | 22 | 21 | 14 | 8 | . village2 17 | 14 | 10 | 22 | 5 | . village3 14 | 10 | 15 | 10 | 13 | . If our calculated value of chi-square is less or equal to the tabular(also called critical) value of chi-square, then H0 holds true. . H0 = &#39;no relation between the variables&#39; a = 0.05 stat, p = ss.chisquare(chi_data[&#39;dog&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: 2.8727272727272726 P-Value: 0.238 Statistically not significant / We cannot trust the statistic Accept H0: no relation between the variables . H0 = &#39;no relation between the variables&#39; a = 0.05 stat, p, dof, expected = ss.chi2_contingency(chi_data.values) print(f&#39;Statistic: {stat} nP-Value: {p:.3f} nDOF: {dof}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: 15.604347636638956 P-Value: 0.048 DOF: 8 Statistically significant / We can trust the statistic Reject H0: no relation between the variables . Fischer&#39;s test . Use the Fisher&#39;s exact test of independence when you have two nominal variables and you want to see whether the proportions of one variable are different depending on the value of the other variable. Use it when the sample size is small. . The null hypothesis is that the relative proportions of one variable are independent of the second variable; in other words, the proportions at one variable are the same for different values of the second variable . fisher_data = chi_data[1:].T[3:] fisher_data . village2 village3 . dragon 22 | 10 | . unicorn 5 | 13 | . H0 = &#39;the two groups are independet&#39; a = 0.05 oddsratio, expected = ss.fisher_exact(fisher_data) print(f&#39;Odds Ratio: {oddsratio} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Odds Ratio: 5.72 P-Value: 0.048 Statistically significant / We can trust the statistic Reject H0: the two groups are independet . Spearman&#39;s rank correlation . The Spearman rank-order correlation coefficient is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. . Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. . Correlations of -1 or +1 imply an exact monotonic relationship. . Positive correlations imply that as x increases, so does y. . Negative correlations imply that as x increases, y decreases. . The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. . H0 = &#39;the two variables are uncorrelated&#39; a = 0.05 stat, p = ss.spearmanr(data[&#39;data1&#39;],data[&#39;data2&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: -0.05454545454545454 P-Value: 0.881 Statistically not significant / We cannot trust the statistic Accept H0: the two variables are uncorrelated . References . Parametric and Non-Parametric tests | Choosing Between a Nonparametric Test and a Parametric Test | Parametric Non-parametric tests | Test Statistics | Choosing the Right Statistical Test | In Depth Linear Regression | Fisher&#39;s exact test | . Applying Statistical Tests . About Data . Dataset &amp; Data Dictionary . Statistical Themes: . Note: in total there are 75 fields the following are just themes the fields fall under . Home Owner Costs: Sum of utilities, property taxes. | Second Mortgage: Households with a second mortgage statistics. | Home Equity Loan: Households with a Home equity Loan statistics. | Debt: Households with any type of debt statistics. | Mortgage Costs: Statistics regarding mortgage payments, home equity loans, utilities and property taxes | Home Owner Costs: Sum of utilities, property taxes statistics | Gross Rent: Contract rent plus the estimated average monthly cost of utility features | Gross Rent as Percentof Income Gross rent: as the percent of income very interesting | High school Graduation: High school graduation statistics. | Population Demographics: Population demographic statistics. | Age Demographics: Age demographic statistics. | Household Income: Total income of people residing in the household. | Family Income: Total income of people related to the householder. | . df = pd.read_csv(&#39;/content/drive/MyDrive/Datasets/real_estate_db.csv&#39;,encoding=&#39;latin8&#39;) df[:3] . UID BLOCKID SUMLEVEL COUNTYID STATEID state state_ab city place type primary zip_code area_code lat lng ALand AWater pop male_pop female_pop rent_mean rent_median rent_stdev rent_sample_weight rent_samples rent_gt_10 rent_gt_15 rent_gt_20 rent_gt_25 rent_gt_30 rent_gt_35 rent_gt_40 rent_gt_50 universe_samples used_samples hi_mean hi_median hi_stdev hi_sample_weight hi_samples family_mean family_median family_stdev family_sample_weight family_samples hc_mortgage_mean hc_mortgage_median hc_mortgage_stdev hc_mortgage_sample_weight hc_mortgage_samples hc_mean hc_median hc_stdev hc_samples hc_sample_weight home_equity_second_mortgage second_mortgage home_equity debt second_mortgage_cdf home_equity_cdf debt_cdf hs_degree hs_degree_male hs_degree_female male_age_mean male_age_median male_age_stdev male_age_sample_weight male_age_samples female_age_mean female_age_median female_age_stdev female_age_sample_weight female_age_samples pct_own married married_snp separated divorced . 0 220336 | NaN | 140 | 16 | 2 | Alaska | AK | Unalaska | Unalaska City | City | tract | 99685 | 907 | 53.621091 | -166.770979 | 2823180154 | 3101986247 | 4619 | 2725 | 1894 | 1366.24657 | 1405.0 | 650.16380 | 131.50967 | 372.0 | 0.85676 | 0.65676 | 0.47838 | 0.35405 | 0.28108 | 0.21081 | 0.15135 | 0.12432 | 661 | 370 | 107394.63092 | 92807.0 | 70691.05352 | 329.85389 | 874.0 | 114330.20465 | 101229.0 | 63955.77136 | 161.15239 | 519.0 | 2266.22562 | 2283.0 | 768.53497 | 41.65644 | 155.0 | 840.67205 | 776.0 | 341.85580 | 58.0 | 29.74375 | 0.00469 | 0.01408 | 0.02817 | 0.72770 | 0.50216 | 0.77143 | 0.30304 | 0.82841 | 0.82784 | 0.82940 | 38.45838 | 39.25000 | 17.65453 | 709.06255 | 2725.0 | 32.78177 | 31.91667 | 19.31875 | 440.46429 | 1894.0 | 0.25053 | 0.47388 | 0.30134 | 0.03443 | 0.09802 | . 1 220342 | NaN | 140 | 20 | 2 | Alaska | AK | Eagle River | Anchorage | City | tract | 99577 | 907 | 61.174250 | -149.284329 | 509234898 | 1859309 | 3727 | 1780 | 1947 | 2347.69441 | 2351.0 | 382.73576 | 4.32064 | 44.0 | 0.79545 | 0.56818 | 0.56818 | 0.45455 | 0.20455 | 0.20455 | 0.20455 | 0.00000 | 50 | 44 | 136547.39117 | 119141.0 | 84268.79529 | 288.40934 | 1103.0 | 148641.70829 | 143026.0 | 69628.72286 | 159.20875 | 836.0 | 2485.10777 | 2306.0 | 919.76234 | 180.92883 | 797.0 | 712.33066 | 742.0 | 336.98847 | 256.0 | 159.32270 | 0.03609 | 0.06078 | 0.07407 | 0.75689 | 0.15520 | 0.56228 | 0.23925 | 0.94090 | 0.97253 | 0.91503 | 37.26216 | 39.33333 | 19.66765 | 503.83410 | 1780.0 | 38.97956 | 39.66667 | 20.05513 | 466.65478 | 1947.0 | 0.94989 | 0.52381 | 0.01777 | 0.00782 | 0.13575 | . 2 220343 | NaN | 140 | 20 | 2 | Alaska | AK | Jber | Anchorage | City | tract | 99505 | 907 | 61.284745 | -149.653973 | 270593047 | 66534601 | 8736 | 5166 | 3570 | 2071.30766 | 2089.0 | 442.89099 | 195.07816 | 1749.0 | 0.99469 | 0.97403 | 0.92680 | 0.89020 | 0.73022 | 0.62574 | 0.54368 | 0.32999 | 1933 | 1694 | 69361.23167 | 57976.0 | 45054.38537 | 1104.22753 | 1955.0 | 67678.50158 | 58248.0 | 38155.76319 | 1023.98149 | 1858.0 | NaN | NaN | NaN | NaN | NaN | 525.89101 | 810.0 | 392.27170 | 22.0 | 10.83444 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 1.00000 | 1.00000 | 1.00000 | 0.99097 | 0.99661 | 0.98408 | 21.96291 | 22.25000 | 11.09657 | 1734.05720 | 5166.0 | 22.20427 | 23.16667 | 13.86575 | 887.67805 | 3570.0 | 0.00759 | 0.50459 | 0.06676 | 0.01000 | 0.01838 | . cols = [&#39;state&#39;,&#39;state_ab&#39;,&#39;city&#39;,&#39;area_code&#39;,&#39;lat&#39;,&#39;lng&#39;,&#39;ALand&#39;,&#39;AWater&#39;,&#39;pop&#39;,&#39;male_pop&#39;,&#39;female_pop&#39;,&#39;debt&#39;,&#39;married&#39;,&#39;divorced&#39;,&#39;separated&#39;] cdf = df[cols].copy() . EDA . Fancy scatter plot by Latitutde and Longitude . sns.scatterplot(data=cdf,x=&#39;lat&#39;,y=&#39;lng&#39;,hue=&#39;state&#39;); plt.legend(bbox_to_anchor=(1.2, -0.1),fancybox=False, shadow=False, ncol=5); . Summary of population and area by state . summary = cdf.groupby(&#39;state_ab&#39;).agg({&#39;pop&#39;:&#39;sum&#39;,&#39;male_pop&#39;:&#39;sum&#39;,&#39;female_pop&#39;:&#39;sum&#39;,&#39;ALand&#39;:&#39;sum&#39;,&#39;AWater&#39;:&#39;sum&#39;,&#39;state&#39;:&#39;first&#39;}) summary . pop male_pop female_pop ALand AWater state . state_ab . AK 469126 | 246520 | 222606 | 722368907357 | 137757072654 | Alaska | . AL 2516214 | 1226877 | 1289337 | 70352962626 | 1732053991 | Alabama | . AR 1540462 | 758011 | 782451 | 69537343629 | 1484286648 | Arkansas | . AZ 3491125 | 1737284 | 1753841 | 180325081831 | 662716229 | Arizona | . CA 20197555 | 10081248 | 10116307 | 235174645131 | 4507004866 | California | . CO 2799395 | 1411330 | 1388065 | 155452913241 | 711290617 | Colorado | . CT 1914899 | 924516 | 990383 | 6865055353 | 312560243 | Connecticut | . DC 357842 | 172707 | 185135 | 89106477 | 13002616 | District of Columbia | . DE 488669 | 236762 | 251907 | 2877996732 | 114847579 | Delaware | . FL 10609057 | 5193417 | 5415640 | 69619448692 | 8393132389 | Florida | . GA 5412817 | 2641952 | 2770865 | 85141396708 | 2126459048 | Georgia | . HI 782728 | 397432 | 385296 | 6393739701 | 558912797 | Hawaii | . IA 1501895 | 743877 | 758018 | 70142911735 | 581858320 | Iowa | . ID 868080 | 433259 | 434821 | 92735376589 | 809101938 | Idaho | . IL 6588759 | 3239561 | 3349198 | 71027589926 | 956940364 | Illinois | . IN 3389012 | 1672018 | 1716994 | 48379840329 | 499474076 | Indiana | . KS 1602050 | 797338 | 804712 | 117291643373 | 734489089 | Kansas | . KY 2243143 | 1103713 | 1139430 | 50493602211 | 1170176450 | Kentucky | . LA 2454659 | 1197975 | 1256684 | 56404296747 | 6722140365 | Louisiana | . MA 3555026 | 1724503 | 1830523 | 10404663063 | 1035756604 | Massachusetts | . MD 3292145 | 1602210 | 1689935 | 14815063927 | 1511194979 | Maryland | . ME 710622 | 347816 | 362806 | 52878488979 | 5008597586 | Maine | . MI 5224877 | 2557263 | 2667614 | 75206589121 | 4940194318 | Michigan | . MN 2879177 | 1426107 | 1453070 | 84805439583 | 4311294489 | Minnesota | . MO 3092144 | 1518324 | 1573820 | 88816439192 | 1426397941 | Missouri | . MS 1558044 | 755465 | 802579 | 65062078135 | 1329197525 | Mississippi | . MT 564861 | 282401 | 282460 | 248905480934 | 2666670939 | Montana | . NC 5184944 | 2526445 | 2658499 | 65055897110 | 3226470120 | North Carolina | . ND 368821 | 188536 | 180285 | 95466673548 | 2182730770 | North Dakota | . NE 970077 | 482309 | 487768 | 89479501945 | 535697730 | Nebraska | . NH 694959 | 344063 | 350896 | 12491054114 | 514406119 | New Hampshire | . NJ 4482520 | 2182652 | 2299868 | 10493982942 | 799251245 | New Jersey | . NM 1203128 | 595608 | 607520 | 172251088200 | 546012923 | New Mexico | . NV 1459071 | 739535 | 719536 | 136008316579 | 1002763632 | Nevada | . NY 10394107 | 5032257 | 5361850 | 62316115428 | 2604980644 | New York | . OH 6139531 | 3010888 | 3128643 | 58182736607 | 788921631 | Ohio | . OK 2022316 | 1003736 | 1018580 | 98611541123 | 1915772469 | Oklahoma | . OR 2162139 | 1070676 | 1091463 | 159980775965 | 1918272471 | Oregon | . PA 6786776 | 3315641 | 3471135 | 58402582122 | 789727993 | Pennsylvania | . PR 1839485 | 883267 | 956218 | 4726099514 | 343252481 | Puerto Rico | . RI 527716 | 256423 | 271293 | 1421606369 | 173869644 | Rhode Island | . SC 2461759 | 1194553 | 1267206 | 41059983803 | 1362937733 | South Carolina | . SD 471349 | 236300 | 235049 | 109275231275 | 2004057966 | South Dakota | . TN 3367715 | 1643458 | 1724257 | 54101294407 | 1202156340 | Tennessee | . TX 13935294 | 6916078 | 7019216 | 358358562994 | 9644620956 | Texas | . UT 1598815 | 804287 | 794528 | 87867228560 | 2427239469 | Utah | . VA 4293918 | 2102950 | 2190968 | 53817438188 | 2389908012 | Virginia | . VT 313577 | 154456 | 159121 | 13268212222 | 507787768 | Vermont | . WA 3870279 | 1930122 | 1940157 | 95953804619 | 4024544353 | Washington | . WI 3009899 | 1494100 | 1515799 | 73736007342 | 4562686687 | Wisconsin | . WV 1045789 | 517338 | 528451 | 32769779880 | 272517541 | West Virginia | . WY 345577 | 175935 | 169642 | 124417325108 | 735843754 | Wyoming | . Male and Female distribution by state . summary[[&#39;male_pop&#39;,&#39;female_pop&#39;]] .div(summary[&#39;pop&#39;],axis=0) .plot.bar(stacked=True,rot=0,figsize=(25,5)) .legend(bbox_to_anchor=(0.58, -0.1),fancybox=False, shadow=False, ncol=2); . States distribution by Land and Water . plt.figure(figsize=(10,5)) sns.scatterplot(data=summary.reset_index(),x=&#39;ALand&#39;,y=&#39;AWater&#39;) for x in summary.reset_index().itertuples(): plt.annotate(x.state,(x.ALand,x.AWater),va=&#39;top&#39;,ha=&#39;right&#39;); . plt.figure(figsize=(25,7)) sns.scatterplot(data=summary[summary.state != &#39;Alaska&#39;],x=&#39;ALand&#39;,y=&#39;AWater&#39;); for x in summary[summary.state != &#39;Alaska&#39;].itertuples(): plt.annotate(x.state,(x.ALand,x.AWater),va=&#39;top&#39;,ha=&#39;right&#39;); . Let&#39;s find some hypotheses . First let&#39;s look if we have some correlations between our variables . corr_data = cdf.select_dtypes(exclude=[&#39;object&#39;]).drop([&#39;area_code&#39;,&#39;lat&#39;,&#39;lng&#39;],axis=1).copy().dropna() corr_matrix = pd.DataFrame([[ss.spearmanr(corr_data[c_x],corr_data[c_y])[1] for c_x in corr_data.columns] for c_y in corr_data.columns],columns=corr_data.columns,index=corr_data.columns) corr_matrix[corr_matrix == 0] = np.nan fig,ax = plt.subplots(1,2,figsize=(25,5)) sns.heatmap(corr_data.corr(method=&#39;spearman&#39;),annot=True,fmt=&#39;.2f&#39;,ax=ax[0],cmap=&#39;Blues&#39;) ax[0].set_title(&#39;Correlation Statistic&#39;) sns.heatmap(corr_matrix,annot=True,fmt=&#39;.2f&#39;,ax=ax[1],cmap=&#39;Blues&#39;) ax[1].set_title(&#39;P-Values&#39;) plt.tight_layout() . H: Is there a corelation between the Land area and number of people - Expect to see the more land, the more people . sns.regplot(x=cdf[&#39;ALand&#39;],y=cdf[&#39;pop&#39;]); slope, intercept, r, p, se = ss.linregress(cdf[&#39;ALand&#39;],cdf[&#39;pop&#39;]) print(f&#39;Slope: {slope:.5f} nIntercept: {intercept:.5f} nP-Value: {p:.5f} nr: {r:.5f} nse: {se:.5f}&#39;) H0 = &#39;changes in the predictor are not associated with changes in the response&#39; a = 0.05 if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Slope: -0.00000 Intercept: 4338.79206 P-Value: 0.00000 r: -0.03189 se: 0.00000 Statistically significant / We can trust the statistic Reject H0: changes in the predictor are not associated with changes in the response . H: Is there a correlation between the percentage of debt and city population - Expect to see more debt where more people . H0 = &#39;the two variables are uncorrelated&#39; a = 0.05 tmp = cdf.dropna(subset=[&#39;pop&#39;,&#39;debt&#39;]) stat, p = ss.pearsonr(tmp[&#39;pop&#39;],tmp[&#39;debt&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: 0.2346457692137584 P-Value: 0.000 Statistically significant / We can trust the statistic Reject H0: the two variables are uncorrelated . H: There is a relationship between the male population and female population . sns.regplot(x=cdf[&#39;male_pop&#39;],y=cdf[&#39;female_pop&#39;]); slope, intercept, r, p, se = ss.linregress(cdf[&#39;male_pop&#39;],cdf[&#39;female_pop&#39;]) print(f&#39;Slope: {slope:.5f} nIntercept: {intercept:.5f} nP-Value: {p:.5f} nr: {r:.5f} nse: {se:.5f}&#39;) H0 = &#39;changes in the predictor are not associated with changes in the response&#39; a = 0.05 if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Slope: 0.90511 Intercept: 268.73639 P-Value: 0.00000 r: 0.91270 se: 0.00205 Statistically significant / We can trust the statistic Reject H0: changes in the predictor are not associated with changes in the response . H0 = &#39;two or more groups have the same population mean&#39; a = 0.05 stat, p = ss.f_oneway(cdf[&#39;male_pop&#39;],cdf[&#39;female_pop&#39;]) print(f&#39;Statistic: {stat} nP-Value: {p:.3f}&#39;) if p &lt;= a: print(&#39;Statistically significant / We can trust the statistic&#39;) print(f&#39;Reject H0: {H0}&#39;) else: print(&#39;Statistically not significant / We cannot trust the statistic&#39;) print(f&#39;Accept H0: {H0}&#39;) . Statistic: 70.82427410415444 P-Value: 0.000 Statistically significant / We can trust the statistic Reject H0: two or more groups have the same population mean . Conclusions . Statistical tests are great for hypothesis testing | Is very important to know what type of test to use and when to use it, but you can do that just by looking at your variables | . Follow up topics and questions: . Have a look on Harvey-Collier multiplier test for linearity | How do we meassure right the statistic from each test | When having a P-Value == 0 mens the test failed and when not | .",
            "url": "https://cristianexer.github.io/blog/2021/01/29/Statistical-Testing.html",
            "relUrl": "/2021/01/29/Statistical-Testing.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Image Segmentation on Skin Cancer",
            "content": ". About Data . Dataset . This set consists of 2357 images of malignant and benign oncological diseases, which were formed from The International Skin Imaging Collaboration (ISIC). . All images were sorted according to the classification taken with ISIC, and all subsets were divided into the same number of images, with the exception of melanomas and moles, whose images are slightly dominant. . The data set contains the following diseases: . actinic keratosis | basal cell carcinoma | dermatofibroma | melanoma | nevus | pigmented benign keratosis | seborrheic keratosis | squamous cell carcinoma | vascular lesion | . Imports . import os,re,zipfile # data manipulation packages import pandas as pd import numpy as np from PIL import Image # data viz import matplotlib.pyplot as plt import seaborn as sns plt.style.use(&#39;seaborn&#39;) sns.set_style(&#39;whitegrid&#39;) . Download the dataset from Kaggle . %%time zip_name = &#39;skin-cancer9-classesisic.zip&#39; if not os.path.exists(zip_name): os.environ[&#39;KAGGLE_USERNAME&#39;] = &quot;&quot; # username from the json file os.environ[&#39;KAGGLE_KEY&#39;] = &quot;&quot; # key from the json file !kaggle datasets download nodoubttome/skin-cancer9-classesisic . CPU times: user 19 µs, sys: 5 µs, total: 24 µs Wall time: 28.6 µs . Unzip the dataset . with zipfile.ZipFile(zip_name, &#39;r&#39;) as zip_ref: zip_ref.extractall(zip_name.split(&#39;.&#39;)[0]) . Paths to Train &amp; Test folders . train_path = &#39;/content/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train&#39; test_path = &#39;/content/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Test&#39; . Create a dataframe with filename, filepath and the disease type . train = list() for cls in os.listdir(train_path): for filename in os.listdir(os.path.join(train_path,cls)): train.append({ &#39;filename&#39;: filename, &#39;filepath&#39;: os.path.join(train_path,cls,filename), &#39;label&#39;: cls }) train = pd.DataFrame(train) train[:3] . filename filepath label . 0 ISIC_0025915.jpg | /content/skin-cancer9-classesisic/Skin cancer ... | pigmented benign keratosis | . 1 ISIC_0024947.jpg | /content/skin-cancer9-classesisic/Skin cancer ... | pigmented benign keratosis | . 2 ISIC_0026539.jpg | /content/skin-cancer9-classesisic/Skin cancer ... | pigmented benign keratosis | . p = (train.label.value_counts() / len(train)) p.plot.pie(cmap=&#39;Paired&#39;,autopct=&#39;%.2f&#39;); . How the image data looks like ? . let&#39;s pick a sample image of melanoma . img_name, img_path, img_label = train[train.label == &#39;melanoma&#39;][4:5].values.ravel() . now we want to load the image using the pillow package . img = Image.open(img_path) . then we want to convert the image object into a numpy array . img_arr = np.array(img) . finally we want to print our image . plt.figure(figsize=(7,7)) plt.imshow(img_arr); . Image as data . A RGB image has 3 color channels as the RGB (Red, Green, Blue). Those 3 channels are 3 different matrices of with pixel values inside, and overlaying them we can get a normal image as we know. . . let&#39;s try to extract and see the image through each one of this channels . img_arr.shape . (768, 1024, 3) . fig,(ax1, ax2, ax3) = plt.subplots(1,3,figsize=(30,7)) # red channel ax1.imshow(img_arr[:,:,0]) ax1.set_title(&#39;Red channel&#39;) # green channel ax2.imshow(img_arr[:,:,1]) ax2.set_title(&#39;Green channel&#39;) # blue channel ax3.imshow(img_arr[:,:,2]) ax3.set_title(&#39;Blue channel&#39;) plt.tight_layout() . here we can see that the clearest image of the melanoma is on the blue channel, for segmenting the melanoma from this image we want to convert the image into black and white . plt.figure(figsize=(7,7)) threshold = 85 # threshold for the binary mask max_val = 255 # biggest value for pixel # create a binary mask where values greater than threshold -&gt; True &amp; False mask * 255 will create 0 and 255 pixels img_bin = (img_arr[:,:,2] &lt; threshold) * max_val plt.imshow(img_bin); . import cv2 from skimage import segmentation . Erosion . For an erosion, you examine all of the pixels in a pixel neighbourhood that are touching the structuring element. If every non-zero pixel is touching a structuring element pixel that is 1, then the output pixel in the corresponding centre position with respect to the input is 1. If there is at least one non-zero pixel that does not touch a structuring pixel that is 1, then the output is 0. . plt.figure(figsize=(7,7)) kernel = np.ones((2,2),np.uint8) erosion = cv2.erode(img_bin.astype(np.uint8),kernel,iterations = 1) plt.imshow(erosion); . Dilation . Dilation is the opposite of erosion. If there is at least one non-zero pixel that touches a pixel in the structuring element that is 1, then the output is 1, else the output is 0. You can think of this as slightly enlarging object areas and making small islands bigger. . plt.figure(figsize=(7,7)) kernel = np.ones((2,2),np.uint8) dilation = cv2.dilate(img_bin.astype(np.uint8),kernel,iterations = 45) plt.imshow(dilation); . Results . Here we can see how we can highlight the mask for the cancer spots . fig,(ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,7)) ax1.imshow(img_arr); # draw contour around the mask ax2.imshow(segmentation.mark_boundaries(img_arr, np.ma.masked_where(erosion == 0, erosion))); # we reverse the pixels and show the eroded mask over the original image ax2.imshow(np.ma.masked_where(erosion == 0, erosion),&#39;RdBu&#39;, alpha=0.7, interpolation=&#39;none&#39;) # draw contour around the mask ax3.imshow(segmentation.mark_boundaries(img_arr, np.ma.masked_where(dilation == 0, dilation))); # we reverse the pixels and show the dilated mask over the original image ax3.imshow(np.ma.masked_where(dilation == 0, dilation),&#39;RdBu&#39;, alpha=0.7, interpolation=&#39;none&#39;) ax1.set_title(&#39;Original Image&#39;) ax2.set_title(&#39;Eroded Image&#39;) ax3.set_title(&#39;Dilated Image&#39;) plt.tight_layout(); .",
            "url": "https://cristianexer.github.io/blog/2020/12/20/Image-Segmentation.html",
            "relUrl": "/2020/12/20/Image-Segmentation.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Magic Mushroom 🍄",
            "content": "Imports . import os import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import seaborn as sns from matplotlib import pyplot as plt plt.style.use(&#39;seaborn&#39;) from sklearn import metrics from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.preprocessing import LabelEncoder import xgboost as xgb import shap . Data . Source . df = pd.read_csv(&#39;/kaggle/input/mushroom-classification/mushrooms.csv&#39;) df[:3] . class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color ... stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . 0 p | x | s | n | t | p | f | c | n | k | ... | s | w | w | p | w | o | p | k | s | u | . 1 e | x | s | y | t | a | f | c | b | k | ... | s | w | w | p | w | o | p | n | n | g | . 2 e | b | s | w | t | l | f | c | b | n | ... | s | w | w | p | w | o | p | n | n | m | . 3 rows × 23 columns . Pre-Processing . df = df.apply(LabelEncoder().fit_transform) df[:3] . class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color ... stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . 0 1 | 5 | 2 | 4 | 1 | 6 | 1 | 0 | 1 | 4 | ... | 2 | 7 | 7 | 0 | 2 | 1 | 4 | 2 | 3 | 5 | . 1 0 | 5 | 2 | 9 | 1 | 0 | 1 | 0 | 0 | 4 | ... | 2 | 7 | 7 | 0 | 2 | 1 | 4 | 3 | 2 | 1 | . 2 0 | 0 | 2 | 8 | 1 | 3 | 1 | 0 | 0 | 5 | ... | 2 | 7 | 7 | 0 | 2 | 1 | 4 | 3 | 2 | 3 | . 3 rows × 23 columns . Modeling . Features . features = list(set(df.columns.tolist()) - set([&#39;class&#39;])) target = &#39;class&#39; . Train/Test Split . train, test = train_test_split(df,test_size=0.33,random_state=42,stratify=df[target]) . Looking for best params . param_grid = { &quot;max_depth&quot; : [ 7, 14, 20,23], &quot;min_child_weight&quot; : [ 3, 5, 7 ], &quot;gamma&quot; : [ 0.1, 0.3], &quot;colsample_bytree&quot; : [ 0.3, 0.5 , 0.7 ], &quot;n_estimators&quot; : [ 100,300,600,1000 ], } . xgc = xgb.XGBClassifier() grid = GridSearchCV(xgc, param_grid, cv=3,verbose=10,n_jobs=-1) grid.fit(train[features],train[target]) . Fitting 3 folds for each of 288 candidates, totalling 864 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 5 tasks | elapsed: 4.3s [Parallel(n_jobs=-1)]: Done 10 tasks | elapsed: 6.0s [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 7.7s [Parallel(n_jobs=-1)]: Done 24 tasks | elapsed: 10.0s [Parallel(n_jobs=-1)]: Done 33 tasks | elapsed: 13.0s [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 16.2s [Parallel(n_jobs=-1)]: Done 53 tasks | elapsed: 19.5s [Parallel(n_jobs=-1)]: Done 64 tasks | elapsed: 23.2s [Parallel(n_jobs=-1)]: Done 77 tasks | elapsed: 26.8s [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 30.7s [Parallel(n_jobs=-1)]: Done 105 tasks | elapsed: 35.5s [Parallel(n_jobs=-1)]: Done 120 tasks | elapsed: 40.0s [Parallel(n_jobs=-1)]: Done 137 tasks | elapsed: 45.2s [Parallel(n_jobs=-1)]: Done 154 tasks | elapsed: 50.6s [Parallel(n_jobs=-1)]: Done 173 tasks | elapsed: 56.2s [Parallel(n_jobs=-1)]: Done 192 tasks | elapsed: 1.0min [Parallel(n_jobs=-1)]: Done 213 tasks | elapsed: 1.1min [Parallel(n_jobs=-1)]: Done 234 tasks | elapsed: 1.3min [Parallel(n_jobs=-1)]: Done 257 tasks | elapsed: 1.4min [Parallel(n_jobs=-1)]: Done 280 tasks | elapsed: 1.5min [Parallel(n_jobs=-1)]: Done 305 tasks | elapsed: 1.6min [Parallel(n_jobs=-1)]: Done 330 tasks | elapsed: 1.8min [Parallel(n_jobs=-1)]: Done 357 tasks | elapsed: 2.0min [Parallel(n_jobs=-1)]: Done 384 tasks | elapsed: 2.1min [Parallel(n_jobs=-1)]: Done 413 tasks | elapsed: 2.3min [Parallel(n_jobs=-1)]: Done 442 tasks | elapsed: 2.5min [Parallel(n_jobs=-1)]: Done 473 tasks | elapsed: 2.6min [Parallel(n_jobs=-1)]: Done 504 tasks | elapsed: 2.8min [Parallel(n_jobs=-1)]: Done 537 tasks | elapsed: 3.0min [Parallel(n_jobs=-1)]: Done 570 tasks | elapsed: 3.2min [Parallel(n_jobs=-1)]: Done 605 tasks | elapsed: 3.5min [Parallel(n_jobs=-1)]: Done 640 tasks | elapsed: 3.7min [Parallel(n_jobs=-1)]: Done 677 tasks | elapsed: 3.9min [Parallel(n_jobs=-1)]: Done 714 tasks | elapsed: 4.2min [Parallel(n_jobs=-1)]: Done 753 tasks | elapsed: 4.4min [Parallel(n_jobs=-1)]: Done 792 tasks | elapsed: 4.7min [Parallel(n_jobs=-1)]: Done 833 tasks | elapsed: 5.0min [Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 5.2min finished . GridSearchCV(cv=3, estimator=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None, colsample_bynode=None, colsample_bytree=None, gamma=None, gpu_id=None, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=None, max_delta_step=None, max_depth=None, min_child_weight=None, missing=nan, monotone_constraints=None, n_estimators=100, n_jobs=None, num_parallel_tree=None, random_state=None, reg_alpha=None, reg_lambda=None, scale_pos_weight=None, subsample=None, tree_method=None, validate_parameters=None, verbosity=None), n_jobs=-1, param_grid={&#39;colsample_bytree&#39;: [0.3, 0.5, 0.7], &#39;gamma&#39;: [0.1, 0.3], &#39;max_depth&#39;: [7, 14, 20, 23], &#39;min_child_weight&#39;: [3, 5, 7], &#39;n_estimators&#39;: [100, 300, 600, 1000]}, verbose=10) . Best params . grid.best_params_ . {&#39;colsample_bytree&#39;: 0.3, &#39;gamma&#39;: 0.1, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 3, &#39;n_estimators&#39;: 100} . Results . results = test.copy() results[&#39;y_pred&#39;] = grid.best_estimator_.predict(test[features]) . Classification Report . print(metrics.classification_report(results[target],results[&#39;y_pred&#39;])) . precision recall f1-score support 0 1.00 1.00 1.00 1389 1 1.00 1.00 1.00 1292 accuracy 1.00 2681 macro avg 1.00 1.00 1.00 2681 weighted avg 1.00 1.00 1.00 2681 . Confusion Matrix . sns.heatmap(metrics.confusion_matrix(results[target],results[&#39;y_pred&#39;]),annot=True,fmt=&#39;d&#39;); . How does the model think ? . fig,ax = plt.subplots(1,1,figsize=(20,20)) xgb.plot_tree(grid.best_estimator_,num_trees=10,ax=ax); . Shap Values / Feature Impact . expl = shap.TreeExplainer(grid.best_estimator_) . shap_values = expl.shap_values(test[features],test[target]) . shap.summary_plot(shap_values,test[features]) .",
            "url": "https://cristianexer.github.io/blog/2020/10/31/Magic-mushroom.html",
            "relUrl": "/2020/10/31/Magic-mushroom.html",
            "date": " • Oct 31, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Quora Questions Deduplication",
            "content": "Imports . Aim: Create a baseline model in PyTorch to solve the Deduplication Task . import os,re,zipfile import pandas as pd import numpy as np from types import SimpleNamespace from matplotlib import pyplot as plt import itertools plt.style.use(&#39;dark_background&#39;) plt.style.use(&#39;seaborn&#39;) # Torch import torch import torch.nn as nn import torch.nn.functional as F torch.backends.cudnn.deterministic = True # metrics from sklearn import metrics # Data processing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from tqdm.notebook import tqdm . Set PyTorch to use GPU if there is one . dev = torch.device(&#39;cuda&#39;) dev . device(type=&#39;cuda&#39;) . Download the dataset using Kaggle credentials . %%time zip_name = &#39;train.csv.zip&#39; if not os.path.exists(zip_name): os.environ[&#39;KAGGLE_USERNAME&#39;] = &quot;&quot; # username from the json file os.environ[&#39;KAGGLE_KEY&#39;] = &quot;&quot; # key from the json file !kaggle competitions download -c quora-question-pairs . CPU times: user 238 µs, sys: 55 µs, total: 293 µs Wall time: 227 µs . Load data . df = pd.read_csv(&#39;train.csv.zip&#39;)[[&#39;question1&#39;,&#39;question2&#39;,&#39;is_duplicate&#39;]].dropna() dups = df[df.is_duplicate == 1].copy() no_dups = df[df.is_duplicate == 0].copy() split_fact = 1.3 df = pd.concat([dups[:int(len(dups) / split_fact)], no_dups[:int(len(no_dups) / split_fact)]],ignore_index=True) df . question1 question2 is_duplicate . 0 Astrology: I am a Capricorn Sun Cap moon and c... | I&#39;m a triple Capricorn (Sun, Moon and ascendan... | 1 | . 1 How can I be a good geologist? | What should I do to be a great geologist? | 1 | . 2 How do I read and find my YouTube comments? | How can I see all my Youtube comments? | 1 | . 3 What can make Physics easy to learn? | How can you make physics easy to learn? | 1 | . 4 What was your first sexual experience like? | What was your first sexual experience? | 1 | . ... ... | ... | ... | . 310984 How do I develop a positive attitude towards e... | How can one develop a positive attitude toward... | 0 | . 310985 How many times have sex in week? | How many times did you have sex in one day? | 0 | . 310986 If something died in outer space, would its bo... | Are there any dead astronaut bodies floating a... | 0 | . 310987 Are American social racial issues self propaga... | Are we the direct cause of racism by constantl... | 0 | . 310988 What are the names of the best games that can ... | How can I make the most of Nvidia GT 940M 2GB ... | 0 | . 310989 rows × 3 columns . Initialize a class for the USE . import tensorflow as tf import tensorflow_hub as hub class UniversalSentenceEncoder: def __init__(self, encoder=&#39;universal-sentence-encoder&#39;, version=&#39;4&#39;): self.version = version self.encoder = encoder self.embd = hub.load(f&quot;https://tfhub.dev/google/{encoder}/{version}&quot;,) def embed(self, sentences): return self.embd(sentences) def squized(self, sentences): return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string)))) . ue = UniversalSentenceEncoder() . Train/Test split the data . train,test = train_test_split(df,test_size=0.33,random_state=42,stratify=df[&#39;is_duplicate&#39;]) . Convert the arrays to Tensors and specify the usage of GPU . %%time x_q1_train = torch.from_numpy(ue.squized(train[&#39;question1&#39;].values)).type(torch.FloatTensor).to(dev) x_q2_train = torch.from_numpy(ue.squized(train[&#39;question2&#39;].values)).type(torch.FloatTensor).to(dev) y_train = torch.from_numpy(train[&#39;is_duplicate&#39;].values).type(torch.LongTensor).to(dev) x_q1_test = torch.from_numpy(ue.squized(test[&#39;question1&#39;].values)).type(torch.FloatTensor).to(dev) x_q2_test = torch.from_numpy(ue.squized(test[&#39;question2&#39;].values)).type(torch.FloatTensor).to(dev) y_test = torch.from_numpy(test[&#39;is_duplicate&#39;].values).type(torch.LongTensor).to(dev) . CPU times: user 1min 47s, sys: 2.95 s, total: 1min 50s Wall time: 1min 39s . Use the data loader to make things easier on training . b_size = 256 train_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_q1_train,x_q2_train,y_train), batch_size=b_size, shuffle=True) test_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_q1_test,x_q2_test,y_test), batch_size=b_size, shuffle=True) . Finally we do all the training in one class in the same cell for the ease of making chagnes . class Net(nn.Module): def __init__(self): super(Net,self).__init__() self.q1_lin = nn.Linear(in_features=512,out_features=1024) self.q2_lin = nn.Linear(in_features=512,out_features=1024) self.lin1 = nn.Linear(in_features=2048,out_features=1024) self.lin2 = nn.Linear(in_features=1024,out_features=512) self.lin3 = nn.Linear(in_features=512,out_features=256) self.lin4 = nn.Linear(in_features=256,out_features=128) self.lin5 = nn.Linear(in_features=128,out_features=2) # here we take the input data and pass it through the chain of layers def forward(self,q1,q2): q1 = self.q1_lin(q1) q2 = self.q2_lin(q2) x = torch.cat((q1,q2),dim=1) # print(x.size()) x = self.lin1(x) x = self.lin2(x) x = self.lin3(x) x = self.lin4(x) x = self.lin5(x) return x # instance our model model = Net().to(dev) # set the number of epochs epochs = 100 # criterion aka loss function -&gt; find more on pytorch doc criterion = nn.CrossEntropyLoss() # optimizer optimizer = torch.optim.Adam(model.parameters(), lr=0.003) # create 3 lists to store the losses and accuracy at each epoch train_losses, test_losses, accuracy = [0]*epochs, [0]*epochs,[0]*epochs # in this current case we don&#39;t use batches for training and we pass the whole data at each epoch for e in tqdm(range(epochs)): for q1,q2,label in train_dl: optimizer.zero_grad() q1 = q1.float() q2 = q2.float() # Comput train loss y_pred = model(q1,q2) loss = criterion(y_pred, label) loss.backward() optimizer.step() # store train loss train_losses[e] = loss.item() for q1,q2,label in test_dl: # Compute the test stats with torch.no_grad(): # Turn on all the nodes model.eval() q1 = q1.float() q2 = q2.float() # Comput test loss ps = model(q1,q2) loss = criterion(ps, label) # store test loss test_losses[e] = loss.item() # # Compute accuracy top_p, top_class = ps.topk(1, dim=1) equals = (top_class == label.view(*top_class.shape)) # # store accuracy accuracy[e] = torch.mean(equals.type(torch.FloatTensor)) # Print the final information print(f&#39;Accuracy : {100*accuracy[-1].item():0.2f}%&#39;) print(f&#39;Train loss: {train_losses[-1]}&#39;) print(f&#39;Test loss : {test_losses[-1]}&#39;) # Plot the results fig,ax = plt.subplots(1,2,figsize=(20,5)) ax[0].set_ylabel(&#39;Accuracy&#39;) ax[0].set_xlabel(&#39;Epochs&#39;) ax[0].set_title(&#39;Model Accuracy&#39;) ax[0].plot(accuracy) ax[1].set_ylabel(&#39;Loss&#39;) ax[1].set_xlabel(&#39;Epochs&#39;) ax[1].set_title(&#39;Train/Test Losses&#39;) ax[1].plot(train_losses, label=&#39;train&#39;) ax[1].plot(test_losses, label=&#39;test&#39;) ax[1].legend() plt.tight_layout() . Accuracy : 66.96% Train loss: 0.5459461212158203 Test loss : 0.6327708959579468 .",
            "url": "https://cristianexer.github.io/blog/2020/10/11/Quora-Questions-Deduplication.html",
            "relUrl": "/2020/10/11/Quora-Questions-Deduplication.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Titanic Torch 🔥",
            "content": ". This notebook aims to solve the titanic classification problem using pytorch . Imports . import pandas as pd import numpy as np # Viz from matplotlib import pyplot as plt import seaborn as sns plt.style.use(&#39;seaborn&#39;) # Torch import torch import torch.nn as nn import torch.nn.functional as F torch.backends.cudnn.deterministic = True # metrics from sklearn import metrics # Data processing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler . Data . df = pd.read_csv(&#39;https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv&#39;) df.columns = df.columns.str.lower().str.replace(&#39;/&#39;,&#39;_&#39;).str.replace(&#39; &#39;,&#39;_&#39;) # drop. the name df = df.drop([&#39;name&#39;],axis=1) # encode the values from the sex column df.sex = df.sex.replace({ &#39;male&#39;:1, &#39;female&#39;: 0 }) df[:3] . survived pclass sex age siblings_spouses_aboard parents_children_aboard fare . 0 0 | 3 | 1 | 22.0 | 1 | 0 | 7.2500 | . 1 1 | 1 | 0 | 38.0 | 1 | 0 | 71.2833 | . 2 1 | 3 | 0 | 26.0 | 0 | 0 | 7.9250 | . Modelling . scaler = StandardScaler() . Set the target and features . target = &#39;survived&#39; features = [&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;siblings_spouses_aboard&#39;,&#39;parents_children_aboard&#39;, &#39;fare&#39;] . Apply scaler on features . df[features] = scaler.fit_transform(df[features]) . Split data into train/test . train,test = train_test_split(df,test_size=0.33,random_state=42,stratify=df[target]) . From pandas/numpy to Tensors (types are important) . x_train = torch.from_numpy(train[features].values).type(torch.FloatTensor) y_train = torch.from_numpy(train[target].values).type(torch.LongTensor) x_test = torch.from_numpy(test[features].values).type(torch.FloatTensor) y_test = torch.from_numpy(test[target].values).type(torch.LongTensor) . Keep this part into one cell, it willl make the model development and testing easier . class Net(nn.Module): def __init__(self): super(Net,self).__init__() # takes an imput of 6 features, and spits out a vector of size 256 self.lin1 = nn.Linear(in_features=6,out_features=256,bias=True) # the second layer takes the 256 vector and process it into a vector of size 64 self.lin2 = nn.Linear(in_features=256,out_features=64,bias=True) # the last layer takes the 64 size vector and returns the output vector which 2 == number of classes self.lin3 = nn.Linear(in_features=64,out_features=2,bias=True) # here we take the input data and pass it through the chain of layers def forward(self,input): x = self.lin1(input) x = self.lin2(x) x = self.lin3(x) return x # instance our model model = Net() # set the number of epochs epochs = 100 # criterion aka loss function -&gt; find more on pytorch doc criterion = nn.CrossEntropyLoss() # optimizer optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # create 3 lists to store the losses and accuracy at each epoch train_losses, test_losses, accuracy = [0]*epochs, [0]*epochs,[0]*epochs # in this current case we don&#39;t use batches for training and we pass the whole data at each epoch for e in range(epochs): optimizer.zero_grad() # Comput train loss y_pred = model(x_train) loss = criterion(y_pred, y_train) loss.backward() optimizer.step() # store train loss train_losses[e] = loss.item() # Compute the test stats with torch.no_grad(): # Turn on all the nodes model.eval() # Comput test loss ps = model(x_test) loss = criterion(ps, y_test) # store test loss test_losses[e] = loss.item() # Compute accuracy top_p, top_class = ps.topk(1, dim=1) equals = (top_class == y_test.view(*top_class.shape)) # store accuracy accuracy[e] = torch.mean(equals.type(torch.FloatTensor)) # Print the final information print(f&#39;Accuracy : {100*accuracy[-1].item():0.2f}%&#39;) print(f&#39;Train loss: {train_losses[-1]}&#39;) print(f&#39;Test loss : {test_losses[-1]}&#39;) # Plot the results fig,ax = plt.subplots(1,2,figsize=(20,5)) ax[0].set_ylabel(&#39;Accuracy&#39;) ax[0].set_xlabel(&#39;Epochs&#39;) ax[0].set_title(&#39;Model Accuracy&#39;) ax[0].plot(accuracy) ax[1].set_ylabel(&#39;Loss&#39;) ax[1].set_xlabel(&#39;Epochs&#39;) ax[1].set_title(&#39;Train/Test Losses&#39;) ax[1].plot(train_losses, label=&#39;train&#39;) ax[1].plot(test_losses, label=&#39;test&#39;) ax[1].legend() plt.tight_layout() . Accuracy : 80.89% Train loss: 0.4383288621902466 Test loss : 0.4455047845840454 . Classificaiton Report . print(metrics.classification_report(test[target],top_class.numpy().ravel())) . precision recall f1-score support 0 0.83 0.87 0.85 180 1 0.78 0.71 0.74 113 accuracy 0.81 293 macro avg 0.80 0.79 0.79 293 weighted avg 0.81 0.81 0.81 293 . Confusion Matrix . sns.heatmap(metrics.confusion_matrix(test[target],top_class.numpy().ravel()),fmt=&#39;d&#39;,annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f427520ed68&gt; .",
            "url": "https://cristianexer.github.io/blog/2020/10/10/TitanicTorch.html",
            "relUrl": "/2020/10/10/TitanicTorch.html",
            "date": " • Oct 10, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "nCov-2019 EDA",
            "content": "%%time import os f_name = &#39;novel-corona-virus-2019-dataset.zip&#39; if not os.path.exists(f_name): os.environ[&#39;KAGGLE_USERNAME&#39;] = &quot;&quot; # username from the json file os.environ[&#39;KAGGLE_KEY&#39;] = &quot;&quot; # key from the json file !kaggle datasets download -d sudalairajkumar/novel-corona-virus-2019-dataset . CPU times: user 325 µs, sys: 65 µs, total: 390 µs Wall time: 605 µs . import zipfile import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt import networkx as nx . df = dict() with zipfile.ZipFile(f_name) as z: for fz in z.namelist(): with z.open(fz) as f: df[fz.split(&#39;.&#39;)[0]] = pd.read_csv(f) . df.keys() . dict_keys([&#39;COVID19_line_list_data&#39;, &#39;COVID19_open_line_list&#39;, &#39;covid_19_data&#39;, &#39;time_series_covid_19_confirmed&#39;, &#39;time_series_covid_19_deaths&#39;, &#39;time_series_covid_19_recovered&#39;]) . First Preview over the data . conf = df.get(&#39;time_series_covid_19_confirmed&#39;) dth = df.get(&#39;time_series_covid_19_deaths&#39;) recov = df.get(&#39;time_series_covid_19_recovered&#39;) . conf[:5] . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 2/1/20 2/2/20 2/3/20 2/4/20 2/5/20 2/6/20 2/7/20 2/8/20 2/9/20 2/10/20 2/11/20 2/12/20 2/13/20 2/14/20 2/15/20 2/16/20 2/17/20 2/18/20 2/19/20 2/20/20 2/21/20 2/22/20 2/23/20 2/24/20 2/25/20 2/26/20 2/27/20 2/28/20 2/29/20 3/1/20 3/2/20 3/3/20 3/4/20 3/5/20 3/6/20 3/7/20 3/8/20 3/9/20 3/10/20 3/11/20 3/12/20 3/13/20 3/14/20 3/15/20 3/16/20 3/17/20 3/18/20 3/19/20 3/20/20 3/21/20 3/22/20 3/23/20 3/24/20 3/25/20 3/26/20 3/27/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 4 | 4 | 5 | 7 | 7 | 7 | 11 | 16 | 21 | 22 | 22 | 22 | 24 | 24 | 40 | 40 | 74 | 84 | 94 | 110 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 10 | 12 | 23 | 33 | 38 | 42 | 51 | 55 | 59 | 64 | 70 | 76 | 89 | 104 | 123 | 146 | 174 | 186 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 5 | 12 | 12 | 17 | 17 | 19 | 20 | 20 | 20 | 24 | 26 | 37 | 48 | 54 | 60 | 74 | 87 | 90 | 139 | 201 | 230 | 264 | 302 | 367 | 409 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 2 | 39 | 39 | 53 | 75 | 88 | 113 | 133 | 164 | 188 | 224 | 267 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 3 | 3 | 3 | 4 | 4 | . dth[:5] . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 2/1/20 2/2/20 2/3/20 2/4/20 2/5/20 2/6/20 2/7/20 2/8/20 2/9/20 2/10/20 2/11/20 2/12/20 2/13/20 2/14/20 2/15/20 2/16/20 2/17/20 2/18/20 2/19/20 2/20/20 2/21/20 2/22/20 2/23/20 2/24/20 2/25/20 2/26/20 2/27/20 2/28/20 2/29/20 3/1/20 3/2/20 3/3/20 3/4/20 3/5/20 3/6/20 3/7/20 3/8/20 3/9/20 3/10/20 3/11/20 3/12/20 3/13/20 3/14/20 3/15/20 3/16/20 3/17/20 3/18/20 3/19/20 3/20/20 3/21/20 3/22/20 3/23/20 3/24/20 3/25/20 3/26/20 3/27/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 4 | 4 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 2 | 2 | 2 | 2 | 2 | 4 | 5 | 5 | 6 | 8 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 2 | 3 | 4 | 4 | 4 | 7 | 9 | 11 | 15 | 17 | 17 | 19 | 21 | 25 | 26 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 3 | 3 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . recov[:5] . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 2/1/20 2/2/20 2/3/20 2/4/20 2/5/20 2/6/20 2/7/20 2/8/20 2/9/20 2/10/20 2/11/20 2/12/20 2/13/20 2/14/20 2/15/20 2/16/20 2/17/20 2/18/20 2/19/20 2/20/20 2/21/20 2/22/20 2/23/20 2/24/20 2/25/20 2/26/20 2/27/20 2/28/20 2/29/20 3/1/20 3/2/20 3/3/20 3/4/20 3/5/20 3/6/20 3/7/20 3/8/20 3/9/20 3/10/20 3/11/20 3/12/20 3/13/20 3/14/20 3/15/20 3/16/20 3/17/20 3/18/20 3/19/20 3/20/20 3/21/20 3/22/20 3/23/20 3/24/20 3/25/20 3/26/20 3/27/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 2 | 2 | 2 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 2 | 2 | 10 | 17 | 17 | 31 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8 | 8 | 12 | 12 | 12 | 12 | 12 | 32 | 32 | 32 | 65 | 65 | 24 | 65 | 29 | 29 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . conf_time_cols = conf.columns.tolist()[4:] dth_time_cols = dth.columns.tolist()[4:] recov_time_cols = recov.columns.tolist()[4:] . conf_graph = nx.Graph() dth_graph = nx.Graph() recov_graph = nx.Graph() . for i in range(conf.__len__()): conf_graph.add_edge(conf.iloc[i][&#39;Country/Region&#39;],conf.iloc[i][&#39;Province/State&#39;],relation=&#39;state-level&#39;) conf_graph.add_edge(conf.iloc[i][&#39;Province/State&#39;],conf.iloc[i][conf_time_cols[-1]],relation=&#39;number-level&#39;) . Confirmed Cases at a Country-State Level . plt.figure(figsize=(50,50)) nx.draw_networkx(conf_graph) . Death Cases at a Country-State Level . for i in range(dth.__len__()): dth_graph.add_edge(dth.iloc[i][&#39;Country/Region&#39;],dth.iloc[i][&#39;Province/State&#39;],relation=&#39;state-level&#39;) dth_graph.add_edge(conf.iloc[i][&#39;Province/State&#39;],conf.iloc[i][dth_time_cols[-1]],relation=&#39;number-level&#39;) . plt.figure(figsize=(50,50)) nx.draw_networkx(dth_graph) . Recovered Cases at a Country-State Level . for i in range(recov.__len__()): recov_graph.add_edge(dth.iloc[i][&#39;Country/Region&#39;],dth.iloc[i][&#39;Province/State&#39;],relation=&#39;state-level&#39;) recov_graph.add_edge(conf.iloc[i][&#39;Province/State&#39;],conf.iloc[i][recov_time_cols[-1]],relation=&#39;number-level&#39;) . plt.figure(figsize=(50,50)) nx.draw_networkx(recov_graph) . Overview over the Confirmed Cases by time . conf[conf_time_cols].describe().boxplot(figsize=(55,20)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd758281940&gt; . Overview over the Death Cases by time . dth[dth_time_cols].describe().boxplot(figsize=(55,20)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd758800d30&gt; . Overview over the Recovered Cases by time . recov[recov_time_cols].describe().boxplot(figsize=(55,20)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd75604aa58&gt; . Latest Confirmed Cases Grouped by Country/Region and Province/State . conf.loc[:,[&#39;Country/Region&#39;,&#39;Province/State&#39;,conf_time_cols[-1]]].groupby([&#39;Country/Region&#39;,&#39;Province/State&#39;]).agg({ conf_time_cols[-1]: &#39;sum&#39; }) . 3/27/20 . Country/Region Province/State . Australia Australian Capital Territory 62 | . New South Wales 1405 | . Northern Territory 12 | . Queensland 555 | . South Australia 257 | . ... ... ... | . United Kingdom Cayman Islands 8 | . Channel Islands 88 | . Gibraltar 55 | . Isle of Man 29 | . Montserrat 5 | . 76 rows × 1 columns . Latest Death Cases Grouped by Country/Region and Province/State . dth.loc[:,[&#39;Country/Region&#39;,&#39;Province/State&#39;,dth_time_cols[-1]]].groupby([&#39;Country/Region&#39;,&#39;Province/State&#39;]).agg({ dth_time_cols[-1]: &#39;sum&#39; }) . 3/27/20 . Country/Region Province/State . Australia Australian Capital Territory 0 | . New South Wales 7 | . Northern Territory 0 | . Queensland 1 | . South Australia 0 | . ... ... ... | . United Kingdom Cayman Islands 1 | . Channel Islands 1 | . Gibraltar 0 | . Isle of Man 0 | . Montserrat 0 | . 76 rows × 1 columns . Latest Recovered Cases Grouped by Country/Region and Province/State . recov.loc[:,[&#39;Country/Region&#39;,&#39;Province/State&#39;,recov_time_cols[-1]]].groupby([&#39;Country/Region&#39;,&#39;Province/State&#39;]).agg({ recov_time_cols[-1]: &#39;sum&#39; }) . 3/27/20 . Country/Region Province/State . Australia Australian Capital Territory 1 | . New South Wales 4 | . Northern Territory 0 | . Queensland 8 | . South Australia 6 | . ... ... ... | . United Kingdom Cayman Islands 0 | . Channel Islands 0 | . Gibraltar 14 | . Isle of Man 0 | . Montserrat 0 | . 61 rows × 1 columns . total_cases = conf[conf_time_cols[-1]].sum() total_deaths = dth[dth_time_cols[-1]].sum() total_recovs = recov[recov_time_cols[-1]].sum() . total_cases, total_deaths, total_recovs . (593291, 27198, 130915) . death_rate = (total_deaths / total_cases) * 100.0 recov_rate = (total_recovs / total_cases) * 100.0 . plt.figure(figsize=(10,10)) objects = (&#39;Death Ratio&#39;, &#39;Recover Ratio&#39;) y_pos = np.arange(len(objects)) performance = [death_rate,recov_rate] plt.bar(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.xticks(y_pos, objects) plt.ylabel(&#39;%&#39;) plt.title(&#39;Death vs Recover Ratio&#39;) plt.show() . plt.figure(figsize=(10,10)) objects = (&#39;Total Confirmed&#39;,&#39;Total Deaths&#39;, &#39;Total Recovered&#39;) y_pos = np.arange(len(objects)) performance = [total_cases, total_deaths, total_recovs] plt.bar(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.xticks(y_pos, objects) plt.title(&quot;Numbers of &quot; + &quot; - &quot;.join(objects)) plt.show() . import plotly.express as px . conf[&#39;cases&#39;] = conf[conf_time_cols[-1]] dth[&#39;cases&#39;] = dth[dth_time_cols[-1]] recov[&#39;cases&#39;] = recov[recov_time_cols[-1]] . hov_data = [&quot;Province/State&quot;, &quot;Country/Region&quot;,&quot;cases&quot;] . Map of Confirmed cases on the last available day . plt.figure(figsize=(25,25)) fig = px.scatter_mapbox(conf, lat=&quot;Lat&quot;, lon=&quot;Long&quot;, hover_name=&quot;Province/State&quot;, hover_data=hov_data, color_discrete_sequence=[&quot;fuchsia&quot;],zoom=3) fig.update_layout(mapbox_style=&quot;carto-darkmatter&quot;) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . . . &lt;Figure size 1800x1800 with 0 Axes&gt; . Map of Death cases on the last available day . plt.figure(figsize=(50,50)) fig = px.scatter_mapbox(dth, lat=&quot;Lat&quot;, lon=&quot;Long&quot;, hover_name=&quot;Province/State&quot;, hover_data=hov_data, color_discrete_sequence=[&quot;fuchsia&quot;],zoom=3) fig.update_layout(mapbox_style=&quot;carto-darkmatter&quot;) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . . . &lt;Figure size 3600x3600 with 0 Axes&gt; . Map of Recovered cases on the last available day . plt.figure(figsize=(25,25)) fig = px.scatter_mapbox(recov, lat=&quot;Lat&quot;, lon=&quot;Long&quot;, hover_name=&quot;Province/State&quot;, hover_data=hov_data, color_discrete_sequence=[&quot;fuchsia&quot;],zoom=3) fig.update_layout(mapbox_style=&quot;carto-darkmatter&quot;) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . . . &lt;Figure size 1800x1800 with 0 Axes&gt; .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/nCov-2019-EDA.html",
            "relUrl": "/2020/10/09/nCov-2019-EDA.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Wine Quality",
            "content": ". Imports . import os,re,zipfile import pandas as pd import numpy as np from matplotlib import pyplot as plt plt.style.use(&#39;dark_background&#39;) plt.style.use(&#39;seaborn&#39;) from sklearn.model_selection import train_test_split,GridSearchCV from sklearn import metrics import xgboost as xgb . Data . fixed acidity | volatile acidity | citric acid | residual sugar | chlorides | free sulfur dioxide | total sulfur dioxide | density | pH | sulphates | alcohol Output variable (based on sensory data): | quality (score between 0 and 10) | type 1/0 (red/white) | . red = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39;,sep=&#39;;&#39;) red[&#39;type&#39;] = 1 white = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39;,sep=&#39;;&#39;) white[&#39;type&#39;] = 0 df = pd.concat([red,white],ignore_index=True) df.columns = [&#39;_&#39;.join(x.split()) for x in df.columns.str.lower()] df[:3] . fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density ph sulphates alcohol quality type . 0 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | 1 | . 1 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25.0 | 67.0 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | 1 | . 2 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15.0 | 54.0 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | 1 | . EDA . number of wine samples by quality . df.quality.value_counts().plot.bar(rot=0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7efe6684db00&gt; . Average features by quality . df.groupby(&#39;quality&#39;).agg({ &#39;fixed_acidity&#39;: &#39;mean&#39;, &#39;volatile_acidity&#39;: &#39;mean&#39;, &#39;citric_acid&#39;: &#39;mean&#39;, &#39;residual_sugar&#39;: &#39;mean&#39;, &#39;chlorides&#39;: &#39;mean&#39;, &#39;free_sulfur_dioxide&#39;: &#39;mean&#39;, &#39;total_sulfur_dioxide&#39;: &#39;mean&#39;, &#39;density&#39;: &#39;mean&#39;, &#39;ph&#39;: &#39;mean&#39;, &#39;sulphates&#39;: &#39;mean&#39;, &#39;alcohol&#39;: &#39;mean&#39;, }).plot.bar(rot=0,figsize=(15,5),cmap=&#39;Paired&#39;).legend(bbox_to_anchor=(1.0,1.0)) . &lt;matplotlib.legend.Legend at 0x7efe6684d0b8&gt; . Predicting Wine Quality . . Split the data into Train &amp; Test . train,test = train_test_split(df,test_size=0.33, random_state=42,stratify=df.quality) . Feature columns . x_cols = [ &#39;fixed_acidity&#39;, &#39;volatile_acidity&#39;, &#39;citric_acid&#39;, &#39;residual_sugar&#39;,&#39;chlorides&#39;, &#39;free_sulfur_dioxide&#39;, &#39;total_sulfur_dioxide&#39;, &#39;density&#39;,&#39;ph&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;, &#39;type&#39; ] . Grid Search Parameters Search . param_grid = { # &quot;learning_rate&quot; : [0.05, 0.10 ] , &quot;max_depth&quot; : [ 1, 4, 7, 14, 20], # &quot;min_child_weight&quot; : [ 3, 5, 7 ], # &quot;gamma&quot; : [ 0.1, 0.3], &quot;colsample_bytree&quot; : [ 0.3, 0.5 , 0.7 ], &quot;n_estimators&quot; : [ 1000 ], &quot;objective&quot;: [&#39;binary:logistic&#39;,&#39;multi:softmax&#39;,&#39;multi:softprob&#39;], &quot;num_class&quot;: [df.quality.nunique()] } . XGBoost Classifier + Hyper-Parameters Tunning . . Search . xgc = xgb.XGBClassifier() grid = GridSearchCV(xgc, param_grid, cv=2,verbose=10,n_jobs=-1) grid.fit(train[x_cols],train[&#39;quality&#39;]) . Already searched the best . best_params = { &#39;colsample_bytree&#39;: 0.5, &#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 1000, &#39;num_class&#39;: 7, &#39;objective&#39;: &#39;binary:logistic&#39; } xgc = xgb.XGBClassifier(**best_params) xgc.fit(train[x_cols],train[&#39;quality&#39;]) . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1, nthread=None, num_class=7, objective=&#39;multi:softprob&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . Feature importance . xgb.plot_importance(xgc) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7efe6668e470&gt; . Results . results = pd.DataFrame({ &#39;y_pred&#39;: xgc.predict(test[x_cols]), &#39;y_true&#39;: test[&#39;quality&#39;]}) results . y_pred y_true . 5783 7 | 7 | . 2962 5 | 4 | . 1384 5 | 5 | . 5905 6 | 6 | . 3083 5 | 3 | . ... ... | ... | . 4066 6 | 6 | . 1083 6 | 6 | . 398 6 | 6 | . 306 5 | 5 | . 6139 6 | 6 | . 2145 rows × 2 columns . Classification Report . print(metrics.classification_report(results.y_true,results.y_pred)) . precision recall f1-score support 3 0.00 0.00 0.00 10 4 0.44 0.17 0.24 71 5 0.71 0.69 0.70 706 6 0.65 0.74 0.70 936 7 0.64 0.60 0.62 356 8 0.88 0.36 0.51 64 9 0.00 0.00 0.00 2 accuracy 0.67 2145 macro avg 0.48 0.37 0.40 2145 weighted avg 0.66 0.67 0.66 2145 . /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . Anomaly Detection . . from sklearn.ensemble import IsolationForest . Columns to search for anomalies . x_cols = [ &#39;fixed_acidity&#39;, &#39;volatile_acidity&#39;, &#39;citric_acid&#39;, &#39;residual_sugar&#39;,&#39;chlorides&#39;, &#39;free_sulfur_dioxide&#39;, &#39;total_sulfur_dioxide&#39;, &#39;density&#39;,&#39;ph&#39;, &#39;sulphates&#39;, &#39;alcohol&#39; ] . Anomalies dataframe . %%time adf = df.copy() adf[&#39;anomalies&#39;] = IsolationForest(random_state=0,n_estimators=1000,n_jobs=-1).fit_predict(adf[x_cols]) adf[:3] . CPU times: user 4.53 s, sys: 255 ms, total: 4.79 s Wall time: 4.3 s . Total Anoamlies . (adf.anomalies.value_counts() / adf.anomalies.value_counts().sum()).rename(index={1:True,-1:False}).plot.pie(autopct=&#39;%1.1f%%&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f52bf32d6a0&gt; . adf.groupby([&#39;anomalies&#39;,&#39;type&#39;,&#39;quality&#39;]).agg({x:&#39;mean&#39; for x in x_cols})#.style.background_gradient(cmap=&#39;Blues&#39;) . fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density ph sulphates alcohol . anomalies type quality . -1 0 3 8.487500 | 0.353750 | 0.335000 | 6.700000 | 0.068000 | 95.812500 | 240.125000 | 0.995799 | 3.107500 | 0.527500 | 10.162500 | . 4 7.791667 | 0.630417 | 0.352500 | 6.062500 | 0.064000 | 42.250000 | 167.958333 | 0.995680 | 3.181667 | 0.487500 | 10.341667 | . 5 7.231507 | 0.340137 | 0.462466 | 11.860274 | 0.083274 | 49.650685 | 182.136986 | 0.997487 | 3.103288 | 0.513014 | 9.487671 | . 6 7.312676 | 0.308873 | 0.506761 | 11.803521 | 0.066183 | 40.767606 | 154.112676 | 0.997205 | 3.145352 | 0.536761 | 10.210563 | . 7 5.419048 | 0.375476 | 0.221429 | 4.226190 | 0.033857 | 39.190476 | 130.285714 | 0.990191 | 3.341905 | 0.641429 | 12.921429 | . 8 5.200000 | 0.318333 | 0.307778 | 3.166667 | 0.032889 | 54.222222 | 148.222222 | 0.989728 | 3.384444 | 0.676667 | 12.688889 | . 1 3 8.700000 | 0.888125 | 0.205000 | 2.806250 | 0.135750 | 10.500000 | 23.500000 | 0.997699 | 3.390000 | 0.565000 | 9.987500 | . 4 7.500000 | 0.822750 | 0.142000 | 2.400000 | 0.110300 | 13.300000 | 34.300000 | 0.996449 | 3.461500 | 0.666500 | 10.530000 | . 5 9.346491 | 0.625263 | 0.333772 | 3.141228 | 0.147377 | 16.964912 | 54.307018 | 0.998277 | 3.246316 | 0.764298 | 10.075000 | . 6 9.688372 | 0.509496 | 0.385271 | 3.042248 | 0.110450 | 13.740310 | 38.767442 | 0.997853 | 3.274651 | 0.769302 | 10.820930 | . 7 9.648214 | 0.463393 | 0.445893 | 3.228571 | 0.086839 | 13.625000 | 45.964286 | 0.997024 | 3.269821 | 0.787857 | 11.638690 | . 8 8.383333 | 0.501667 | 0.360000 | 3.166667 | 0.063333 | 12.666667 | 47.666667 | 0.995000 | 3.351667 | 0.785000 | 12.783333 | . 1 0 3 7.008333 | 0.319583 | 0.336667 | 6.187500 | 0.045167 | 25.000000 | 124.250000 | 0.994274 | 3.240833 | 0.439167 | 10.466667 | . 4 7.076821 | 0.361424 | 0.300397 | 4.514238 | 0.048993 | 21.857616 | 121.887417 | 0.994165 | 3.182980 | 0.475232 | 10.137417 | . 5 6.918280 | 0.300000 | 0.331069 | 7.096279 | 0.049873 | 35.734827 | 149.257225 | 0.995145 | 3.172290 | 0.480578 | 9.825780 | . 6 6.821815 | 0.258952 | 0.332393 | 6.262623 | 0.044518 | 35.479784 | 136.477668 | 0.993853 | 3.190042 | 0.489582 | 10.587549 | . 7 6.766880 | 0.260012 | 0.328172 | 5.209953 | 0.038297 | 34.001746 | 124.988359 | 0.992508 | 3.210768 | 0.499721 | 11.329957 | . 8 6.736145 | 0.275181 | 0.327530 | 5.807229 | 0.038608 | 35.771084 | 124.969880 | 0.992372 | 3.209699 | 0.475904 | 11.578916 | . 9 7.420000 | 0.298000 | 0.386000 | 4.120000 | 0.027400 | 33.400000 | 116.000000 | 0.991460 | 3.308000 | 0.466000 | 12.180000 | . 1 3 7.000000 | 0.870000 | 0.035000 | 1.950000 | 0.069500 | 13.000000 | 30.500000 | 0.996525 | 3.430000 | 0.590000 | 9.825000 | . 4 7.948485 | 0.615909 | 0.193636 | 2.872727 | 0.078788 | 11.636364 | 37.424242 | 0.996599 | 3.333030 | 0.553939 | 10.104545 | . 5 7.930159 | 0.567346 | 0.225573 | 2.405732 | 0.081750 | 16.987654 | 56.957672 | 0.996868 | 3.316737 | 0.592152 | 9.864462 | . 6 8.007269 | 0.494440 | 0.245580 | 2.333988 | 0.078495 | 16.211198 | 41.402750 | 0.996301 | 3.329077 | 0.651513 | 10.581009 | . 7 8.568531 | 0.380629 | 0.347483 | 2.521678 | 0.072573 | 14.209790 | 30.734266 | 0.995744 | 3.298951 | 0.723007 | 11.398252 | . 8 8.658333 | 0.384167 | 0.406667 | 2.283333 | 0.071000 | 13.583333 | 26.333333 | 0.995318 | 3.225000 | 0.759167 | 11.750000 | .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/WineQuality.html",
            "relUrl": "/2020/10/09/WineQuality.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Used Cars Database",
            "content": ". Imports . import os,re,zipfile import pandas as pd import numpy as np from types import SimpleNamespace from matplotlib import pyplot as plt plt.style.use(&#39;dark_background&#39;) plt.style.use(&#39;seaborn&#39;) base_size = 10 sizes=SimpleNamespace(**dict(small=(1*base_size,1*base_size),medium=(2*base_size,2*base_size),large=(3*base_size,3*base_size))) . Download and Unzip . %%time zip_name = &#39;used-cars-database.zip&#39; if not os.path.exists(zip_name): os.environ[&#39;KAGGLE_USERNAME&#39;] = &quot;&quot; # username from the json file os.environ[&#39;KAGGLE_KEY&#39;] = &quot;&quot; # key from the json file !kaggle datasets download orgesleka/used-cars-database . Downloading used-cars-database.zip to /content 52% 9.00M/17.3M [00:00&lt;00:00, 29.0MB/s] 100% 17.3M/17.3M [00:00&lt;00:00, 43.8MB/s] CPU times: user 22.3 ms, sys: 3.06 ms, total: 25.4 ms Wall time: 2.34 s . with zipfile.ZipFile(zip_name, &#39;r&#39;) as zip_ref: zip_ref.extractall(zip_name.split(&#39;.&#39;)[0]) . os.listdir(zip_name.split(&#39;.&#39;)[0]) . [&#39;autos.csv&#39;, &#39;cnt_km_year_powerPS_minPrice_maxPrice_avgPrice_sdPrice.csv&#39;] . Load data . auto = pd.read_csv(zip_name.split(&#39;.&#39;)[0]+&#39;/autos.csv&#39;,encoding=&#39;ISO-8859-1&#39;) auto . dateCrawled name seller offerType price abtest vehicleType yearOfRegistration gearbox powerPS model kilometer monthOfRegistration fuelType brand notRepairedDamage dateCreated nrOfPictures postalCode lastSeen . 0 2016-03-24 11:52:17 | Golf_3_1.6 | privat | Angebot | 480 | test | NaN | 1993 | manuell | 0 | golf | 150000 | 0 | benzin | volkswagen | NaN | 2016-03-24 00:00:00 | 0 | 70435 | 2016-04-07 03:16:57 | . 1 2016-03-24 10:58:45 | A5_Sportback_2.7_Tdi | privat | Angebot | 18300 | test | coupe | 2011 | manuell | 190 | NaN | 125000 | 5 | diesel | audi | ja | 2016-03-24 00:00:00 | 0 | 66954 | 2016-04-07 01:46:50 | . 2 2016-03-14 12:52:21 | Jeep_Grand_Cherokee_&quot;Overland&quot; | privat | Angebot | 9800 | test | suv | 2004 | automatik | 163 | grand | 125000 | 8 | diesel | jeep | NaN | 2016-03-14 00:00:00 | 0 | 90480 | 2016-04-05 12:47:46 | . 3 2016-03-17 16:54:04 | GOLF_4_1_4__3TÜRER | privat | Angebot | 1500 | test | kleinwagen | 2001 | manuell | 75 | golf | 150000 | 6 | benzin | volkswagen | nein | 2016-03-17 00:00:00 | 0 | 91074 | 2016-03-17 17:40:17 | . 4 2016-03-31 17:25:20 | Skoda_Fabia_1.4_TDI_PD_Classic | privat | Angebot | 3600 | test | kleinwagen | 2008 | manuell | 69 | fabia | 90000 | 7 | diesel | skoda | nein | 2016-03-31 00:00:00 | 0 | 60437 | 2016-04-06 10:17:21 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 371523 2016-03-14 17:48:27 | Suche_t4___vito_ab_6_sitze | privat | Angebot | 2200 | test | NaN | 2005 | NaN | 0 | NaN | 20000 | 1 | NaN | sonstige_autos | NaN | 2016-03-14 00:00:00 | 0 | 39576 | 2016-04-06 00:46:52 | . 371524 2016-03-05 19:56:21 | Smart_smart_leistungssteigerung_100ps | privat | Angebot | 1199 | test | cabrio | 2000 | automatik | 101 | fortwo | 125000 | 3 | benzin | smart | nein | 2016-03-05 00:00:00 | 0 | 26135 | 2016-03-11 18:17:12 | . 371525 2016-03-19 18:57:12 | Volkswagen_Multivan_T4_TDI_7DC_UY2 | privat | Angebot | 9200 | test | bus | 1996 | manuell | 102 | transporter | 150000 | 3 | diesel | volkswagen | nein | 2016-03-19 00:00:00 | 0 | 87439 | 2016-04-07 07:15:26 | . 371526 2016-03-20 19:41:08 | VW_Golf_Kombi_1_9l_TDI | privat | Angebot | 3400 | test | kombi | 2002 | manuell | 100 | golf | 150000 | 6 | diesel | volkswagen | NaN | 2016-03-20 00:00:00 | 0 | 40764 | 2016-03-24 12:45:21 | . 371527 2016-03-07 19:39:19 | BMW_M135i_vollausgestattet_NP_52.720____Euro | privat | Angebot | 28990 | control | limousine | 2013 | manuell | 320 | m_reihe | 50000 | 8 | benzin | bmw | nein | 2016-03-07 00:00:00 | 0 | 73326 | 2016-03-22 03:17:10 | . 371528 rows × 20 columns . missing_values_cols = auto.isnull().sum() missing_values_cols = missing_values_cols[missing_values_cols &gt; 0] missing_values_cols = missing_values_cols.index.tolist() missing_values_cols . [&#39;vehicleType&#39;, &#39;gearbox&#39;, &#39;model&#39;, &#39;fuelType&#39;, &#39;notRepairedDamage&#39;] . # auto.groupby([&#39;category&#39;, &#39;name&#39;])[&#39;value&#39;].transform(lambda x: x.fillna(x.mean())) . Quick EDA . fig,ax = plt.subplots(2,4,figsize=(30,15)) auto.vehicleType.value_counts().plot.barh(ax=ax.ravel()[0],title=&#39;Number of vehicles by type&#39;) auto.fuelType.value_counts().plot.barh(ax=ax.ravel()[1],title=&#39;Number of vehicles by fuel type&#39;) auto.yearOfRegistration.value_counts()[:20].plot.barh(ax=ax.ravel()[2],title=&#39;Number of vehicles by year of registration (first 20)&#39;) auto.groupby(&#39;vehicleType&#39;)[[&#39;price&#39;]].mean().plot.barh(ax=ax.ravel()[3],title=&#39;Mean price of vehicles by type&#39;) auto.groupby(&#39;fuelType&#39;)[[&#39;price&#39;]].mean().plot.barh(ax=ax.ravel()[4],title=&#39;Mean price of vehicles fuel type&#39;) auto.groupby(&#39;vehicleType&#39;)[[&#39;kilometer&#39;]].mean().plot.barh(ax=ax.ravel()[5],title=&#39;Mean kilometer of vehicles by type&#39;) auto.groupby(&#39;fuelType&#39;)[[&#39;kilometer&#39;]].mean().plot.barh(ax=ax.ravel()[6],title=&#39;Mean kilometer of vehicles fuel type&#39;) auto.gearbox.value_counts().plot.barh(ax=ax.ravel()[7],title=&#39;Number of vehicles by gearbox&#39;) plt.tight_layout() . Kilometers and Prices by Brand and Model . tmp = auto.groupby([&#39;brand&#39;,&#39;model&#39;]).agg({ &#39;kilometer&#39;: &#39;mean&#39;, &#39;price&#39;: &#39;mean&#39; }) t = list(set([x[0] for x in tmp.index])) fig, ax = plt.subplots(int(len(t) / 4)+1, 4,figsize=(50,50)) for i,brand in enumerate(t): try: tmp.loc[brand].plot.barh(ax=ax.ravel()[i], title=f&#39;Mean (kilometer, price) for {brand.replace(&quot;_&quot;,&quot; &quot;).capitalize()} car models&#39;) except: print(f&#39;error for {brand}&#39;) continue plt.tight_layout() . After the quick overview let&#39;s build a model to help us set the price . XGBoost and LabelEncoder should do the job for us . from sklearn.model_selection import train_test_split import xgboost as xgb from sklearn.preprocessing import LabelEncoder from sklearn import metrics from collections import defaultdict d = defaultdict(LabelEncoder) d . defaultdict(sklearn.preprocessing._label.LabelEncoder, {}) . x_cols are our features and target is what we look for to predict . x_cols=[&#39;brand&#39;,&#39;model&#39;,&#39;kilometer&#39;,&#39;yearOfRegistration&#39;,&#39;monthOfRegistration&#39;,&#39;fuelType&#39;,&#39;vehicleType&#39;,&#39;notRepairedDamage&#39;,&#39;powerPS&#39;,&#39;gearbox&#39;] target = &#39;price&#39; . Copy the dataframe without the missing values | Define the columns that need encoding | Apply the encoding | . auto_copy = auto.dropna().copy() enc_cols = list(set(x_cols) - set([&#39;powerPS&#39;,&#39;yearOfRegistration&#39;,&#39;monthOfRegistration&#39;,&#39;kilometer&#39;])) auto_copy[enc_cols] = auto_copy[enc_cols].apply(lambda x: d[x.name].fit_transform(x)) . After we dropped the missing values we still have 70% of the data . len(auto_copy) / len(auto) . 0.7023858228720312 . Now, let&#39;s split our data into train/test . X_train, X_test, y_train, y_test = train_test_split(auto_copy[x_cols], auto_copy[target], test_size=0.33, random_state=42) . Further on we can use the XGBRegressor with a gamma distribution and train it on the train data . %%time xg = xgb.XGBRegressor(objective =&#39;reg:gamma&#39;, n_estimators = 1000).fit(X_train,y_train) . CPU times: user 1min 23s, sys: 67.9 ms, total: 1min 23s Wall time: 1min 23s . Make a dataframe with test target vs predicted test target . preds = pd.DataFrame({ &#39;true&#39;: y_test, &#39;predicted&#39;: xg.predict(X_test) }) . Use MAE for our metric . metrics.mean_absolute_error(preds.true,preds.predicted) . 2078.077884307823 . Let&#39;s have a look into our Feature Importance . plt.figure(figsize=sizes.small) xgb.plot_importance(xg) plt.show() . &lt;Figure size 720x720 with 0 Axes&gt; . And here we have a copy of our test dataframe with decoded columns, but also with original and predicted price . xt = X_test.copy() xt[&#39;original_price&#39;] = y_test xt[&#39;predicted_price&#39;] = xg.predict(X_test) xt[enc_cols] = xt[enc_cols].apply(lambda x: d[x.name].inverse_transform(x)) xt[:30] . brand model kilometer yearOfRegistration monthOfRegistration fuelType vehicleType notRepairedDamage powerPS gearbox original_price predicted_price . 336427 volkswagen | polo | 150000 | 2003 | 12 | benzin | kleinwagen | nein | 75 | manuell | 2990 | 2454.927246 | . 339513 ford | fiesta | 125000 | 1998 | 6 | benzin | kleinwagen | nein | 0 | automatik | 850 | 980.325439 | . 54995 renault | megane | 150000 | 1999 | 11 | benzin | kombi | nein | 0 | manuell | 950 | 830.935181 | . 297839 mercedes_benz | a_klasse | 150000 | 2000 | 5 | diesel | limousine | nein | 90 | automatik | 1500 | 1719.381470 | . 353328 volkswagen | andere | 125000 | 1995 | 5 | benzin | limousine | nein | 90 | automatik | 1700 | 1200.977295 | . 213296 bmw | 3er | 125000 | 1997 | 9 | benzin | limousine | nein | 102 | manuell | 2450 | 1498.408813 | . 57967 opel | agila | 30000 | 2009 | 11 | benzin | kleinwagen | nein | 60 | manuell | 5850 | 5020.946777 | . 114503 nissan | micra | 150000 | 2000 | 10 | benzin | kleinwagen | nein | 54 | manuell | 399 | 853.951660 | . 126949 seat | ibiza | 80000 | 2010 | 8 | diesel | kleinwagen | nein | 143 | manuell | 10760 | 10703.540039 | . 22843 audi | a6 | 150000 | 2007 | 11 | diesel | kombi | nein | 179 | manuell | 10450 | 21701.361328 | . 281191 mercedes_benz | e_klasse | 150000 | 2002 | 5 | diesel | limousine | nein | 150 | manuell | 5350 | 3735.010254 | . 210491 seat | leon | 80000 | 2008 | 10 | benzin | limousine | ja | 125 | manuell | 3900 | 5240.312988 | . 298231 fiat | seicento | 150000 | 2001 | 2 | benzin | kleinwagen | nein | 55 | manuell | 0 | 837.973206 | . 246949 fiat | punto | 125000 | 2000 | 5 | benzin | kleinwagen | nein | 60 | manuell | 1100 | 935.326477 | . 316596 volkswagen | polo | 150000 | 1997 | 10 | benzin | kleinwagen | nein | 60 | manuell | 500 | 750.544250 | . 249809 bmw | 5er | 150000 | 2006 | 3 | diesel | limousine | nein | 163 | automatik | 10449 | 8034.567871 | . 241669 suzuki | swift | 70000 | 2008 | 1 | benzin | kleinwagen | nein | 92 | manuell | 4500 | 4527.567871 | . 288208 opel | astra | 50000 | 2000 | 9 | benzin | limousine | nein | 0 | automatik | 3100 | 2609.267090 | . 56016 audi | 80 | 50000 | 1994 | 1 | benzin | limousine | nein | 90 | manuell | 3500 | 2869.840820 | . 274185 volkswagen | golf | 70000 | 2008 | 10 | benzin | coupe | nein | 161 | automatik | 10000 | 13441.850586 | . 357311 volvo | c_reihe | 150000 | 2000 | 10 | benzin | cabrio | nein | 163 | manuell | 4500 | 4575.593262 | . 279920 peugeot | 3_reihe | 150000 | 2008 | 5 | diesel | cabrio | nein | 136 | manuell | 4666 | 6645.180664 | . 73242 mercedes_benz | a_klasse | 150000 | 1999 | 4 | benzin | limousine | nein | 82 | manuell | 2250 | 1340.504639 | . 66915 opel | astra | 150000 | 2004 | 10 | benzin | limousine | nein | 105 | manuell | 3790 | 2607.811768 | . 112509 volkswagen | golf | 150000 | 2004 | 5 | benzin | coupe | nein | 75 | manuell | 2500 | 3468.785645 | . 265868 volkswagen | golf | 150000 | 1999 | 4 | benzin | limousine | nein | 125 | manuell | 1300 | 1867.259277 | . 116073 mazda | andere | 150000 | 2004 | 9 | diesel | bus | nein | 136 | manuell | 1299 | 3971.486572 | . 321078 kia | sorento | 150000 | 2003 | 7 | diesel | suv | nein | 140 | manuell | 4550 | 4021.498535 | . 124545 opel | astra | 150000 | 2001 | 5 | benzin | cabrio | nein | 147 | manuell | 3900 | 2662.085449 | . 246943 fiat | punto | 60000 | 2011 | 1 | benzin | kleinwagen | nein | 105 | manuell | 6850 | 6856.173828 | .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/UsedCarsDatabase.html",
            "relUrl": "/2020/10/09/UsedCarsDatabase.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Vector analysis over twitter posts",
            "content": "Now let&#39;s do some mining on twitter . We use faster-than-requests becaue is new, fast and fancy . try: import faster_than_requests as req except: !pip install faster-than-requests import faster_than_requests as req . we upgrade gensim at this point we we won&#39;t have to restart the runtime after we get the data in . !pip install --upgrade gensim . Here we import the packages we need, good to know we use fake_useragent to change the User-Agent in the header of each request to make it look like IE7. Thus it will force Twitter to go into the old web version without the web component based front-end and easy to crawl . import pandas as pd import numpy as np from matplotlib import pyplot as plt from bs4 import BeautifulSoup try: from fake_useragent import UserAgent except: !pip install fake_useragent from fake_useragent import UserAgent from os.path import exists import urllib.parse . More utils which are going to be used later, please use TF &lt;=2.0 . from gensim.utils import simple_preprocess import tensorflow as tf import tensorflow_hub as hub from sklearn.metrics.pairwise import cosine_similarity import networkx as nx from wordcloud import WordCloud import nltk nltk.download(&#39;stopwords&#39;) nltk.download(&#39;punkt&#39;) from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from nltk.tokenize import RegexpTokenizer . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! . Here is the fancy trick with the user agent . req.set_headers([ (&#39;User-Agent&#39;, UserAgent().ie) ]) . So here we have: . twitter_selectors a dictionary with the css selectors for each tweet . | get_soup is sending a get request to get the html content from the url right into the BeautifulSoup html parser creating a object which I call soup . | get_tweets is using a soup to extract the username and content of each tweet . | get_tweets_df finally this function is using all functions from above into a loop to mine all the tweets and returning a pandas dataframe . | . twitter_selectors = { &#39;post&#39;: &#39;.tweet&#39;, # &#39;full_name&#39;: &#39;.fullname&#39;, &#39;username&#39;: &#39;.username&#39;, &#39;content&#39;: &#39;.tweet-text&#39; } def get_soup(url): return BeautifulSoup(req.get2str(url),&#39;html.parser&#39;) def get_tweets(soup): tweets = list() for tweet in soup.select(twitter_selectors.get(&#39;post&#39;)): tweets.append({ &#39;username&#39;: tweet.select_one(twitter_selectors.get(&#39;username&#39;)).text, &#39;content&#39;: tweet.select_one(twitter_selectors.get(&#39;content&#39;)).text, }) return tweets def get_tweets_df(keyword,limit=None): url = f&#39;https://mobile.twitter.com/search?q={urllib.parse.quote_plus(keyword)}&#39; tweets = [] stop = False while(stop != True): try: soup = get_soup(url=url) tweets+=get_tweets(soup=soup) next = soup.select_one(&#39;.w-button-more a&#39;) if next: url = &#39;https://mobile.twitter.com&#39; + next[&#39;href&#39;] else: stop = True except: continue if limit != None and limit &lt;= tweets.__len__(): stop = True print(f&#39;{tweets.__len__()} tweets has been crawled&#39;) return pd.DataFrame.from_dict(tweets) . Then we call it with a 1000 tweets limit to get all the tweets for the related topic, in our case coronavirus. Since we are mining by tweets/page we get a few more tweets. . %%time df = get_tweets_df(&#39;coronavirus&#39;,limit = 1000) . 1015 tweets has been crawled CPU times: user 2.45 s, sys: 43.4 ms, total: 2.5 s Wall time: 27.5 s . Further on we use this lambda function to remove any breakline @ and # . df = df.apply(lambda x: x.str.replace(&#39; n&#39;,&#39;&#39;).replace(&#39;@&#39;,&#39;&#39;).replace(&#39;#&#39;,&#39;&#39;),axis=1) . Sentence Embeddings of tweets content . Now we use this Universal Sentence Encoder to get the sentence embeddings from the tweets content . embd = hub.load(f&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;) def embed(sentences): return np.array(embd(tf.squeeze(tf.cast(sentences, tf.string)))) . We save it as a column . df[&#39;content_sent_vects&#39;] = embed(df.content.values).tolist() . df . username content content_wordlist content_sent_vects . 0 @allkpop | Jaejoong facing possible punishment by KCDC ... | NaN | [-0.07023757696151733, 0.04490802809596062, -0... | . 1 @Corona_Bot__ | CONFIRMED: Barney tests positive for Coronav... | NaN | [0.05011240020394325, -0.031166449189186096, -... | . 2 @nytimes | The Korean star known as Jaejoong of the K-p... | NaN | [-0.01596658118069172, 0.04994076490402222, 0.... | . 3 @PIB_India | Approx 1800 people related to #TablighJamaat... | NaN | [-0.06460247933864594, 0.05959327518939972, -0... | . 4 @SkyNews | Coronavirus: Woman fined £650 for refusing t... | NaN | [-0.05083870142698288, 0.03122670203447342, 0.... | . ... ... | ... | ... | ... | . 1010 @ASlavitt | Andy Slavitt: slowing spread of coronavirus ... | NaN | [0.02724810130894184, -0.002946038730442524, 0... | . 1011 @fred_guttenberg | Many reasons that I wish @JoeBiden were Pres... | NaN | [0.04280007258057594, -0.05422208085656166, 0.... | . 1012 @FOXLA | Coronavirus droplets could travel 27 feet, wa... | NaN | [0.03585874289274216, 0.06049816682934761, 0.0... | . 1013 @joncoopertweets | Fox News purportedly bracing for “legal bloo... | NaN | [0.03799957036972046, 0.0289924293756485, -0.0... | . 1014 @NBCLatino | On #CesarChavezDay, Democratic presidential ... | NaN | [0.020493853837251663, 0.03346347063779831, 0.... | . 1015 rows × 4 columns . LDA model . More packages . from gensim.models import Phrases from gensim.models.phrases import Phraser import spacy import gensim.corpora as corpora from gensim.models.ldamodel import LdaModel from pprint import pprint from gensim.models import CoherenceModel from gensim.models.wrappers import LdaMallet from os.path import exists import requests, zipfile, io import os . I&#39;ve put together a class following this tutorial to handler easier the LDA model, still having some issues with the mallet model inside of the colab, but I guess I will try to sort that out later . Nothing very fancy here, just building and storing everytning in a class, as further on we might need to reuse bigrams or trigrams. It is not the best approach as it is using loads of other packages inside so passing arugments into a constructor to initialise or change each module parameters might be pain so for the moment we will stick with this configuration and hopefully we can find a fast way to work with the mallet model. . class LDA: def __init__(self,sentences,mallet=False): self.sentences = sentences self.bigram = Phrases(self.sentences, min_count=5, threshold=100) # higher threshold fewer phrases. self.trigram = Phrases(self.bigram[self.sentences], threshold=100) self.bigram_mod = Phraser(self.bigram) self.trigram_mod = Phraser(self.trigram) self.stop_words = stopwords.words(&#39;english&#39;) self.nlp = spacy.load(&#39;en&#39;, disable=[&#39;parser&#39;, &#39;ner&#39;]) self.download_mallet_path = &#39;http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip&#39; self.mallet_path = &#39;/content/mallet-2.0.8/bin/mallet&#39; self.ldamallet = None self.ldamallet = None self.topics = None self.java_installed = False os.environ[&#39;MALLET_HOME&#39;] = &#39;/content/mallet-2.0.8&#39; ###################### self.make() # if mallet: # self.build_ldamallet() # else: # self.build_lda() ###################### def remove_stopwords(self): self.sent_no_stops = [[word for word in simple_preprocess(str(doc)) if word not in self.stop_words] for doc in self.sentences] def make_bigrams(self): self.bigram_data = [self.bigram_mod[doc] for doc in self.sent_no_stops] def make_trigrams(self): self.trigram_data = [self.trigram_mod[self.bigram_mod[doc]] for doc in self.sent_no_stops] def lemmatization(self, allowed_postags=[&#39;NOUN&#39;, &#39;ADJ&#39;, &#39;VERB&#39;, &#39;ADV&#39;]): self.lemm_sentences = [ [token.lemma_ for token in self.nlp(&quot; &quot;.join(sent)) if token.pos_ in allowed_postags] for sent in self.bigram_data] def dictionary(self): self.id2word = corpora.Dictionary(self.lemm_sentences) def corpus(self): self.corpus = [self.id2word.doc2bow(text) for text in self.lemm_sentences] def make(self): self.remove_stopwords() self.make_bigrams() self.make_trigrams() self.lemmatization() self.dictionary() self.corpus() def build_lda(self,num_topics=20,random_state=100,update_every=1,chunksize=100,passes=10,alpha=&#39;auto&#39;): self.lda_model = LdaModel( corpus=self.corpus, id2word=self.id2word, num_topics=num_topics, random_state=random_state, update_every=update_every, chunksize=chunksize, passes=passes, alpha=alpha, per_word_topics=True) def download_mallet(self): r = requests.get(self.download_mallet_path) z = zipfile.ZipFile(io.BytesIO(r.content)) z.extractall() if not self.java_installed: self.install_java() def install_java(self): !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null #install openjdk os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; #set environment variable !java -version #check java version def build_ldamallet(self,num_topics=20): if not exists(self.mallet_path): self.download_mallet() self.ldamallet = LdaMallet(self.mallet_path, corpus=self.corpus, num_topics=num_topics, id2word=self.id2word) def coherence_score(self,mallet=False): if mallet: pprint(self.ldamallet.show_topics(formatted=False)) md = self.ldamallet else: md = self.lda_model pprint(self.lda_model.print_topics()) coherence_model = CoherenceModel(model=md, texts=self.lemm_sentences, dictionary=self.id2word, coherence=&#39;c_v&#39;) coh = coherence_model.get_coherence() print(&#39; nCoherence Score: &#39;, coh) def compute_coherence_values(self, limit, start=2, step=3, mallet=False): coherence_values = [] for num_topics in range(start, limit, step): if mallet: self.build_ldamallet(num_topics=num_topics) md = self.ldamallet else: self.build_lda(num_topics=num_topics) md = self.lda_model coherencemodel = CoherenceModel(model=md, texts=self.lemm_sentences, dictionary=self.id2word, coherence=&#39;c_v&#39;) coherence_values.append({ &#39;num_topics&#39;: num_topics, &#39;coherence&#39;: coherencemodel.get_coherence() }) return coherence_values . We create an object of our LDA class and passing the raw sentences as a list. Then we compute a coherence search for the best number of topics related to our content. However since those tweets are mined, this process might have different results on each new data. . %%time ld = LDA(df.content.values.tolist()) ld_coherence_values = ld.compute_coherence_values(start=2, limit=40, step=6) pprint(ld_coherence_values) . [{&#39;coherence&#39;: 0.24454476285561758, &#39;num_topics&#39;: 2}, {&#39;coherence&#39;: 0.46119096489624833, &#39;num_topics&#39;: 8}, {&#39;coherence&#39;: 0.4516224532223637, &#39;num_topics&#39;: 14}, {&#39;coherence&#39;: 0.42736241847827366, &#39;num_topics&#39;: 20}, {&#39;coherence&#39;: 0.3930428008422175, &#39;num_topics&#39;: 26}, {&#39;coherence&#39;: 0.4105924951482943, &#39;num_topics&#39;: 32}, {&#39;coherence&#39;: 0.39407950264156383, &#39;num_topics&#39;: 38}] CPU times: user 34.9 s, sys: 183 ms, total: 35.1 s Wall time: 35.2 s . From the previous search we found the best number of topics, which now we are using to rebuild the model and show the topics we found. . ld.build_lda(num_topics=8) ld.lda_model.print_topics() . [(0, &#39;0.148*&#34;com&#34; + 0.075*&#34;twitter&#34; + 0.061*&#34;pic&#34; + 0.046*&#34;virus&#34; + 0.032*&#34;go&#34; + 0.026*&#34;people&#34; + 0.022*&#34;corona&#34; + 0.018*&#34;test&#34; + 0.018*&#34;make&#34; + 0.018*&#34;day&#34;&#39;), (1, &#39;0.026*&#34;would&#34; + 0.025*&#34;fight&#34; + 0.021*&#34;die&#34; + 0.019*&#34;watch&#34; + 0.018*&#34;turn&#34; + 0.017*&#34;hard&#34; + 0.016*&#34;leader&#34; + 0.016*&#34;thank&#34; + 0.013*&#34;presidential&#34; + 0.013*&#34;bar&#34;&#39;), (2, &#39;0.036*&#34;week&#34; + 0.027*&#34;tell&#34; + 0.023*&#34;see&#34; + 0.019*&#34;official&#34; + 0.018*&#34;tough&#34; + 0.017*&#34;break&#34; + 0.015*&#34;story&#34; + 0.015*&#34;worker&#34; + 0.014*&#34;end&#34; + 0.014*&#34;good&#34;&#39;), (3, &#39;0.025*&#34;shift&#34; + 0.022*&#34;use&#34; + 0.018*&#34;slow&#34; + 0.017*&#34;do&#34; + 0.017*&#34;state&#34; + 0.016*&#34;company&#34; + 0.015*&#34;truth&#34; + 0.015*&#34;crisis&#34; + 0.015*&#34;few&#34; + 0.015*&#34;time&#34;&#39;), (4, &#39;0.037*&#34;say&#34; + 0.031*&#34;today&#34; + 0.029*&#34;doctor&#34; + 0.028*&#34;many&#34; + 0.028*&#34;want&#34; + 0.024*&#34;nurse&#34; + 0.021*&#34;work&#34; + 0.015*&#34;can&#34; + 0.013*&#34;american&#34; + 0.012*&#34;job&#34;&#39;), (5, &#39;0.069*&#34;trump&#34; + 0.027*&#34;country&#34; + 0.017*&#34;tirelessly&#34; + 0.016*&#34;join&#34; + 0.016*&#34;term&#34; + 0.014*&#34;enemy&#34; + 0.014*&#34;view&#34; + 0.013*&#34;concerned&#34; + 0.013*&#34;republican&#34; + 0.012*&#34;nation&#34;&#39;), (6, &#39;0.055*&#34;case&#34; + 0.053*&#34;death&#34; + 0.043*&#34;new&#34; + 0.021*&#34;spread&#34; + 0.021*&#34;high&#34; + 0.018*&#34;back&#34; + 0.017*&#34;call&#34; + 0.016*&#34;important&#34; + 0.014*&#34;look&#34; + 0.013*&#34;country&#34;&#39;), (7, &#39;0.031*&#34;incredible&#34; + 0.029*&#34;say&#34; + 0.025*&#34;warn&#34; + 0.024*&#34;rather&#34; + 0.023*&#34;report&#34; + 0.020*&#34;could&#34; + 0.020*&#34;bravery&#34; + 0.016*&#34;force&#34; + 0.016*&#34;think&#34; + 0.016*&#34;care&#34;&#39;)] . Graph Clustering . We use the cosine similarity on the extracted vectors to create a similarity matrix . %%time similarity_matrix = cosine_similarity(df.content_sent_vects.values.tolist()) . CPU times: user 2.88 s, sys: 150 ms, total: 3.03 s Wall time: 1.96 s . We put this together into a dataframe with the username as columns and index thus it will look quite similar with a correlation dataframe or heatmap . simdf = pd.DataFrame( similarity_matrix, columns = df.username.values.tolist(), index = df.username.values.tolist() ) . However we want this unstacked to make make it easier to pass into our graph, also we&#39;ve kept just the username as each one is unique . So here you can see the similarity between the content of each user . long_form = simdf.unstack() # rename columns and turn into a dataframe long_form.index.rename([&#39;t1&#39;, &#39;t2&#39;], inplace=True) long_form = long_form.to_frame(&#39;sim&#39;).reset_index() long_form = long_form[long_form.t1 != long_form.t2] long_form[:3] . t1 t2 sim . 1 @AdamSchefter | @NorbertElekes | 0.133930 | . 2 @AdamSchefter | @BreitbartNews | 0.295476 | . 3 @AdamSchefter | @DarylDust | 0.150086 | . Here we want to select the similarity treshold to filter the tweets and create our graph from the formatted dataframe . sim_weight = 0.95 gdf = long_form[long_form.sim &gt; sim_weight] plt.figure(figsize=(25,25)) pd_graph = nx.Graph() pd_graph = nx.from_pandas_edgelist(gdf, &#39;t1&#39;, &#39;t2&#39;) pos = nx.spring_layout(pd_graph) nx.draw_networkx(pd_graph,pos,with_labels=True,font_size=10,font_color=&#39;#fff&#39;,edge_color=&#39;#f00&#39;,node_size = 30) . Now we get the connected components into a dataframe . l=list(nx.connected_components(pd_graph)) L=[dict.fromkeys(y,x) for x, y in enumerate(l)] d=[{&#39;users&#39;:k , &#39;groups&#39;:v }for d in L for k, v in d.items()] . gcd = pd.DataFrame.from_dict(d) . Here is the number of groups or clusters we have extracted for the chosen similarity treshold . gcd.groups.nunique() . 65 . We add the content for each user into the grouped dataframe . gcd[&#39;content&#39;] = gcd.users.apply(lambda x: df[df.username==x].content.values.tolist()[0] ) gcd[:5] . users groups content . 0 @AdamSchefter | 0 | Cardinals All-Pro linebacker Chandler Jones ... | . 1 @kfitz134 | 0 | Cardinals All-Pro linebacker Chandler Jones ... | . 2 @Aletteri66 | 1 | The Corona virus can’t kill me because I alr... | . 3 @BrandonM0721 | 1 | The Corona Virus can’t kill me, I already di... | . 4 @realDailyWire | 2 | Epidemiologist Behind Highly-Cited Coronavir... | . Word Clouds for the tweets . We use a nltk tokenizer to extract just the words and remove the stop words . tok = RegexpTokenizer(r&#39; w+&#39;) stop_words = set(stopwords.words(&#39;english&#39;)) def clean(string): return &quot; &quot;.join([w for w in word_tokenize(&quot; &quot;.join(tok.tokenize(string))) if not w in stop_words]) gcd.content = gcd.content.apply(lambda x: clean(x)) . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! . Our big_groups is a list of indexes from our grouped dataset with the index of the top 12 groups sorted descending. . Then we iterate through these indexes and create WordCloud of each group whihc is stored in the clouds dictionary . %%time clouds = dict() big_groups = pd.DataFrame({ &#39;counts&#39;:gcd.groups.value_counts() }).sort_values(by=&#39;counts&#39;,ascending=False)[:12].index.values.tolist() for group in big_groups: text = gcd[gcd.groups == group].content.values wordcloud = WordCloud(width=1000, height=1000).generate(str(text)) clouds[group] = wordcloud . CPU times: user 28.7 s, sys: 581 ms, total: 29.3 s Wall time: 29.4 s . def plot_figures(figures, nrows = 1, ncols=1): fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=(20,20)) for ind,title in zip(range(len(figures)), figures): axeslist.ravel()[ind].imshow(figures[title], cmap=plt.jet()) axeslist.ravel()[ind].set_title(f&#39;Most Freqent words for the group {title+1}&#39;) axeslist.ravel()[ind].set_axis_off() plt.tight_layout() # optional . Then we plot them with the help of our plot_figures function . plt.style.use(&quot;dark_background&quot;) plot_figures(clouds, 3, 4) plt.show() .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/TwitterGraphAnalysis.html",
            "relUrl": "/2020/10/09/TwitterGraphAnalysis.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "🐍 PyTorch 🔥",
            "content": "Sources: . PyTorch Starter | PyTorch Tutorial for Deep Learning Lovers | Handwritten Digit Recognition | PyTorch basics - Linear Regression from scratch | Implement Linear Regression on Boston Housing Dataset by PyTorch | . What is PyTorch ? . It’s a Python-based scientific computing package targeted at two sets of audiences: . a replacement for NumPy to use the power of GPUs | a deep learning research platform that provides maximum flexibility and speed | . From NumPy to PyTorch . Tensors . Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing. . NumPy to Torch . np.array() == torch.Tensor() | np.ones() == torch.ones() | np.random.rand() == torch.rand() | type(array) == tensor.type | np.shape(array) == tensor.shape | np.resize(array,size) == tensor.view(size) | np.add(x,y) == torch.add(x,y) | np.sub(x,y) == torch.sub(x,y) | np.multiply(x,y) == torch.mul(x,y) | np.divide(x,y) == torch.div(x,y) | np.mean(array) == tensor.mean() | np.std(array) == tensor.std() | . We can also convert arrays NumPy &lt;-&gt; PyTorch . Variables . A Variable wraps a Tensor. It supports nearly all the API’s defined by a Tensor. Variable also provides a backward method to perform backpropagation. . For example, to backpropagate a loss function to train model parameter x, we use a variable loss to store the value computed by a loss function. . Then, we call loss.backward which computes the gradients for all trainable parameters. . PyTorch will store the gradient results back in the corresponding variable x. . Autograd is a PyTorch package for the differentiation for all operations on Tensors. It performs the backpropagation starting from a variable. . In deep learning, this variable often holds the value of the cost function. backward executes the backward pass and computes all the backpropagation gradients automatically. . Layers . Linear Layers . Identity | Linear | Bilinear | . | Non-Linear Activation (weighted sum nonlinearity) . ReLu | . | . Imports . import torch import pandas as pd import numpy as np import os,re,sys,io from matplotlib import pyplot as plt from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn import metrics from time import time import seaborn as sns scaler = MinMaxScaler() plt.style.use(&#39;seaborn&#39;) . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . Linear Regression . Boston Dataset . The Boston house-price data of Harrison, D. and Rubinfeld, D.L. ‘Hedonic prices and the demand for clean air’, J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, ‘Regression diagnostics …’, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. . CRIM per capita crime rate by town | ZN proportion of residential land zoned for lots over 25,000 sq.ft. | INDUS proportion of non-retail business acres per town | CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) | NOX nitric oxides concentration (parts per 10 million) | RM average number of rooms per dwelling | AGE proportion of owner-occupied units built prior to 1940 | DIS weighted distances to five Boston employment centres | RAD index of accessibility to radial highways | TAX full-value property-tax rate per $10,000 . | PTRATIO pupil-teacher ratio by town . | B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town | LSTAT % lower status of the population | MEDV Median value of owner-occupied homes in $1000 s | . boston_dataset = load_boston() boston = pd.DataFrame(data=boston_dataset.get(&#39;data&#39;),columns=boston_dataset.get(&#39;feature_names&#39;)) boston[&#39;PRICE&#39;] = boston_dataset.get(&#39;target&#39;) boston . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 501 0.06263 | 0.0 | 11.93 | 0.0 | 0.573 | 6.593 | 69.1 | 2.4786 | 1.0 | 273.0 | 21.0 | 391.99 | 9.67 | 22.4 | . 502 0.04527 | 0.0 | 11.93 | 0.0 | 0.573 | 6.120 | 76.7 | 2.2875 | 1.0 | 273.0 | 21.0 | 396.90 | 9.08 | 20.6 | . 503 0.06076 | 0.0 | 11.93 | 0.0 | 0.573 | 6.976 | 91.0 | 2.1675 | 1.0 | 273.0 | 21.0 | 396.90 | 5.64 | 23.9 | . 504 0.10959 | 0.0 | 11.93 | 0.0 | 0.573 | 6.794 | 89.3 | 2.3889 | 1.0 | 273.0 | 21.0 | 393.45 | 6.48 | 22.0 | . 505 0.04741 | 0.0 | 11.93 | 0.0 | 0.573 | 6.030 | 80.8 | 2.5050 | 1.0 | 273.0 | 21.0 | 396.90 | 7.88 | 11.9 | . 506 rows × 14 columns . It can be seen that the value range of data is different and the difference is large, so we need to make standardization. . Suppose each feature has a mean value μ and a standard deviation σ on the whole dataset. Hence we can subtract each value of the feature and then divide μ by σ to get the normalized value of each feature. (Tutorial approach) . | Another option is to use the MinMaxScaler from sklearn . | . for col in boston.columns[:-1]: boston[[col]] = scaler.fit_transform(boston[[col]]) . PyTorch Linear Rregression . Then we split the data into train/test while casting the data to numpy arrays . X_train, X_test, y_train, y_test = train_test_split(boston[boston_dataset.get(&#39;feature_names&#39;)].to_numpy(), boston[&#39;PRICE&#39;].to_numpy(), test_size=0.3, random_state=42) . Further on we cast our splitted data to tensors . X_train = torch.tensor(X_train, dtype=torch.float) X_test = torch.tensor(X_test, dtype=torch.float) y_train = torch.tensor(y_train, dtype=torch.float).view(-1, 1) y_test = torch.tensor(y_test, dtype=torch.float).view(-1, 1) . Linear Regression Model . w_num = X_train.shape[1] net = torch.nn.Sequential( # sequential layer torch.nn.Linear(w_num, 1) # linear layer ) torch.nn.init.normal_(net[0].weight, mean=0, std=0.1) torch.nn.init.constant_(net[0].bias, val=0) . Parameter containing: tensor([0.], requires_grad=True) . Dataset Processor . dataset = torch.utils.data.TensorDataset(X_train, y_train) . Data Loader . At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for . map-style and iterable-style datasets | customizing data loading order | automatic batching | single- and multi-process data loading | automatic memory pinning | . train_iter = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True) . Loss Function . We can compare the predictions with the actual targets, using the following method: . Calculate the difference between the two matrices (preds and targets). Square all elements of the difference matrix to remove negative values. Calculate the average of the elements in the resulting matrix. The result is a single number, known as the mean squared error (MSE). . loss = torch.nn.MSELoss() . Optimizers . Rather than manually updating the weights of the model as we have been doing, we use the optim package to define an Optimizer that will update the weights for us. . All optimizers . For this model the optimizer is SGD -&gt; Stochastic Gradient Descent . optimizer = torch.optim.SGD(net.parameters(), lr=0.05) . Training . For a number of epochs Get each features and label Use the model to predict the features into output | Calculate the loss between output and real label | Set the otimizer&#39;s gradient to 0 | l.backward() back propagates the loss calculated for each x that requires gradient | optimizer.step() updates the weights | . | . | . num_epochs = 375 for epoch in range(num_epochs): for x, y in train_iter: output = net(x) l = loss(output, y) optimizer.zero_grad() l.backward() optimizer.step() if epoch % 25 == 0: print(&quot;epoch {} loss: {:.4f}&quot;.format(epoch, l.item())) . epoch 0 loss: 43.4257 epoch 25 loss: 40.3690 epoch 50 loss: 21.1435 epoch 75 loss: 21.3177 epoch 100 loss: 7.8272 epoch 125 loss: 176.3772 epoch 150 loss: 51.4810 epoch 175 loss: 15.8238 epoch 200 loss: 19.9211 epoch 225 loss: 5.0811 epoch 250 loss: 17.8753 epoch 275 loss: 17.3649 epoch 300 loss: 8.3193 epoch 325 loss: 11.2076 epoch 350 loss: 13.8668 . Metrics . pred = pd.DataFrame({ &#39;true&#39;: [x[0].tolist() for x in y_test], &#39;predicted&#39;: [x[0].tolist() for x in net(X_test)], }) pred.plot.hist(alpha=0.5,figsize=(16,7),title=f&#39;MSE: {loss(net(X_test), y_test).item():.4f}&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f76868560f0&gt; . GLM Linear Regression . import statsmodels.api as sm import patsy . Load the data . boston_dataset = load_boston() boston = pd.DataFrame(data=boston_dataset.get(&#39;data&#39;),columns=boston_dataset.get(&#39;feature_names&#39;)) boston[&#39;PRICE&#39;] = boston_dataset.get(&#39;target&#39;) boston . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 501 0.06263 | 0.0 | 11.93 | 0.0 | 0.573 | 6.593 | 69.1 | 2.4786 | 1.0 | 273.0 | 21.0 | 391.99 | 9.67 | 22.4 | . 502 0.04527 | 0.0 | 11.93 | 0.0 | 0.573 | 6.120 | 76.7 | 2.2875 | 1.0 | 273.0 | 21.0 | 396.90 | 9.08 | 20.6 | . 503 0.06076 | 0.0 | 11.93 | 0.0 | 0.573 | 6.976 | 91.0 | 2.1675 | 1.0 | 273.0 | 21.0 | 396.90 | 5.64 | 23.9 | . 504 0.10959 | 0.0 | 11.93 | 0.0 | 0.573 | 6.794 | 89.3 | 2.3889 | 1.0 | 273.0 | 21.0 | 393.45 | 6.48 | 22.0 | . 505 0.04741 | 0.0 | 11.93 | 0.0 | 0.573 | 6.030 | 80.8 | 2.5050 | 1.0 | 273.0 | 21.0 | 396.90 | 7.88 | 11.9 | . 506 rows × 14 columns . We split again the data in train/test but this time a bit different . train,test = train_test_split(boston, test_size=0.3, random_state=42) . Then we create the R style formula for our matrices . formula = f&quot;PRICE ~ {&#39; + &#39;.join(boston.columns[:-1])}&quot; formula . &#39;PRICE ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT&#39; . Pass the formula to the dmatrices method from patsy for both train and test dataframes . y_train, X_train = patsy.dmatrices(formula, train, return_type=&#39;matrix&#39;) y_test, X_test = patsy.dmatrices(formula, test, return_type=&#39;matrix&#39;) . General Liniar Mode (GLM) . Using Gaussian Distribution . glm_gaussian = sm.GLM(y_train,X_train,data=train, family=sm.families.Gaussian(sm.families.links.log())).fit() print(glm_gaussian.summary()) . Generalized Linear Model Regression Results ============================================================================== Dep. Variable: PRICE No. Observations: 354 Model: GLM Df Residuals: 340 Model Family: Gaussian Df Model: 13 Link Function: log Scale: 17.746 Method: IRLS Log-Likelihood: -1004.4 Date: Mon, 08 Jun 2020 Deviance: 6033.6 Time: 07:54:17 Pearson chi2: 6.03e+03 No. Iterations: 8 Covariance Type: nonrobust ============================================================================== coef std err z P&gt;|z| [0.025 0.975] Intercept 3.5788 0.230 15.530 0.000 3.127 4.030 CRIM -0.0087 0.003 -2.878 0.004 -0.015 -0.003 ZN 0.0008 0.001 1.464 0.143 -0.000 0.002 INDUS 0.0034 0.003 1.305 0.192 -0.002 0.008 CHAS 0.0648 0.030 2.142 0.032 0.006 0.124 NOX -0.5497 0.196 -2.802 0.005 -0.934 -0.165 RM 0.1375 0.017 8.169 0.000 0.105 0.170 AGE -0.0001 0.001 -0.218 0.827 -0.001 0.001 DIS -0.0476 0.009 -5.450 0.000 -0.065 -0.031 RAD 0.0138 0.003 4.025 0.000 0.007 0.021 TAX -0.0004 0.000 -2.333 0.020 -0.001 -6.53e-05 PTRATIO -0.0343 0.006 -6.234 0.000 -0.045 -0.024 B 0.0006 0.000 2.828 0.005 0.000 0.001 LSTAT -0.0368 0.003 -12.897 0.000 -0.042 -0.031 ============================================================================== . Metrics . pred = pd.DataFrame({ &#39;true&#39;: [x[0] for x in y_test.tolist()], &#39;predicted&#39;: glm_gaussian.predict(X_test) }) pred.plot.hist(alpha=0.5,figsize=(16,7),title=f&quot;MSE: {metrics.mean_squared_error(pred[&#39;true&#39;],pred.predicted):.4f}&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f767caf8ac8&gt; . Using Gamma Distribution . glm_gamma = sm.GLM(y_train,X_train,data=train, family=sm.families.Gamma(sm.families.links.log())).fit() print(glm_gamma.summary()) . Generalized Linear Model Regression Results ============================================================================== Dep. Variable: PRICE No. Observations: 354 Model: GLM Df Residuals: 340 Model Family: Gamma Df Model: 13 Link Function: log Scale: 0.040116 Method: IRLS Log-Likelihood: -997.01 Date: Mon, 08 Jun 2020 Deviance: 12.717 Time: 07:54:34 Pearson chi2: 13.6 No. Iterations: 19 Covariance Type: nonrobust ============================================================================== coef std err z P&gt;|z| [0.025 0.975] Intercept 3.9604 0.250 15.820 0.000 3.470 4.451 CRIM -0.0105 0.002 -6.236 0.000 -0.014 -0.007 ZN 0.0010 0.001 1.387 0.165 -0.000 0.002 INDUS 0.0027 0.003 0.889 0.374 -0.003 0.009 CHAS 0.1144 0.043 2.670 0.008 0.030 0.198 NOX -0.6999 0.196 -3.564 0.000 -1.085 -0.315 RM 0.0902 0.021 4.399 0.000 0.050 0.130 AGE -0.0002 0.001 -0.311 0.756 -0.002 0.001 DIS -0.0495 0.010 -4.951 0.000 -0.069 -0.030 RAD 0.0120 0.003 3.546 0.000 0.005 0.019 TAX -0.0005 0.000 -2.361 0.018 -0.001 -7.64e-05 PTRATIO -0.0363 0.006 -5.693 0.000 -0.049 -0.024 B 0.0006 0.000 4.484 0.000 0.000 0.001 LSTAT -0.0298 0.002 -12.150 0.000 -0.035 -0.025 ============================================================================== . Metrics . pred = pd.DataFrame({ &#39;true&#39;: [x[0] for x in y_test.tolist()], &#39;predicted&#39;: glm_gamma.predict(X_test) }) pred.plot.hist(alpha=0.5,figsize=(16,7),title=f&quot;MSE: {metrics.mean_squared_error(pred[&#39;true&#39;],pred.predicted):.4f}&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1025873940&gt; . XGBoost Linear Regression . import xgboost as xgb . Load the data . boston_dataset = load_boston() boston = pd.DataFrame(data=boston_dataset.get(&#39;data&#39;),columns=boston_dataset.get(&#39;feature_names&#39;)) boston[&#39;PRICE&#39;] = boston_dataset.get(&#39;target&#39;) boston . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 501 0.06263 | 0.0 | 11.93 | 0.0 | 0.573 | 6.593 | 69.1 | 2.4786 | 1.0 | 273.0 | 21.0 | 391.99 | 9.67 | 22.4 | . 502 0.04527 | 0.0 | 11.93 | 0.0 | 0.573 | 6.120 | 76.7 | 2.2875 | 1.0 | 273.0 | 21.0 | 396.90 | 9.08 | 20.6 | . 503 0.06076 | 0.0 | 11.93 | 0.0 | 0.573 | 6.976 | 91.0 | 2.1675 | 1.0 | 273.0 | 21.0 | 396.90 | 5.64 | 23.9 | . 504 0.10959 | 0.0 | 11.93 | 0.0 | 0.573 | 6.794 | 89.3 | 2.3889 | 1.0 | 273.0 | 21.0 | 393.45 | 6.48 | 22.0 | . 505 0.04741 | 0.0 | 11.93 | 0.0 | 0.573 | 6.030 | 80.8 | 2.5050 | 1.0 | 273.0 | 21.0 | 396.90 | 7.88 | 11.9 | . 506 rows × 14 columns . Train/Test Split . X_train, X_test, y_train, y_test = train_test_split(boston[boston_dataset.get(&#39;feature_names&#39;)], boston[&#39;PRICE&#39;], test_size=0.3, random_state=42) . Train a Regression model using reg:gamma for objective aka Gamma distribution . reg = xgb.XGBRegressor(n_estimators=1000,objective=&#39;reg:gamma&#39;) reg.fit(X_train, y_train,eval_set=[(X_test, y_test)],eval_metric=&#39;rmse&#39;,verbose=100) . [0] validation_0-rmse:22.5723 [100] validation_0-rmse:3.35307 [200] validation_0-rmse:3.17188 [300] validation_0-rmse:3.14021 [400] validation_0-rmse:3.12192 [500] validation_0-rmse:3.1099 [600] validation_0-rmse:3.11599 [700] validation_0-rmse:3.12263 [800] validation_0-rmse:3.12669 [900] validation_0-rmse:3.13014 [999] validation_0-rmse:3.13269 . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, importance_type=&#39;gain&#39;, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1, nthread=None, objective=&#39;reg:gamma&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) . Metrics . pred = pd.DataFrame({ &#39;true&#39;: y_test.values, &#39;predicted&#39;: reg.predict(X_test) }) pred.plot.hist(alpha=0.5,figsize=(16,7),title=f&quot;MSE: {metrics.mean_squared_error(pred[&#39;true&#39;],pred.predicted):.4f}&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7685700320&gt; . Classification . MNIST Dataset . . Training: 60,000 samples . Test: 10,000 samples . Image Dimension: 28x28 . Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Dataset Original Link . The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems . from torchvision import datasets, transforms . Compose a transformer to nomralize the data . transforms.ToTensor() converts the image into numbers, that are understandable by the system. It separates the image into three color channels (separate images): red, green &amp; blue. Then it converts the pixels of each image to the brightness of their color between 0 and 255. These values are then scaled down to a range between 0 and 1. . | transforms.Normalize() normalizes the tensor with a mean and standard deviation which goes as the two parameters respectively. . | . transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), ]) . Download the dataset and normalize it . trainset = datasets.MNIST(&#39;drive/My Drive/mnist/MNIST_data/&#39;, download=True, train=True, transform=transform) valset = datasets.MNIST(&#39;drive/My Drive/mnist/MNIST_data/&#39;, download=True, train=False, transform=transform) . Load the train/test into a DataLoader . trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) . dataiter = iter(trainloader) images, labels = dataiter.next() print(type(images)) print(images.shape) print(labels.shape) . &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([64, 1, 28, 28]) torch.Size([64]) . def plot_digit(digit,**fig_params): plt.figure(**fig_params) plt.axis(&#39;off&#39;) plt.imshow(digit.numpy().squeeze(), cmap=&#39;gray_r&#39;) . plot_digit(images[13],figsize=(1,1)) . PyTorch Classification . Sources: . Handwritten Digit Recognition Using PyTorch — Intro To Neural Networks | Handwritten Digit Recognition | . Then we define a Seqeuntial model with 3 levels of layers, Linear which applies a linear transformation and ReLu which applies the rectified linear, the output of this chain of transofrmation being passed into LogSoftmax activation function . input_size = 784 hidden_sizes = [128, 64] output_size = 10 # Build a feed-forward network model = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_sizes[0]), torch.nn.ReLU(), torch.nn.Linear(hidden_sizes[0], hidden_sizes[1]), torch.nn.ReLU(), torch.nn.Linear(hidden_sizes[1], output_size), torch.nn.LogSoftmax(dim=1)) print(model) . Sequential( (0): Linear(in_features=784, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=64, bias=True) (3): ReLU() (4): Linear(in_features=64, out_features=10, bias=True) (5): LogSoftmax() ) . Our loss function is NegativeLogLoss . loss = torch.nn.NLLLoss() . For this model the optimizer is SGD -&gt; Stochastic Gradient Descent . optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9) . This time on the training process we reshape each image matrix into one 1x1 array . time0 = time() epochs = 15 for e in range(epochs): running_loss = 0 for images, labels in trainloader: # Flatten MNIST images into a 784 long vector images = images.view(images.shape[0], -1) optimizer.zero_grad() output = model(images) l = loss(output, labels) l.backward() optimizer.step() running_loss += l.item() else: print(&quot;Epoch {} - Training loss: {}&quot;.format(e, running_loss/len(trainloader))) print(&quot; nTraining Time (in minutes) =&quot;,(time()-time0)/60) . Epoch 0 - Training loss: 0.6175047809492423 Epoch 1 - Training loss: 0.27926216799535475 Epoch 2 - Training loss: 0.21707710018877918 Epoch 3 - Training loss: 0.17828493828434488 Epoch 4 - Training loss: 0.14850337554349194 Epoch 5 - Training loss: 0.12661053494476815 Epoch 6 - Training loss: 0.11251892998659693 Epoch 7 - Training loss: 0.10000329123718589 Epoch 8 - Training loss: 0.08876785766164552 Epoch 9 - Training loss: 0.08140811096054754 Epoch 10 - Training loss: 0.07434628869015683 Epoch 11 - Training loss: 0.06872579670681962 Epoch 12 - Training loss: 0.06227882651151466 Epoch 13 - Training loss: 0.05694495400846767 Epoch 14 - Training loss: 0.05275964385930147 Training Time (in minutes) = 3.717697087923686 . Validation Process . y_true,y_pred = list(),list() correct_count, all_count = 0, 0 for images,labels in valloader: for i in range(len(labels)): img = images[i].view(1, 784) # Turn off gradients to speed up this part with torch.no_grad(): logps = model(img) # Output of the network are log-probabilities, need to take exponential for probabilities ps = torch.exp(logps) probab = list(ps.numpy()[0]) pred_label = probab.index(max(probab)) true_label = labels.numpy()[i] y_true.append(true_label) y_pred.append(pred_label) if(true_label == pred_label): correct_count += 1 all_count += 1 print(&quot;Number Of Images Tested =&quot;, all_count) print(&quot; nModel Accuracy =&quot;, (correct_count/all_count)) . Number Of Images Tested = 10000 Model Accuracy = 0.9745 . Classification Report . print(metrics.classification_report(y_true=y_true,y_pred=y_pred)) . precision recall f1-score support 0 0.98 0.99 0.98 980 1 0.98 0.99 0.99 1135 2 0.98 0.97 0.97 1032 3 0.97 0.98 0.97 1010 4 0.97 0.97 0.97 982 5 0.97 0.96 0.97 892 6 0.98 0.97 0.98 958 7 0.96 0.98 0.97 1028 8 0.98 0.97 0.97 974 9 0.98 0.95 0.96 1009 accuracy 0.97 10000 macro avg 0.97 0.97 0.97 10000 weighted avg 0.97 0.97 0.97 10000 . Confusion Matrix . pd.DataFrame(data=metrics.confusion_matrix(y_true=y_true,y_pred=y_pred)) . 0 1 2 3 4 5 6 7 8 9 . 0 968 | 0 | 0 | 1 | 0 | 4 | 2 | 2 | 2 | 1 | . 1 0 | 1124 | 3 | 2 | 0 | 1 | 1 | 1 | 3 | 0 | . 2 4 | 1 | 1005 | 5 | 3 | 0 | 1 | 9 | 4 | 0 | . 3 0 | 0 | 3 | 991 | 0 | 5 | 0 | 6 | 4 | 1 | . 4 0 | 0 | 7 | 1 | 953 | 0 | 1 | 6 | 1 | 13 | . 5 4 | 2 | 0 | 11 | 2 | 860 | 6 | 1 | 2 | 4 | . 6 5 | 4 | 1 | 0 | 4 | 7 | 932 | 0 | 5 | 0 | . 7 1 | 6 | 7 | 1 | 0 | 0 | 0 | 1010 | 0 | 3 | . 8 3 | 2 | 4 | 4 | 4 | 3 | 3 | 3 | 947 | 1 | . 9 2 | 5 | 0 | 7 | 14 | 3 | 1 | 19 | 3 | 955 | . plt.figure(figsize=(15,10)) sns.heatmap(pd.DataFrame(data=metrics.confusion_matrix(y_true=y_true,y_pred=y_pred),columns=list(range(10)),index=list(range(10))),cmap=&#39;Blues&#39;,annot=True, fmt=&quot;d&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe37524ba20&gt; . XGBoost Classification . Sources: . Ensemble Learning case study: Running XGBoost on Google Colab free GPU | Multiclass &amp; Multilabel Classification with XGBoost | . import xgboost as xgb . Extract train/test data from the data loaders . X_train = np.array([x.flatten() for x in trainset.data.numpy()]) y_train = np.array([x.flatten() for x in trainset.targets.numpy()]) X_test = np.array([x.flatten() for x in valset.data.numpy()]) y_test = np.array([x.flatten() for x in valset.targets.numpy()]) . For the XGBClassifier the multi:softmax objective is used to permit training on multiple label classificaiton . %%time xgc = xgb.XGBClassifier(n_jobs=-1,objective=&#39;multi:softmax&#39;,num_class=10 ,max_depth=4) xgc.fit(X_train,y_train.ravel()) . CPU times: user 2min 48s, sys: 1min 55s, total: 4min 44s Wall time: 4min 44s . Classification Report . preds = pd.DataFrame({ &#39;true&#39;: y_test.ravel(), &#39;preds&#39;: xgc.predict(X_test) }) print(metrics.classification_report(y_true=preds[&#39;true&#39;],y_pred=preds[&#39;preds&#39;])) . precision recall f1-score support 0 0.96 0.99 0.97 980 1 0.98 0.99 0.99 1135 2 0.95 0.95 0.95 1032 3 0.96 0.95 0.95 1010 4 0.96 0.94 0.95 982 5 0.96 0.95 0.95 892 6 0.97 0.97 0.97 958 7 0.96 0.94 0.95 1028 8 0.94 0.94 0.94 974 9 0.91 0.94 0.93 1009 accuracy 0.95 10000 macro avg 0.95 0.95 0.95 10000 weighted avg 0.95 0.95 0.95 10000 . Confusion Matrix . pd.DataFrame(data=metrics.confusion_matrix(y_true=preds[&#39;true&#39;],y_pred=preds[&#39;preds&#39;])) . 0 1 2 3 4 5 6 7 8 9 . 0 969 | 0 | 1 | 0 | 0 | 1 | 3 | 1 | 4 | 1 | . 1 0 | 1123 | 2 | 1 | 0 | 1 | 4 | 1 | 3 | 0 | . 2 9 | 0 | 978 | 12 | 9 | 0 | 1 | 11 | 10 | 2 | . 3 2 | 0 | 14 | 956 | 0 | 11 | 2 | 9 | 8 | 8 | . 4 2 | 0 | 4 | 0 | 922 | 1 | 7 | 1 | 5 | 40 | . 5 5 | 3 | 2 | 13 | 1 | 843 | 6 | 4 | 10 | 5 | . 6 7 | 3 | 0 | 0 | 5 | 11 | 925 | 0 | 7 | 0 | . 7 2 | 6 | 24 | 3 | 3 | 1 | 0 | 963 | 6 | 20 | . 8 5 | 2 | 5 | 5 | 4 | 5 | 6 | 7 | 918 | 17 | . 9 8 | 7 | 2 | 8 | 15 | 5 | 0 | 6 | 7 | 951 | . plt.figure(figsize=(15,10)) sns.heatmap(pd.DataFrame(data=metrics.confusion_matrix(y_true=preds[&#39;true&#39;],y_pred=preds[&#39;preds&#39;]),columns=list(range(10)),index=list(range(10))),cmap=&#39;Blues&#39;,annot=True, fmt=&quot;d&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe374f9ea90&gt; .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/PytorchHero.html",
            "relUrl": "/2020/10/09/PytorchHero.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Divorce Predictors",
            "content": ". Imports . import os,re,zipfile import pandas as pd import numpy as np from types import SimpleNamespace from matplotlib import pyplot as plt plt.style.use(&#39;dark_background&#39;) plt.style.use(&#39;seaborn&#39;) . Data . Data Dictionary . If one of us apologizes when our discussion deteriorates, the discussion ends. | I know we can ignore our differences, even if things get hard sometimes. | When we need it, we can take our discussions with my spouse from the beginning and correct it. | When I discuss with my spouse, to contact him will eventually work. | The time I spent with my wife is special for us. | We don&#39;t have time at home as partners. | We are like two strangers who share the same environment at home rather than family. | I enjoy our holidays with my wife. | I enjoy traveling with my wife. | Most of our goals are common to my spouse. | I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other. | My spouse and I have similar values in terms of personal freedom. | My spouse and I have similar sense of entertainment. | Most of our goals for people (children, friends, etc.) are the same. | Our dreams with my spouse are similar and harmonious. | We&#39;re compatible with my spouse about what love should be. | We share the same views about being happy in our life with my spouse | My spouse and I have similar ideas about how marriage should be | My spouse and I have similar ideas about how roles should be in marriage | My spouse and I have similar values in trust. | I know exactly what my wife likes. | I know how my spouse wants to be taken care of when she/he sick. | I know my spouse&#39;s favorite food. | I can tell you what kind of stress my spouse is facing in her/his life. | I have knowledge of my spouse&#39;s inner world. | I know my spouse&#39;s basic anxieties. | I know what my spouse&#39;s current sources of stress are. | I know my spouse&#39;s hopes and wishes. | I know my spouse very well. | I know my spouse&#39;s friends and their social relationships. | I feel aggressive when I argue with my spouse. | When discussing with my spouse, I usually use expressions such as ‘you always’ or ‘you never’ . | I can use negative statements about my spouse&#39;s personality during our discussions. | I can use offensive expressions during our discussions. | I can insult my spouse during our discussions. | I can be humiliating when we discussions. | My discussion with my spouse is not calm. | I hate my spouse&#39;s way of open a subject. | Our discussions often occur suddenly. | We&#39;re just starting a discussion before I know what&#39;s going on. | When I talk to my spouse about something, my calm suddenly breaks. | When I argue with my spouse, ı only go out and I don&#39;t say a word. | I mostly stay silent to calm the environment a little bit. | Sometimes I think it&#39;s good for me to leave home for a while. | I&#39;d rather stay silent than discuss with my spouse. | Even if I&#39;m right in the discussion, I stay silent to hurt my spouse. | When I discuss with my spouse, I stay silent because I am afraid of not being able to control my anger. | I feel right in our discussions. | I have nothing to do with what I&#39;ve been accused of. | I&#39;m not actually the one who&#39;s guilty about what I&#39;m accused of. | I&#39;m not the one who&#39;s wrong about problems at home. | I wouldn&#39;t hesitate to tell my spouse about her/his inadequacy. | When I discuss, I remind my spouse of her/his inadequacy. | I&#39;m not afraid to tell my spouse about her/his incompetence. | Load Data . zip_name = &#39;divorce.csv&#39; if zip_name not in os.listdir(&#39;.&#39;): !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00497/divorce.rar !unrar e divorce.rar os.listdir(&#39;.&#39;) . [&#39;.config&#39;, &#39;divorce.csv&#39;, &#39;divorce.xlsx&#39;, &#39;divorce.rar&#39;, &#39;divorce_text.csv&#39;, &#39;sample_data&#39;] . df = pd.read_csv(&#39;divorce.csv&#39;,sep=&#39;;&#39;) df . Atr1 Atr2 Atr3 Atr4 Atr5 Atr6 Atr7 Atr8 Atr9 Atr10 Atr11 Atr12 Atr13 Atr14 Atr15 Atr16 Atr17 Atr18 Atr19 Atr20 Atr21 Atr22 Atr23 Atr24 Atr25 Atr26 Atr27 Atr28 Atr29 Atr30 Atr31 Atr32 Atr33 Atr34 Atr35 Atr36 Atr37 Atr38 Atr39 Atr40 Atr41 Atr42 Atr43 Atr44 Atr45 Atr46 Atr47 Atr48 Atr49 Atr50 Atr51 Atr52 Atr53 Atr54 Class . 0 2 | 2 | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 2 | 1 | 2 | 0 | 1 | 2 | 1 | 3 | 3 | 2 | 1 | 1 | 2 | 3 | 2 | 1 | 3 | 3 | 3 | 2 | 3 | 2 | 1 | 1 | . 1 4 | 4 | 4 | 4 | 4 | 0 | 0 | 4 | 4 | 4 | 4 | 3 | 4 | 0 | 4 | 4 | 4 | 4 | 3 | 2 | 1 | 1 | 0 | 2 | 2 | 1 | 2 | 0 | 1 | 1 | 0 | 4 | 2 | 3 | 0 | 2 | 3 | 4 | 2 | 4 | 2 | 2 | 3 | 4 | 2 | 2 | 2 | 3 | 4 | 4 | 4 | 4 | 2 | 2 | 1 | . 2 2 | 2 | 2 | 2 | 1 | 3 | 2 | 1 | 1 | 2 | 3 | 4 | 2 | 3 | 3 | 3 | 3 | 3 | 3 | 2 | 1 | 0 | 1 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 3 | 3 | 1 | 1 | 1 | 1 | 2 | 1 | 3 | 3 | 3 | 3 | 2 | 3 | 2 | 3 | 2 | 3 | 1 | 1 | 1 | 2 | 2 | 2 | 1 | . 3 3 | 2 | 3 | 2 | 3 | 3 | 3 | 3 | 3 | 3 | 4 | 3 | 3 | 4 | 3 | 3 | 3 | 3 | 3 | 4 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | 3 | 2 | 3 | 2 | 2 | 1 | 1 | 3 | 3 | 4 | 4 | 2 | 2 | 3 | 2 | 3 | 2 | 2 | 3 | 3 | 3 | 3 | 2 | 2 | 2 | 1 | . 4 2 | 2 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 2 | 1 | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 2 | 3 | 0 | 2 | 2 | 1 | 2 | 3 | 2 | 2 | 2 | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 165 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 4 | 3 | 4 | 0 | 0 | 4 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 4 | 1 | 1 | 4 | 2 | 2 | 2 | 0 | . 166 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 3 | 4 | 1 | 2 | 2 | 2 | 2 | 3 | 2 | 2 | 0 | . 167 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 2 | 1 | 3 | 3 | 0 | 2 | 3 | 0 | 2 | 0 | 1 | 1 | 3 | 0 | 0 | 0 | . 168 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 4 | 1 | 2 | 1 | 1 | 0 | 4 | 3 | 3 | 2 | 2 | 3 | 2 | 4 | 3 | 1 | 0 | . 169 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 2 | 0 | 1 | 3 | 4 | 4 | 0 | 1 | 3 | 3 | 3 | 1 | 0 | . 170 rows × 55 columns . Model . XGBoost + Hyper-Parameters Tuning . import xgboost as xgb from sklearn.model_selection import GridSearchCV from sklearn.ensemble import IsolationForest . param_grid = { &quot;learning_rate&quot; : [0.05, 0.10 ], &quot;max_depth&quot; : [ 1, 4, 7, 14, 20], &quot;min_child_weight&quot; : [ 3, 5, 7 ], &quot;gamma&quot; : [ 0.1, 0.3], &quot;colsample_bytree&quot; : [ 0.3, 0.5 , 0.7 ], &quot;n_estimators&quot; : [ 1000 ], &quot;objective&quot;: [&#39;binary:logistic&#39;,&#39;multi:softmax&#39;,&#39;multi:softprob&#39;], &quot;num_class&quot;: [df.Class.nunique()] } . xgc = xgb.XGBClassifier() grid = GridSearchCV(xgc, param_grid, cv=3,verbose=10,n_jobs=-1) grid.fit(df[df.columns[:-1]],df[&#39;Class&#39;]) . Fitting 3 folds for each of 540 candidates, totalling 1620 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=-1)]: Done 1 tasks | elapsed: 1.3s [Parallel(n_jobs=-1)]: Done 4 tasks | elapsed: 1.6s [Parallel(n_jobs=-1)]: Done 9 tasks | elapsed: 2.1s [Parallel(n_jobs=-1)]: Done 14 tasks | elapsed: 2.5s [Parallel(n_jobs=-1)]: Done 21 tasks | elapsed: 3.0s [Parallel(n_jobs=-1)]: Done 28 tasks | elapsed: 3.7s [Parallel(n_jobs=-1)]: Done 37 tasks | elapsed: 4.5s [Parallel(n_jobs=-1)]: Done 46 tasks | elapsed: 5.3s [Parallel(n_jobs=-1)]: Done 57 tasks | elapsed: 6.1s [Parallel(n_jobs=-1)]: Done 68 tasks | elapsed: 7.2s [Parallel(n_jobs=-1)]: Done 81 tasks | elapsed: 8.4s [Parallel(n_jobs=-1)]: Done 94 tasks | elapsed: 9.5s [Parallel(n_jobs=-1)]: Done 109 tasks | elapsed: 10.8s [Parallel(n_jobs=-1)]: Done 124 tasks | elapsed: 12.1s [Parallel(n_jobs=-1)]: Done 141 tasks | elapsed: 13.7s [Parallel(n_jobs=-1)]: Done 158 tasks | elapsed: 15.0s [Parallel(n_jobs=-1)]: Done 177 tasks | elapsed: 16.8s [Parallel(n_jobs=-1)]: Done 196 tasks | elapsed: 18.3s [Parallel(n_jobs=-1)]: Done 217 tasks | elapsed: 20.2s [Parallel(n_jobs=-1)]: Done 238 tasks | elapsed: 22.0s [Parallel(n_jobs=-1)]: Done 261 tasks | elapsed: 24.0s [Parallel(n_jobs=-1)]: Done 284 tasks | elapsed: 25.9s [Parallel(n_jobs=-1)]: Done 309 tasks | elapsed: 27.9s [Parallel(n_jobs=-1)]: Done 334 tasks | elapsed: 30.3s [Parallel(n_jobs=-1)]: Done 361 tasks | elapsed: 32.6s [Parallel(n_jobs=-1)]: Done 388 tasks | elapsed: 35.0s [Parallel(n_jobs=-1)]: Done 417 tasks | elapsed: 37.4s [Parallel(n_jobs=-1)]: Done 446 tasks | elapsed: 40.0s [Parallel(n_jobs=-1)]: Done 477 tasks | elapsed: 42.9s [Parallel(n_jobs=-1)]: Done 508 tasks | elapsed: 45.6s [Parallel(n_jobs=-1)]: Done 541 tasks | elapsed: 48.4s [Parallel(n_jobs=-1)]: Done 574 tasks | elapsed: 51.7s [Parallel(n_jobs=-1)]: Done 609 tasks | elapsed: 55.3s [Parallel(n_jobs=-1)]: Done 644 tasks | elapsed: 58.6s [Parallel(n_jobs=-1)]: Done 681 tasks | elapsed: 1.0min [Parallel(n_jobs=-1)]: Done 718 tasks | elapsed: 1.1min [Parallel(n_jobs=-1)]: Done 757 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 796 tasks | elapsed: 1.2min [Parallel(n_jobs=-1)]: Done 837 tasks | elapsed: 1.3min [Parallel(n_jobs=-1)]: Done 878 tasks | elapsed: 1.4min [Parallel(n_jobs=-1)]: Done 921 tasks | elapsed: 1.4min [Parallel(n_jobs=-1)]: Done 964 tasks | elapsed: 1.5min [Parallel(n_jobs=-1)]: Done 1009 tasks | elapsed: 1.6min [Parallel(n_jobs=-1)]: Done 1054 tasks | elapsed: 1.7min [Parallel(n_jobs=-1)]: Done 1101 tasks | elapsed: 1.7min [Parallel(n_jobs=-1)]: Done 1148 tasks | elapsed: 1.8min [Parallel(n_jobs=-1)]: Done 1197 tasks | elapsed: 1.9min [Parallel(n_jobs=-1)]: Done 1246 tasks | elapsed: 2.0min [Parallel(n_jobs=-1)]: Done 1297 tasks | elapsed: 2.1min [Parallel(n_jobs=-1)]: Done 1348 tasks | elapsed: 2.2min [Parallel(n_jobs=-1)]: Done 1401 tasks | elapsed: 2.3min [Parallel(n_jobs=-1)]: Done 1454 tasks | elapsed: 2.4min [Parallel(n_jobs=-1)]: Done 1509 tasks | elapsed: 2.5min [Parallel(n_jobs=-1)]: Done 1564 tasks | elapsed: 2.6min [Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed: 2.7min finished . GridSearchCV(cv=3, error_score=nan, estimator=XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;binary:logistic&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_po... iid=&#39;deprecated&#39;, n_jobs=-1, param_grid={&#39;colsample_bytree&#39;: [0.3, 0.5, 0.7], &#39;gamma&#39;: [0.1, 0.3], &#39;learning_rate&#39;: [0.05, 0.1], &#39;max_depth&#39;: [1, 4, 7, 14, 20], &#39;min_child_weight&#39;: [3, 5, 7], &#39;n_estimators&#39;: [1000], &#39;num_class&#39;: [2], &#39;objective&#39;: [&#39;binary:logistic&#39;, &#39;multi:softmax&#39;, &#39;multi:softprob&#39;]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=10) . Best Hyper-Parameters . for k,v in grid.best_params_.items(): print(f&#39;{k} : {v}&#39;) . colsample_bytree : 0.3 gamma : 0.1 learning_rate : 0.1 max_depth : 4 min_child_weight : 3 n_estimators : 1000 num_class : 2 objective : multi:softmax . Feature Importance . After tuning the most important questions are: . My spouse and I have similar ideas about how marriage should be | | My spouse and I have similar values in trust. | | We&#39;re just starting a discussion before I know what&#39;s going on. | | . fig,ax = plt.subplots(1,1,figsize=(7,10)) xgb.plot_importance(grid.best_estimator_,height=.4,ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1e142cc2e8&gt; . Anomalies . isf = IsolationForest(n_estimators=1000,n_jobs=-1) anomalies = isf.fit_predict(df) . df[&#39;anomalies&#39;] = anomalies . df.anomalies.value_counts().plot.bar(rot=0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1e0cfc64e0&gt; . Top 3 Samples with Anomalies . df[df.anomalies == -1][:3] . Atr1 Atr2 Atr3 Atr4 Atr5 Atr6 Atr7 Atr8 Atr9 Atr10 Atr11 Atr12 Atr13 Atr14 Atr15 Atr16 Atr17 Atr18 Atr19 Atr20 Atr21 Atr22 Atr23 Atr24 Atr25 Atr26 Atr27 Atr28 Atr29 Atr30 Atr31 Atr32 Atr33 Atr34 Atr35 Atr36 Atr37 Atr38 Atr39 Atr40 Atr41 Atr42 Atr43 Atr44 Atr45 Atr46 Atr47 Atr48 Atr49 Atr50 Atr51 Atr52 Atr53 Atr54 Class anomalies . 0 2 | 2 | 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 2 | 1 | 2 | 0 | 1 | 2 | 1 | 3 | 3 | 2 | 1 | 1 | 2 | 3 | 2 | 1 | 3 | 3 | 3 | 2 | 3 | 2 | 1 | 1 | -1 | . 1 4 | 4 | 4 | 4 | 4 | 0 | 0 | 4 | 4 | 4 | 4 | 3 | 4 | 0 | 4 | 4 | 4 | 4 | 3 | 2 | 1 | 1 | 0 | 2 | 2 | 1 | 2 | 0 | 1 | 1 | 0 | 4 | 2 | 3 | 0 | 2 | 3 | 4 | 2 | 4 | 2 | 2 | 3 | 4 | 2 | 2 | 2 | 3 | 4 | 4 | 4 | 4 | 2 | 2 | 1 | -1 | . 2 2 | 2 | 2 | 2 | 1 | 3 | 2 | 1 | 1 | 2 | 3 | 4 | 2 | 3 | 3 | 3 | 3 | 3 | 3 | 2 | 1 | 0 | 1 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 3 | 3 | 1 | 1 | 1 | 1 | 2 | 1 | 3 | 3 | 3 | 3 | 2 | 3 | 2 | 3 | 2 | 3 | 1 | 1 | 1 | 2 | 2 | 2 | 1 | -1 | .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/DivorcePredictors.html",
            "relUrl": "/2020/10/09/DivorcePredictors.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Caesar Cipher",
            "content": ". The action of a Caesar cipher is to replace each plaintext letter with a different one a fixed number of places down the alphabet. The cipher illustrated here uses a left shift of three, so that (for example) each occurrence of E in the plaintext becomes B in the ciphertext. In cryptography, a Caesar cipher, also known as Caesar&#39;s cipher, the shift cipher, Caesar&#39;s code or Caesar shift, is one of the simplest and most widely known encryption techniques. It is a type of substitution cipher in which each letter in the plaintext is replaced by a letter some fixed number of positions down the alphabet. For example, with a left shift of 3, D would be replaced by A, E would become B, and so on. The method is named after Julius Caesar, who used it in his private correspondence. . The encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenère cipher, and still has modern application in the ROT13 system. As with all single-alphabet substitution ciphers, the Caesar cipher is easily broken and in modern practice offers essentially no communications security. . We want to use this package to generate random texts . try: import lorem except: !pip install lorem import lorem . now everytime we will run this cell the s variable will have a random sentence in latin . s = lorem.sentence() s . &#39;Tempora neque est magnam numquam.&#39; . Using this function we and the shift parameter we can rotate the ASCII code for each letter in the sentence . ASCII stands for American Standard Code for Information Interchange. Computers can only understand numbers, so an ASCII code is the numerical representation of a character such as &#39;a&#39; or &#39;@&#39; or an action of some sort. ASCII was developed a long time ago and now the non-printing characters are rarely used for their original purpose. Below is the ASCII character table and this includes descriptions of the first 32 non-printing characters. ASCII was actually designed for use with teletypes and so the descriptions are somewhat obscure. If someone says they want your CV however in ASCII format, all this means is they want &#39;plain&#39; text with no formatting such as tabs, bold or underscoring - the raw format that any computer can understand. This is usually so they can easily import the file into their own applications without issues. Notepad.exe creates ASCII text, or in MS Word you can save a file as &#39;text only&#39; . . I am sure there might be multiple implementations of this function but here is mine . def caesar(text,shift=3): def to_cipher(x,shift): x = ord(x) if x &lt;= 122 and x &gt;= 93: if (x + shift) &gt;= 122: return chr(x + shift - 122 + 93) else: return chr(x + shift) if x &lt;= 90 and x &gt;= 65: if (x + shift) &gt; 90: return chr(x + shift - 90 + 65) else: return chr(x + shift) return str(x + shift) return &#39;&#39;.join([ to_cipher(x,shift) for x in list(text)]) . caesar(s,25) . &#39;Tailkn]57jamqa57aop57i]cj]i57jqimq]i71&#39; .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/Caesar-Cipher.html",
            "relUrl": "/2020/10/09/Caesar-Cipher.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "CORD-19 Clustering Articles and Papers related to Coronavirus",
            "content": "Kaggle credentials to download the dataset | Remove duplicates and missing values . %%time import os if not os.path.exists(&#39;CORD-19-research-challenge.zip&#39;): os.environ[&#39;KAGGLE_USERNAME&#39;] = &quot;&quot; # username from the json file os.environ[&#39;KAGGLE_KEY&#39;] = &quot;&quot; # key from the json file !kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge . CPU times: user 125 µs, sys: 0 ns, total: 125 µs Wall time: 220 µs . Find the dataset and unpack it . os.listdir(&#39;./&#39;) . [&#39;.config&#39;, &#39;CORD-19-research-challenge.zip&#39;, &#39;sample_data&#39;] . import pandas as pd import numpy as np from matplotlib import pyplot as plt import zipfile . with zipfile.ZipFile(&quot;CORD-19-research-challenge.zip&quot;) as z: with z.open(&quot;metadata.csv&quot;) as f: df = pd.read_csv(f) . df[:3] . sha source_x title doi pmcid pubmed_id license abstract publish_time authors journal Microsoft Academic Paper ID WHO #Covidence has_full_text full_text_file . 0 NaN | Elsevier | Intrauterine virus infections and congenital h... | 10.1016/0002-8703(72)90077-4 | NaN | 4361535.0 | els-covid | Abstract The etiologic basis for the vast majo... | 1972-12-31 | Overall, James C. | American Heart Journal | NaN | NaN | False | custom_license | . 1 NaN | Elsevier | Coronaviruses in Balkan nephritis | 10.1016/0002-8703(80)90355-5 | NaN | 6243850.0 | els-covid | NaN | 1980-03-31 | Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;... | American Heart Journal | NaN | NaN | False | custom_license | . 2 NaN | Elsevier | Cigarette smoking and coronary heart disease: ... | 10.1016/0002-8703(80)90356-7 | NaN | 7355701.0 | els-covid | NaN | 1980-03-31 | Friedman, Gary D | American Heart Journal | NaN | NaN | False | custom_license | . So far we are interested just in title,abstract and maybe authors . First we get rid of rows with missing values | Then we get rid of the duplicated values | . df = df[[&#39;title&#39;,&#39;abstract&#39;,&#39;authors&#39;]] . df.isnull().sum() . title 224 abstract 8414 authors 3146 dtype: int64 . df.dropna(inplace=True) . df.isnull().sum() . title 0 abstract 0 authors 0 dtype: int64 . df[df.duplicated()].__len__() . 20 . df.drop_duplicates(inplace=True) df[df.duplicated()].__len__() . 0 . df[:3] . title abstract authors . 0 Intrauterine virus infections and congenital h... | Abstract The etiologic basis for the vast majo... | Overall, James C. | . 3 Clinical and immunologic studies in identical ... | Abstract Middle-aged female identical twins, o... | Brunner, Carolyn M.; Horwitz, David A.; Shann,... | . 4 Epidemiology of community-acquired respiratory... | Abstract Upper respiratory tract infections ar... | Garibaldi, Richard A. | . Tensorflow Upgrade . Here we prepare some simple clustering functions to be used later on . !pip install --upgrade tensorflow . Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.1.0) Requirement already satisfied, skipping upgrade: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0) Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1) Requirement already satisfied, skipping upgrade: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1) Requirement already satisfied, skipping upgrade: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2) Requirement already satisfied, skipping upgrade: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2) Requirement already satisfied, skipping upgrade: six&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0) Requirement already satisfied, skipping upgrade: keras-preprocessing&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0) Requirement already satisfied, skipping upgrade: tensorflow-estimator&lt;2.2.0,&gt;=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.0) Requirement already satisfied, skipping upgrade: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0) Requirement already satisfied, skipping upgrade: termcolor&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0) Requirement already satisfied, skipping upgrade: keras-applications&gt;=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8) Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2) Requirement already satisfied, skipping upgrade: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0) Requirement already satisfied, skipping upgrade: tensorboard&lt;2.2.0,&gt;=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1) Requirement already satisfied, skipping upgrade: grpcio&gt;=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3) Requirement already satisfied, skipping upgrade: protobuf&gt;=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0) Requirement already satisfied, skipping upgrade: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1) Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tensorflow) (2.8.0) Requirement already satisfied, skipping upgrade: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (46.0.0) Requirement already satisfied, skipping upgrade: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (0.4.1) Requirement already satisfied, skipping upgrade: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (2.21.0) Requirement already satisfied, skipping upgrade: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (1.7.2) Requirement already satisfied, skipping upgrade: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (3.2.1) Requirement already satisfied, skipping upgrade: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (1.0.0) Requirement already satisfied, skipping upgrade: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (1.3.0) Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (1.24.3) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (2019.11.28) Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (2.8) Requirement already satisfied, skipping upgrade: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (0.2.8) Requirement already satisfied, skipping upgrade: cachetools&lt;3.2,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (3.1.1) Requirement already satisfied, skipping upgrade: rsa&lt;4.1,&gt;=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (4.0) Requirement already satisfied, skipping upgrade: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (3.1.0) Requirement already satisfied, skipping upgrade: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;2.2.0,&gt;=2.1.0-&gt;tensorflow) (0.4.8) . Text features extraction . import tensorflow as tf import tensorflow_hub as hub class UniversalSenteneceEncoder: def __init__(self, encoder=&#39;universal-sentence-encoder&#39;, version=&#39;4&#39;): self.version = version self.encoder = encoder self.embd = hub.load(f&quot;https://tfhub.dev/google/{encoder}/{version}&quot;) def embed(self, sentences): return self.embd(sentences) def squized(self, sentences): return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string)))) . encoder = UniversalSenteneceEncoder(encoder=&#39;universal-sentence-encoder&#39;,version=&#39;4&#39;) . Since this process is quite consuming we will slice the dataset . Title Feature correlation . %%time df[&#39;title_sent_vects&#39;] = encoder.squized(df.title.values).tolist() . CPU times: user 8.13 s, sys: 3.23 s, total: 11.4 s Wall time: 5.22 s . import plotly.graph_objects as go sents = 50 labels = df[:sents].title.values features = df[:sents].title_sent_vects.values.tolist() fig = go.Figure(data=go.Heatmap( z=np.inner(features, features), x=labels, y=labels, colorscale=&#39;Viridis&#39;, )) fig.update_layout( margin=dict(l=40, r=40, t=40, b=40), height=1000, xaxis=dict( autorange=True, showgrid=False, ticks=&#39;&#39;, showticklabels=False ), yaxis=dict( autorange=True, showgrid=False, ticks=&#39;&#39;, showticklabels=False ) ) fig.show() . . . Abstract Features Correlation . # df[&#39;abstract_sent_vects&#39;] = encoder.squized(df.abstract.values).tolist() . # sents = 30 # labels = df[:sents].abstract.values # features = df[:sents].abstract_sent_vects.values.tolist() # fig = go.Figure(data=go.Heatmap( # z=np.inner(features, features), # x=labels, # y=labels, # colorscale=&#39;Viridis&#39;, # )) # fig.update_layout( # margin=dict(l=140, r=140, t=140, b=140), # height=1000, # xaxis=dict( # autorange=True, # showgrid=False, # ticks=&#39;&#39;, # showticklabels=False # ), # yaxis=dict( # autorange=True, # showgrid=False, # ticks=&#39;&#39;, # showticklabels=False # ), # hoverlabel = dict(namelength = -1) # ) # fig.show() . PCA &amp; Clustering . from sklearn.cluster import KMeans import matplotlib.cm as cm from sklearn.decomposition import PCA from datetime import datetime import plotly.express as px . Title Level Clustering . %%time n_clusters = 10 vectors = df.title_sent_vects.values.tolist() kmeans = KMeans(n_clusters = n_clusters, init = &#39;k-means++&#39;, random_state = 0) kmean_indices = kmeans.fit_predict(vectors) pca = PCA(n_components=512) scatter_plot_points = pca.fit_transform(vectors) tmp = pd.DataFrame({ &#39;Feature space for the 1st feature&#39;: scatter_plot_points[:,0], &#39;Feature space for the 2nd feature&#39;: scatter_plot_points[:,1], &#39;labels&#39;: kmean_indices, &#39;title&#39;: df.title.values.tolist()[:vectors.__len__()] }) fig = px.scatter(tmp, x=&#39;Feature space for the 1st feature&#39;, y=&#39;Feature space for the 2nd feature&#39;, color=&#39;labels&#39;, size=&#39;labels&#39;, hover_data=[&#39;title&#39;]) fig.update_layout( margin=dict(l=20, r=20, t=20, b=20), height=1000 ) fig.show() . Abstract Level Clustering . # n_clusters = 10 # vectors = df.abstract_sent_vects.values.tolist() # kmeans = KMeans(n_clusters = n_clusters, init = &#39;k-means++&#39;, random_state = 0) # kmean_indices = kmeans.fit_predict(vectors) # pca = PCA(n_components=512) # scatter_plot_points = pca.fit_transform(vectors) # tmp = pd.DataFrame({ # &#39;Feature space for the 1st feature&#39;: scatter_plot_points[:,0], # &#39;Feature space for the 2nd feature&#39;: scatter_plot_points[:,1], # &#39;labels&#39;: kmean_indices, # &#39;title&#39;: df.abstract.values.tolist()[:vectors.__len__()] # }) # fig = px.scatter(tmp, x=&#39;Feature space for the 1st feature&#39;, y=&#39;Feature space for the 2nd feature&#39;, color=&#39;labels&#39;, # size=&#39;labels&#39;, hover_data=[&#39;title&#39;]) # fig.update_layout( # margin=dict(l=20, r=20, t=20, b=20), # height=1000 # ) # fig.show() . Graphs . Preparing the data for graphs . import networkx as nx . from sklearn.metrics.pairwise import cosine_similarity . Make a copy of our dataframe and create the similarity matrix for the extracted title vectors . %%time sdf = df.copy() similarity_matrix = cosine_similarity(sdf.title_sent_vects.values.tolist()) . CPU times: user 51.4 s, sys: 1min 51s, total: 2min 42s Wall time: 13.9 s . Add them into a dataframe, similiar to what a heatmap looks likes . simdf = pd.DataFrame( similarity_matrix, columns = sdf.title.values.tolist(), index = sdf.title.values.tolist() ) . Let&#39;s unstack them to add them easier into our graph . long_form = simdf.unstack() # rename columns and turn into a dataframe long_form.index.rename([&#39;t1&#39;, &#39;t2&#39;], inplace=True) long_form = long_form.to_frame(&#39;sim&#39;).reset_index() . long_form = long_form[long_form.t1 != long_form.t2] long_form[:3] . Plotly Graph . !pip install python-igraph . import igraph as ig . %%time lng = long_form[long_form.sim &gt; 0.75] tuples = [tuple(x) for x in lng.values] Gm = ig.Graph.TupleList(tuples, edge_attrs = [&#39;sim&#39;]) layt=Gm.layout(&#39;kk&#39;, dim=3) . Xn=[layt[k][0] for k in range(layt.__len__())]# x-coordinates of nodes Yn=[layt[k][1] for k in range(layt.__len__())]# y-coordinates Zn=[layt[k][2] for k in range(layt.__len__())]# z-coordinates Xe=[] Ye=[] Ze=[] for e in Gm.get_edgelist(): Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends Ye+=[layt[e[0]][1],layt[e[1]][1], None] Ze+=[layt[e[0]][2],layt[e[1]][2], None] . import plotly.graph_objs as go . trace1 = go.Scatter3d(x = Xe, y = Ye, z = Ze, mode = &#39;lines&#39;, line = dict(color = &#39;rgb(0,0,0)&#39;, width = 1), hoverinfo = &#39;none&#39; ) trace2 = go.Scatter3d(x = Xn, y = Yn, z = Zn, mode = &#39;markers&#39;, name = &#39;actors&#39;, marker = dict(symbol = &#39;circle&#39;, size = 6, # color = group, colorscale = &#39;Viridis&#39;, line = dict(color = &#39;rgb(50,50,50)&#39;, width = 0.5) ), text = lng.t1.values.tolist(), hoverinfo = &#39;text&#39; ) axis = dict(showbackground = False, showline = False, zeroline = False, showgrid = False, showticklabels = False, title = &#39;&#39; ) layout = go.Layout( title = &quot;Network of similarity between CORD-19 Articles(3D visualization)&quot;, width = 1500, height = 1500, showlegend = False, scene = dict( xaxis = dict(axis), yaxis = dict(axis), zaxis = dict(axis), ), margin = dict( t = 100, l = 20, r = 20 ), ) . fig=go.Figure(data=[trace1,trace2], layout=layout) fig.show() . Finding Communities . Next step, we filter them nodes with higher similarity . sim_weight = 0.75 gdf = long_form[long_form.sim &gt; sim_weight] . We create our graph from our dataframe . plt.figure(figsize=(50,50)) pd_graph = nx.Graph() pd_graph = nx.from_pandas_edgelist(gdf, &#39;t1&#39;, &#39;t2&#39;) pos = nx.spring_layout(pd_graph) nx.draw_networkx(pd_graph,pos,with_labels=True,font_size=10, node_size = 30) . Now let&#39;s try to find communities in our graph . betCent = nx.betweenness_centrality(pd_graph, normalized=True, endpoints=True) node_color = [20000.0 * pd_graph.degree(v) for v in pd_graph] node_size = [v * 10000 for v in betCent.values()] plt.figure(figsize=(35,35)) nx.draw_networkx(pd_graph, pos=pos, with_labels=True, font_size=5, node_color=node_color, node_size=node_size ) . Now let&#39;s get our groups . l=list(nx.connected_components(pd_graph)) L=[dict.fromkeys(y,x) for x, y in enumerate(l)] d=[{&#39;articles&#39;:k , &#39;groups&#39;:v }for d in L for k, v in d.items()] . We&#39;ve got our &#39;clustered&#39; dataframe of articles, however since we filtered the data to take just the most similar articles, we&#39;ve ended up havin left just 660 from the 5k data . Creating word clouds for grooups . gcd = pd.DataFrame.from_dict(d) . import nltk nltk.download(&#39;stopwords&#39;) nltk.download(&#39;punkt&#39;) from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from nltk.tokenize import RegexpTokenizer tok = RegexpTokenizer(r&#39; w+&#39;) stop_words = set(stopwords.words(&#39;english&#39;)) def clean(string): return &quot; &quot;.join([w for w in word_tokenize(&quot; &quot;.join(tok.tokenize(string))) if not w in stop_words]) gcd.articles = gcd.articles.apply(lambda x: clean(x)) . gcd.__len__(),gcd.__len__() / df.__len__() . from wordcloud import WordCloud . %%time clouds = dict() big_groups = pd.DataFrame({ &#39;counts&#39;:gcd.groups.value_counts() }).sort_values(by=&#39;counts&#39;,ascending=False)[:9].index.values.tolist() for group in big_groups: text = gcd[gcd.groups == group].articles.values wordcloud = WordCloud(width=1000, height=1000).generate(str(text)) clouds[group] = wordcloud . def plot_figures(figures, nrows = 1, ncols=1): &quot;&quot;&quot;Plot a dictionary of figures. Parameters - figures : &lt;title, figure&gt; dictionary ncols : number of columns of subplots wanted in the display nrows : number of rows of subplots wanted in the figure &quot;&quot;&quot; fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=(20,20)) for ind,title in zip(range(len(figures)), figures): axeslist.ravel()[ind].imshow(figures[title], cmap=plt.jet()) axeslist.ravel()[ind].set_title(f&#39;Most Freqent words for the group {title+1}&#39;) axeslist.ravel()[ind].set_axis_off() plt.tight_layout() # optional . plot_figures(clouds, 3, 3) plt.show() .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/CORD-19.html",
            "relUrl": "/2020/10/09/CORD-19.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Anime Recomendation System",
            "content": ". Content-Based Filtering . Data Loading . imports . we&#39;ve got some basic system imports, 🐼 and num🐍, 📊lib and fancy SimpleNamespace for using dictionaries cuz &#39;&quot;[] are annoying . import os,re,zipfile import pandas as pd import numpy as np from types import SimpleNamespace from matplotlib import pyplot as plt import itertools plt.style.use(&#39;seaborn&#39;) base_size = 10 sizes = SimpleNamespace(**dict(small=(1*base_size,1*base_size),medium=(2*base_size,2*base_size),large=(3*base_size,3*base_size))) . Downlaod the data . %%time zip_name = &#39;anime-recommendations-database.zip&#39; if not os.path.exists(zip_name): os.environ[&#39;KAGGLE_USERNAME&#39;] = &quot;&quot; # username from the json file os.environ[&#39;KAGGLE_KEY&#39;] = &quot;&quot; # key from the json file !kaggle datasets download CooperUnion/anime-recommendations-database . Downloading anime-recommendations-database.zip to /content 100% 25.0M/25.0M [00:00&lt;00:00, 38.8MB/s] 100% 25.0M/25.0M [00:00&lt;00:00, 83.3MB/s] CPU times: user 24.8 ms, sys: 4.82 ms, total: 29.6 ms Wall time: 2.41 s . Unzip the data . with zipfile.ZipFile(zip_name, &#39;r&#39;) as zip_ref: zip_ref.extractall(zip_name.split(&#39;.&#39;)[0]) . Let&#39;s find the files . os.listdir(zip_name.split(&#39;.&#39;)[0]) . [&#39;anime.csv&#39;, &#39;rating.csv&#39;] . Have a look throught the dataset . anime = pd.read_csv(zip_name.split(&#39;.&#39;)[0] + &#39;/anime.csv&#39;) anime.genre = anime.genre.fillna(&#39;None&#39;) anime . anime_id name genre type episodes rating members . 0 32281 | Kimi no Na wa. | Drama, Romance, School, Supernatural | Movie | 1 | 9.37 | 200630 | . 1 5114 | Fullmetal Alchemist: Brotherhood | Action, Adventure, Drama, Fantasy, Magic, Mili... | TV | 64 | 9.26 | 793665 | . 2 28977 | Gintama° | Action, Comedy, Historical, Parody, Samurai, S... | TV | 51 | 9.25 | 114262 | . 3 9253 | Steins;Gate | Sci-Fi, Thriller | TV | 24 | 9.17 | 673572 | . 4 9969 | Gintama&amp;#039; | Action, Comedy, Historical, Parody, Samurai, S... | TV | 51 | 9.16 | 151266 | . ... ... | ... | ... | ... | ... | ... | ... | . 12289 9316 | Toushindai My Lover: Minami tai Mecha-Minami | Hentai | OVA | 1 | 4.15 | 211 | . 12290 5543 | Under World | Hentai | OVA | 1 | 4.28 | 183 | . 12291 5621 | Violence Gekiga David no Hoshi | Hentai | OVA | 4 | 4.88 | 219 | . 12292 6133 | Violence Gekiga Shin David no Hoshi: Inma Dens... | Hentai | OVA | 1 | 4.98 | 175 | . 12293 26081 | Yasuji no Pornorama: Yacchimae!! | Hentai | Movie | 1 | 5.46 | 142 | . 12294 rows × 7 columns . Data Aggregation . Let&#39;s get a list of unique genre . unique_genres = list(set([x.strip() for x in list(itertools.chain(*anime.genre.fillna(&#39;None&#39;).str.split(&#39;,&#39;)))])) for gen in unique_genres: anime[gen] = 0 . For each row we create something similar with the output of pd.get_dummies where we check the found genres against all the unique ones resulting into an array of 1 and 0 where given genres vs unique_genres . def binary_match(x,y): x = [t.strip() for t in x] y = [t.strip() for t in y] matches = dict(zip(y,np.zeros(len(y)))) for j in y: if j in x: matches[j] = 1 else: matches[j] = 0 return matches . test = anime.genre.apply(lambda x: binary_match(x.split(&#39;,&#39;),unique_genres)) . binary_df = pd.DataFrame(test.values.tolist()) . anime[unique_genres] = binary_df[unique_genres] . anime . anime_id name genre type episodes rating members Military Harem Shoujo Ai Hentai Action Space Ecchi Magic Josei Yaoi Cars Shounen Ai Police Kids Seinen Psychological Drama Music Shoujo School None Fantasy Shounen Vampire Sci-Fi Super Power Horror Demons Sports Supernatural Slice of Life Parody Dementia Romance Yuri Game Adventure Historical Samurai Thriller Martial Arts Comedy Mecha Mystery . 0 32281 | Kimi no Na wa. | Drama, Romance, School, Supernatural | Movie | 1 | 9.37 | 200630 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 5114 | Fullmetal Alchemist: Brotherhood | Action, Adventure, Drama, Fantasy, Magic, Mili... | TV | 64 | 9.26 | 793665 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 28977 | Gintama° | Action, Comedy, Historical, Parody, Samurai, S... | TV | 51 | 9.25 | 114262 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . 3 9253 | Steins;Gate | Sci-Fi, Thriller | TV | 24 | 9.17 | 673572 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 4 9969 | Gintama&amp;#039; | Action, Comedy, Historical, Parody, Samurai, S... | TV | 51 | 9.16 | 151266 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 12289 9316 | Toushindai My Lover: Minami tai Mecha-Minami | Hentai | OVA | 1 | 4.15 | 211 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12290 5543 | Under World | Hentai | OVA | 1 | 4.28 | 183 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12291 5621 | Violence Gekiga David no Hoshi | Hentai | OVA | 4 | 4.88 | 219 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12292 6133 | Violence Gekiga Shin David no Hoshi: Inma Dens... | Hentai | OVA | 1 | 4.98 | 175 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12293 26081 | Yasuji no Pornorama: Yacchimae!! | Hentai | Movie | 1 | 5.46 | 142 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12294 rows × 51 columns . Looks good, right ? . . KMeans Clustering for groups . from sklearn.cluster import KMeans . We use the number of unique genres as the number of clusters for a fast testing . kmeans = KMeans(n_clusters=len(unique_genres), random_state=0).fit(anime[unique_genres]) anime[&#39;clusters&#39;] = kmeans.labels_ . anime.clusters.value_counts().plot.barh(figsize=(10,10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9f632fc198&gt; . Then we define a searching by term function getting all the animes from that cluster sorted by rating . def awesome_find(df,search_term): tmpdf = df.copy() for x in search_term.split(): tmpdf = tmpdf[tmpdf.name.str.upper().str.contains(x.strip().upper())] return df[df.clusters == tmpdf.sort_values(by=&#39;rating&#39;,ascending=False)[:1].clusters.values[0]][[&#39;anime_id&#39;,&#39;name&#39;,&#39;genre&#39;,&#39;type&#39;,&#39;episodes&#39;,&#39;members&#39;]] . . awesome_find(anime,&#39;Akame ga Kill&#39;) . anime_id name genre type episodes members . 1 5114 | Fullmetal Alchemist: Brotherhood | Action, Adventure, Drama, Fantasy, Magic, Mili... | TV | 64 | 793665 | . 24 164 | Mononoke Hime | Action, Adventure, Fantasy | Movie | 1 | 339556 | . 101 18115 | Magi: The Kingdom of Magic | Action, Adventure, Fantasy, Magic, Shounen | TV | 25 | 245026 | . 121 486 | Kino no Tabi: The Beautiful World | Action, Adventure, Slice of Life | TV | 13 | 102822 | . 133 2685 | Tsubasa: Tokyo Revelations | Action, Adventure, Drama, Fantasy, Romance, Sh... | OVA | 3 | 57963 | . ... ... | ... | ... | ... | ... | ... | . 11076 30736 | Shingeki no Bahamut: Virgin Soul | Action, Adventure, Demons, Fantasy, Magic, Sup... | TV | 24 | 20953 | . 11085 34086 | Tales of Zestiria the X (2017) | Action, Adventure, Fantasy | TV | 12 | 10848 | . 11644 1331 | Dragon Pink | Action, Adventure, Comedy, Fantasy, Hentai, Ma... | OVA | 3 | 3467 | . 11871 2185 | Words Worth Gaiden | Action, Adventure, Fantasy, Hentai | OVA | 2 | 2402 | . 11957 1763 | Midnight Panther | Action, Adventure, Fantasy, Hentai | OVA | 2 | 1144 | . 211 rows × 6 columns . from wordcloud import WordCloud . def create_wc(list_of_lists): words = list() for x in list_of_lists: for y in x.split(&#39;,&#39;): words.append(y.strip()) # Create and generate a word cloud image: wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=&quot;white&quot;).generate(&#39; &#39;.join(words)) return wordcloud . def plot_figures(figures, nrows = 1, ncols=1): fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=(20,20)) for ind,title in zip(range(len(figures)), figures): axeslist.ravel()[ind].imshow(figures[title], cmap=plt.jet()) axeslist.ravel()[ind].set_title(f&#39;Most Freqent words for the group {title+1}&#39;) axeslist.ravel()[ind].set_axis_off() plt.tight_layout() # optional . clouds = dict() for x in anime.clusters.unique(): clouds[x] = create_wc(anime[anime.clusters == x].genre.values.tolist()) . Let&#39;s see our clusters . plot_figures(clouds,9,5) . Cosine Similarity . from sklearn.metrics.pairwise import cosine_similarity . Our function computes the cosine similarity between the genre_vector of the given search term against all of the others . def search_cosine(df,search): tmpdf = df.copy() search_term = tmpdf[tmpdf.name == search][unique_genres].values tmpdf[&#39;similarity&#39;] = cosine_similarity(search_term,tmpdf[unique_genres].values)[0].tolist() return tmpdf.sort_values(by=&#39;similarity&#39;,ascending=False)[[&#39;anime_id&#39;,&#39;name&#39;,&#39;genre&#39;,&#39;type&#39;,&#39;episodes&#39;,&#39;members&#39;,&#39;similarity&#39;]] . The only do downgrade on this approach is that the search has to be an exact match, however this can be solved through an elasticsearch index . . src = search_cosine(anime,&#39;Sword Art Online&#39;) src . anime_id name genre type episodes members similarity . 2132 21881 | Sword Art Online II | Action, Adventure, Fantasy, Game, Romance | TV | 24 | 537892 | 1.000000 | . 804 11757 | Sword Art Online | Action, Adventure, Fantasy, Game, Romance | TV | 25 | 893100 | 1.000000 | . 3392 20021 | Sword Art Online: Extra Edition | Action, Adventure, Fantasy, Game, Romance | Special | 1 | 121722 | 1.000000 | . 10908 31765 | Sword Art Online Movie: Ordinal Scale | Action, Adventure, Fantasy, Game, Romance | Movie | 1 | 50944 | 1.000000 | . 5078 27891 | Sword Art Online II: Debriefing | Action, Adventure, Fantasy, Game | Special | 1 | 39092 | 0.894427 | . ... ... | ... | ... | ... | ... | ... | ... | . 7737 29944 | Kuchao | Dementia | Movie | 1 | 184 | 0.000000 | . 7736 21335 | Double Circle | Sci-Fi, Super Power | ONA | 6 | 4249 | 0.000000 | . 7735 32765 | Cherry Blossom (Music) | Music | Music | 1 | 113 | 0.000000 | . 2255 10232 | Tamayura: Hitotose | Comedy, Drama, Slice of Life | TV | 12 | 25760 | 0.000000 | . 12293 26081 | Yasuji no Pornorama: Yacchimae!! | Hentai | Movie | 1 | 142 | 0.000000 | . 12294 rows × 7 columns . Collaborative Filtering . . rating = pd.read_csv(zip_name.split(&#39;.&#39;)[0]+&#39;/rating.csv&#39;) rating . user_id anime_id rating . 0 1 | 20 | -1 | . 1 1 | 24 | -1 | . 2 1 | 79 | -1 | . 3 1 | 226 | -1 | . 4 1 | 241 | -1 | . ... ... | ... | ... | . 7813732 73515 | 16512 | 7 | . 7813733 73515 | 17187 | 9 | . 7813734 73515 | 22145 | 10 | . 7813735 73516 | 790 | 9 | . 7813736 73516 | 8074 | 9 | . 7813737 rows × 3 columns . mrating = rating.merge(rating.groupby(&#39;user_id&#39;).agg({&#39;rating&#39;:&#39;mean&#39;}).reset_index(),on=&#39;user_id&#39;) mrating = mrating.drop(mrating[mrating.rating_x &lt; mrating.rating_y].index) mrating . user_id anime_id rating_x rating_y . 47 1 | 8074 | 10 | -0.712418 | . 81 1 | 11617 | 10 | -0.712418 | . 83 1 | 11757 | 10 | -0.712418 | . 101 1 | 15451 | 10 | -0.712418 | . 153 2 | 11771 | 10 | 2.666667 | . ... ... | ... | ... | ... | . 7813730 73515 | 13659 | 8 | 7.719388 | . 7813733 73515 | 17187 | 9 | 7.719388 | . 7813734 73515 | 22145 | 10 | 7.719388 | . 7813735 73516 | 790 | 9 | 9.000000 | . 7813736 73516 | 8074 | 9 | 9.000000 | . 4262566 rows × 4 columns . PCA . from sklearn.decomposition import PCA . n_comps = 3 . comp = pd.DataFrame(PCA(n_components=n_comps).fit_transform(mrating[[&#39;user_id&#39;,&#39;anime_id&#39;,&#39;rating_x&#39;]]),columns=[f&#39;comp_{x}&#39;for x in range(1,4)]) comp[&#39;anime_id&#39;] = mrating[&#39;anime_id&#39;] comp = comp.merge(anime[[&#39;anime_id&#39;,&#39;name&#39;]],right_on=&#39;anime_id&#39;,left_on=&#39;anime_id&#39;,how=&#39;inner&#39;) comp . comp_1 comp_2 comp_3 anime_id name . 0 36979.379560 | 19271.801823 | -1.435758 | 8074.0 | Highschool of the Dead | . 1 36864.979321 | 12890.819470 | 8.604092 | 8074.0 | Highschool of the Dead | . 2 36501.255218 | -8729.123562 | -2.260925 | 8074.0 | Highschool of the Dead | . 3 36504.981554 | -7890.072244 | -2.266146 | 8074.0 | Highschool of the Dead | . 4 36575.509130 | -2723.534636 | -2.298376 | 8074.0 | Highschool of the Dead | . ... ... | ... | ... | ... | ... | . 2330380 -36744.486077 | -7441.741918 | 0.851111 | 17147.0 | Gakkatsu! 2nd Season | . 2330381 -36660.120947 | 648.960605 | -1.199318 | 25087.0 | Ie Naki Ko Remi Specials (2001) | . 2330382 -36641.406527 | 1792.807522 | -1.206458 | 29757.0 | Mori no e | . 2330383 -36896.998157 | -8205.349210 | -2.143892 | 7616.0 | Michi | . 2330384 -36959.758217 | -7762.263236 | -1.146544 | 28639.0 | Futon | . 2330385 rows × 5 columns . comp_grp = comp.groupby(&#39;name&#39;).mean().reset_index() comp_grp.name = comp_grp.name.str.replace(&#39;&amp;quot;&#39;,&quot;`&quot;) comp_grp . name comp_1 comp_2 comp_3 anime_id . 0 `0` | -11120.206150 | -5219.625973 | -0.704695 | 20707.0 | . 1 `Bungaku Shoujo` Kyou no Oyatsu: Hatsukoi | 1635.899945 | -518.535030 | 0.007314 | 7669.0 | . 2 `Bungaku Shoujo` Memoire | -1095.114790 | -231.043774 | 0.065327 | 8481.0 | . 3 `Bungaku Shoujo` Movie | -1846.898975 | 40.053205 | -0.135169 | 6408.0 | . 4 `Eiji` | -7032.249068 | 261.233554 | 1.754537 | 6076.0 | . ... ... | ... | ... | ... | ... | . 8488 xxxHOLiC | 1419.997992 | 93.118756 | -0.038304 | 861.0 | . 8489 xxxHOLiC Kei | 2172.334257 | 179.512393 | -0.058045 | 3091.0 | . 8490 xxxHOLiC Movie: Manatsu no Yoru no Yume | 3869.821060 | 436.500647 | 0.132941 | 793.0 | . 8491 xxxHOLiC Rou | -1700.534327 | 738.029495 | -0.003196 | 6864.0 | . 8492 xxxHOLiC Shunmuki | 1465.196974 | 298.304491 | 0.074259 | 4918.0 | . 8493 rows × 5 columns . import plotly.express as px . fig = px.scatter_3d(comp_grp, x=&#39;comp_1&#39;, y=&#39;comp_2&#39;, z=&#39;comp_3&#39;,color=&#39;name&#39;,size_max=7, opacity=0.7) fig.update_layout(margin=dict(l=0, r=0, b=0, t=0)) fig.write_html(&#39;pca.html&#39;) . ELK Stack Recommendation System . anime[&#39;genre_list&#39;] = anime.genre.str.split(&#39;,&#39;).apply(lambda x: [y.strip() for y in x]) . anime[:3] . anime_id name genre type episodes rating members genre_list . 0 32281 | Kimi no Na wa. | Drama, Romance, School, Supernatural | Movie | 1 | 9.37 | 200630 | [Drama, Romance, School, Supernatural] | . 1 5114 | Fullmetal Alchemist: Brotherhood | Action, Adventure, Drama, Fantasy, Magic, Mili... | TV | 64 | 9.26 | 793665 | [Action, Adventure, Drama, Fantasy, Magic, Mil... | . 2 28977 | Gintama° | Action, Comedy, Historical, Parody, Samurai, S... | TV | 51 | 9.25 | 114262 | [Action, Comedy, Historical, Parody, Samurai, ... | . rating[:3] . user_id anime_id rating . 0 1 | 20 | -1 | . 1 1 | 24 | -1 | . 2 1 | 79 | -1 | . try: import elasticsearch except: !pip install elasticsearch import elasticsearch . from elasticsearch.helpers import bulk . import requests as req . es_host = &#39;&#39; es_port = &#39;&#39; es_user = &#39;&#39; es_pass = &#39;&#39; . print(req.get(f&#39;{es_host}:{es_port}/_cat/indices&#39;,auth=(es_user, es_pass)).text) . green open apm-7.7.0-onboarding-2020.06.01 8AuzajpiSha3LKUIS1-0DA 1 1 1 0 14kb 7kb green open apm-7.7.0-error-000001 g0mvvPSNRBqvTKT3XkxEtA 1 1 0 0 416b 208b green open .apm-agent-configuration NuHbtEHjReCuzcGzkUkTvQ 1 1 0 0 416b 208b green open apm-7.7.0-transaction-000001 okf6EQuSRBWP_uPze2r0_g 1 1 0 0 416b 208b green open .kibana_1 9pXwv0ngRtaJYCuSSEmeRg 1 1 660 3 2.4mb 1.2mb green open kibana_sample_data_flights _JNZbj9KQnOL4cD8CIhfEw 1 1 13059 0 12.6mb 6.3mb green open apm-7.7.0-span-000001 N8UCNrmaRjiaX8NnjHnlzw 1 1 0 0 416b 208b green open .security-7 cyFn8axBRwOjJDq5JHUhhQ 1 1 47 2 179.1kb 89.5kb green open .apm-custom-link NM_Cj4S4RCCzFNKFyyrwCQ 1 1 0 0 416b 208b green open kibana_sample_data_ecommerce ioSMO_biTUa2cusEJqScnA 1 1 4675 0 9.7mb 4.8mb green open .kibana_task_manager_1 VHh1p97rQDCas32Xmi5LRA 1 1 5 1 83kb 20kb green open apm-7.7.0-metric-000001 d9U7HlGqTYiXRSC_gZkrww 1 1 0 0 416b 208b green open apm-7.7.0-profile-000001 TjVhTDXNQqaSW85kTRz3Ww 1 1 0 0 416b 208b . es = elasticsearch.Elasticsearch( [ f&#39;{es_host}:{es_port}&#39; ], http_auth=(es_user, es_pass), ) . print(es.indices.get(&#39;*&#39;)) . def index_template(index,id,doc): return { &#39;_index&#39;: index, &#39;_id&#39;: id, &#39;_source&#39;: doc } . anime[:2] . anime_id name genre type episodes rating members genre_list . 0 32281 | Kimi no Na wa. | Drama, Romance, School, Supernatural | Movie | 1 | 9.37 | 200630 | [Drama, Romance, School, Supernatural] | . 1 5114 | Fullmetal Alchemist: Brotherhood | Action, Adventure, Drama, Fantasy, Magic, Mili... | TV | 64 | 9.26 | 793665 | [Action, Adventure, Drama, Fantasy, Magic, Mil... | . anime.anime_id.nunique() / len(anime) . 1.0 . anime_actions = [ index_template(&#39;anime&#39;,x.anime_id,{ &#39;anime_id&#39;: x.anime_id , &#39;name&#39;: x.name, &#39;genres&#39;: x.genre_list, &#39;type&#39;: x.type if str(x.type) != &#39;nan&#39; else &#39;Unknown&#39;, &#39;episodes&#39;: int(x.episodes) if x.episodes not in [&#39;Unknown&#39;] else 0, &#39;rating&#39;: x.rating if str(x.rating) != &#39;nan&#39; else 0 }) for x in anime.itertuples() ] anime_actions[:2] . [{&#39;_id&#39;: 32281, &#39;_index&#39;: &#39;anime&#39;, &#39;_source&#39;: {&#39;anime_id&#39;: 32281, &#39;episodes&#39;: 1, &#39;genres&#39;: [&#39;Drama&#39;, &#39;Romance&#39;, &#39;School&#39;, &#39;Supernatural&#39;], &#39;name&#39;: &#39;Kimi no Na wa.&#39;, &#39;rating&#39;: 9.37, &#39;type&#39;: &#39;Movie&#39;}}, {&#39;_id&#39;: 5114, &#39;_index&#39;: &#39;anime&#39;, &#39;_source&#39;: {&#39;anime_id&#39;: 5114, &#39;episodes&#39;: 64, &#39;genres&#39;: [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Drama&#39;, &#39;Fantasy&#39;, &#39;Magic&#39;, &#39;Military&#39;, &#39;Shounen&#39;], &#39;name&#39;: &#39;Fullmetal Alchemist: Brotherhood&#39;, &#39;rating&#39;: 9.26, &#39;type&#39;: &#39;TV&#39;}}] . %%time bulk(es,anime_actions) . CPU times: user 306 ms, sys: 13.1 ms, total: 319 ms Wall time: 8.6 s . (12294, []) . def positive(x): return 0 if x &lt; 0 else x . frating = rating.groupby(&#39;user_id&#39;).agg({ &#39;anime_id&#39;: list, &#39;rating&#39;: list }).reset_index() frating[&#39;mixed&#39;] = frating[[&#39;anime_id&#39;,&#39;rating&#39;]].apply(lambda x: [ {&#39;anime_id&#39;: i, &#39;rating&#39;: positive(r) if str(r) != &#39;nan&#39; else 0} for i,r in zip(x.anime_id,x.rating)] ,axis=1) frating . user_id anime_id rating mixed . 0 1 | [20, 24, 79, 226, 241, 355, 356, 442, 487, 846... | [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -... | [{&#39;anime_id&#39;: 20, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 2... | . 1 2 | [11771, 12189, 16417] | [10, -1, -1] | [{&#39;anime_id&#39;: 11771, &#39;rating&#39;: 10}, {&#39;anime_id... | . 2 3 | [20, 154, 170, 199, 225, 341, 430, 527, 552, 8... | [8, 6, 9, 10, 9, 6, 7, 7, 7, 10, 7, 7, 7, 8, 6... | [{&#39;anime_id&#39;: 20, &#39;rating&#39;: 8}, {&#39;anime_id&#39;: 1... | . 3 4 | [6, 72, 121, 150, 166, 205, 226, 857, 1292, 14... | [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -... | [{&#39;anime_id&#39;: 6, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 72... | . 4 5 | [6, 15, 17, 18, 20, 22, 24, 30, 45, 47, 57, 63... | [8, 6, 6, 6, 6, 5, 1, 1, 7, 8, 7, 1, 6, 6, 6, ... | [{&#39;anime_id&#39;: 6, &#39;rating&#39;: 8}, {&#39;anime_id&#39;: 15... | . ... ... | ... | ... | ... | . 73510 73512 | [60, 98, 101, 120, 199, 226, 339, 355, 552, 65... | [10, 8, 10, 5, 10, 9, 10, -1, 7, 8, 8, 10, 8] | [{&#39;anime_id&#39;: 60, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: ... | . 73511 73513 | [1, 5, 71, 101, 164, 180, 181, 195, 196, 199, ... | [9, 8, 6, 8, 8, 5, 5, 7, 7, 9, 8, 8, 7, 10, 8,... | [{&#39;anime_id&#39;: 1, &#39;rating&#39;: 9}, {&#39;anime_id&#39;: 5,... | . 73512 73514 | [512] | [10] | [{&#39;anime_id&#39;: 512, &#39;rating&#39;: 10}] | . 73513 73515 | [1, 5, 6, 19, 27, 30, 33, 57, 67, 71, 72, 73, ... | [10, 10, 10, 9, 9, 8, 10, 8, 8, 9, 10, 9, 10, ... | [{&#39;anime_id&#39;: 1, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: 5... | . 73514 73516 | [790, 8074] | [9, 9] | [{&#39;anime_id&#39;: 790, &#39;rating&#39;: 9}, {&#39;anime_id&#39;: ... | . 73515 rows × 4 columns . user_actions = [ index_template(&#39;anime-users&#39;,x.user_id,{ &#39;user_id&#39;: x.user_id, &#39;rated&#39;: x.mixed }) for x in frating.itertuples() ] user_actions[:2] . [{&#39;_id&#39;: 1, &#39;_index&#39;: &#39;anime-users&#39;, &#39;_source&#39;: {&#39;rated&#39;: [{&#39;anime_id&#39;: 20, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 24, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 79, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 226, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 241, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 355, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 356, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 442, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 487, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 846, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 936, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 1546, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 1692, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 1836, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 2001, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 2025, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 2144, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 2787, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 2993, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 3455, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4063, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4214, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4224, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4581, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4744, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4898, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 4999, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 5034, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 5277, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 5667, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 5781, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 5958, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6163, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6205, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6324, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6500, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6547, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6682, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6707, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6747, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6773, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 6793, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 7088, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 7148, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 7593, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 7739, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 7858, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 8074, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: 8407, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 8424, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 8525, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 8630, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 8841, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9041, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9062, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9136, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9181, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9330, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9367, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9515, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9581, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9675, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9750, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9790, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 9919, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10067, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10073, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10076, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10079, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10080, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10209, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10578, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10604, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10719, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10790, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10793, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10794, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10805, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 10897, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 11161, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 11266, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 11617, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: 11737, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 11757, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: 11759, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 11771, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 12293, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 12549, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 12729, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 13357, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 13367, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 13411, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 13561, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 13663, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 13759, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 14749, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 14813, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 14833, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 14967, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 15117, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 15437, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 15451, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: 15583, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 15609, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 16011, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 16498, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 16706, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 17265, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 17729, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 18247, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 18277, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 18753, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 18897, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 19163, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 19221, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 19285, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 19429, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 19815, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 20045, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 20785, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 20787, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 21033, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 21881, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22147, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22199, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22319, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22535, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22547, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22663, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 22877, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 23233, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 23321, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 23847, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 24133, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 24455, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 24873, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 25099, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 25157, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 25159, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 25283, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 25397, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 26243, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 27775, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 27899, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 28121, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 28677, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 29093, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 29095, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 30015, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 30296, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 30544, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 31338, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 31845, &#39;rating&#39;: 0}], &#39;user_id&#39;: 1}}, {&#39;_id&#39;: 2, &#39;_index&#39;: &#39;anime-users&#39;, &#39;_source&#39;: {&#39;rated&#39;: [{&#39;anime_id&#39;: 11771, &#39;rating&#39;: 10}, {&#39;anime_id&#39;: 12189, &#39;rating&#39;: 0}, {&#39;anime_id&#39;: 16417, &#39;rating&#39;: 0}], &#39;user_id&#39;: 2}}] . %%time bulk(es,user_actions) . CPU times: user 8.01 s, sys: 166 ms, total: 8.18 s Wall time: 1min 41s . (73515, []) .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/Anime-Recommendation-System.html",
            "relUrl": "/2020/10/09/Anime-Recommendation-System.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Airbnb Pricing Optimization",
            "content": ". Airbnb claims to be part of the &quot;sharing economy&quot; and disrupting the hotel industry. However, data shows that the majority of Airbnb listings in most cities are entire homes, many of which are rented all year round - disrupting housing and communities. . . Aims and Objectives . EDA Locations | Renters | Hosts | . | Pricing Optimisation Property Price | Investment Portfolio | . | . Imports . import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns plt.style.use(&#39;dark_background&#39;) plt.style.use(&#39;seaborn&#39;) . Extract Data . 1 Year properties listing from San Francisco on Airbnb . . urls = [ &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-06-08/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-05-06/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-04-07/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-03-13/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-02-12/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-01-04/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2020-01-02/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2019-12-04/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2019-11-01/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2019-10-14/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2019-09-12/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2019-08-06/data/listings.csv&#39;, &#39;http://data.insideairbnb.com/united-states/ca/san-francisco/2019-07-08/data/listings.csv&#39; ] dfs = pd.concat([pd.read_csv(x) for x in urls],ignore_index=True) dfs[&#39;availability&#39;] = dfs[&#39;availability_365&#39;] / 365 # dfs[&#39;date&#39;] = pd.to_datetime(dfs[&#39;last_scraped&#39;]) # dfs[&#39;dayofweek&#39;] = dfs[&#39;date&#39;].dt.dayofweek # dfs[&#39;quarter&#39;] = dfs[&#39;date&#39;].dt.quarter # dfs[&#39;month&#39;] = dfs[&#39;date&#39;].dt.month # dfs[&#39;year&#39;] = dfs[&#39;date&#39;].dt.year # dfs[&#39;dayofyear&#39;] = dfs[&#39;date&#39;].dt.dayofyear # dfs[&#39;dayofmonth&#39;] = dfs[&#39;date&#39;].dt.day # dfs[&#39;weekofyear&#39;] = dfs[&#39;date&#39;].dt.weekofyear # time_columns = [&#39;dayofweek&#39;,&#39;quarter&#39;,&#39;month&#39;,&#39;year&#39;,&#39;dayofyear&#39;,&#39;dayofmonth&#39;,&#39;weekofyear&#39;] # clean price for x in [&#39;price&#39;,&#39;weekly_price&#39;,&#39;monthly_price&#39;,&#39;security_deposit&#39;,&#39;cleaning_fee&#39;]: dfs[x] = dfs[x].str.replace(&#39;$&#39;,&#39;&#39;).str.replace(&#39;,&#39;,&#39;&#39;).astype(float).fillna(0) dfs[:1] . /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (61,62) have mixed types.Specify dtype option on import or set low_memory=False. if self.run_code(code, result): . id listing_url scrape_id last_scraped name summary space description experiences_offered neighborhood_overview notes transit access interaction house_rules thumbnail_url medium_url picture_url xl_picture_url host_id host_url host_name host_since host_location host_about host_response_time host_response_rate host_acceptance_rate host_is_superhost host_thumbnail_url host_picture_url host_neighbourhood host_listings_count host_total_listings_count host_verifications host_has_profile_pic host_identity_verified street neighbourhood neighbourhood_cleansed ... minimum_nights maximum_nights minimum_minimum_nights maximum_minimum_nights minimum_maximum_nights maximum_maximum_nights minimum_nights_avg_ntm maximum_nights_avg_ntm calendar_updated has_availability availability_30 availability_60 availability_90 availability_365 calendar_last_scraped number_of_reviews number_of_reviews_ltm first_review last_review review_scores_rating review_scores_accuracy review_scores_cleanliness review_scores_checkin review_scores_communication review_scores_location review_scores_value requires_license license jurisdiction_names instant_bookable is_business_travel_ready cancellation_policy require_guest_profile_picture require_guest_phone_verification calculated_host_listings_count calculated_host_listings_count_entire_homes calculated_host_listings_count_private_rooms calculated_host_listings_count_shared_rooms reviews_per_month availability . 0 958 | https://www.airbnb.com/rooms/958 | 20200608144408 | 2020-06-08 | Bright, Modern Garden Unit - 1BR/1BTH | New update: the house next door is under const... | Newly remodeled, modern, and bright garden uni... | New update: the house next door is under const... | none | *Quiet cul de sac in friendly neighborhood *St... | Due to the fact that we have children and a do... | *Public Transportation is 1/2 block away. *Ce... | *Full access to patio and backyard (shared wit... | A family of 4 lives upstairs with their dog. N... | * No Pets - even visiting guests for a short t... | NaN | NaN | https://a0.muscache.com/im/pictures/b7c2a199-4... | NaN | 1169 | https://www.airbnb.com/users/show/1169 | Holly | 2008-07-31 | San Francisco, California, United States | We are a family with 2 boys born in 2009 and 2... | within an hour | 100% | 99% | t | https://a0.muscache.com/im/pictures/user/efdad... | https://a0.muscache.com/im/pictures/user/efdad... | Duboce Triangle | 1.0 | 1.0 | [&#39;email&#39;, &#39;phone&#39;, &#39;facebook&#39;, &#39;reviews&#39;, &#39;kba&#39;] | t | t | San Francisco, CA, United States | Duboce Triangle | Western Addition | ... | 1 | 1125 | 1 | 1 | 1125 | 1125 | 1.0 | 1125.0 | 5 weeks ago | t | 8 | 24 | 43 | 143 | 2020-06-08 | 241 | 47 | 2009-07-23 | 2020-03-28 | 97.0 | 10.0 | 10.0 | 10.0 | 10.0 | 10.0 | 10.0 | t | STR-0001256 | {&quot;SAN FRANCISCO&quot;} | f | f | moderate | f | f | 1 | 1 | 0 | 0 | 1.82 | 0.391781 | . 1 rows × 107 columns . Latest Month . df = dfs[dfs.last_scraped == &#39;2020-06-08&#39;].copy() # save a list of columns before filtering all_columns = df.columns.tolist() selected_columns = [&#39;id&#39;,&#39;last_scraped&#39;,&#39;listing_url&#39;,&#39;host_id&#39;,&#39;property_type&#39;,&#39;zipcode&#39;,&#39;accommodates&#39;, &#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;beds&#39;, &#39;price&#39;, &#39;weekly_price&#39;,&#39;monthly_price&#39;, &#39;security_deposit&#39;, &#39;cleaning_fee&#39;,&#39;number_of_reviews&#39;,&#39;review_scores_rating&#39;,&#39;cancellation_policy&#39;,&#39;neighbourhood&#39;,&#39;availability_365&#39;,&#39;latitude&#39;,&#39;longitude&#39;] # filter columns df = df[selected_columns] df[:1] . id last_scraped listing_url host_id property_type zipcode accommodates bathrooms bedrooms beds price weekly_price monthly_price security_deposit cleaning_fee number_of_reviews review_scores_rating cancellation_policy neighbourhood availability_365 latitude longitude . 0 958 | 2020-06-08 | https://www.airbnb.com/rooms/958 | 1169 | Apartment | 94117 | 3 | 1.0 | 1.0 | 2.0 | 170.0 | 1120.0 | 4200.0 | 100.0 | 100.0 | 241 | 97.0 | moderate | Duboce Triangle | 143 | 37.76931 | -122.43386 | . EDA . fig,ax = plt.subplots(1,2,figsize=(25,10)) sns.scatterplot(data=df,x=&#39;latitude&#39;,y=&#39;longitude&#39;,hue=&#39;property_type&#39;,palette=sns.color_palette(&quot;Paired&quot;, df.property_type.nunique()),ax=ax.ravel()[0]) sns.scatterplot(data=df,x=&#39;latitude&#39;,y=&#39;longitude&#39;,hue=&#39;neighbourhood&#39;,palette=sns.color_palette(&quot;Paired&quot;, df.neighbourhood.nunique()),ax=ax.ravel()[1]) ax.ravel()[0].legend(bbox_to_anchor=(1.0,1.0)) ax.ravel()[1].legend(bbox_to_anchor=(1.0,1.0)) plt.tight_layout() . Diversity and Dominance . def shannon(series): &quot;&quot;&quot; series: pd.Series &quot;&quot;&quot; N = series.sum() s = np.array([ (n/N) * np.log(n/N) for n in series if n != 0]) s = s[~np.isnan(s)] return np.abs(s.sum()) def simpson(series): &quot;&quot;&quot; series: pd.Series &quot;&quot;&quot; N = series.sum() return np.array([ ( n * (n-1) ) / ( N * (N-1) ) for n in series if n != 0]).sum() . Diversity and Dominance of property types by neighbourhood . plt.figure(figsize=(25,10)) ngh = pd.crosstab(df.property_type,df.neighbourhood) ngh_diversity = ngh.apply(shannon) ngh_dominance = ngh.apply(simpson) ngh_df = pd.DataFrame({ &#39;diversity&#39;: ngh_diversity, &#39;dominance&#39;: ngh_dominance, &#39;total_number_of_properties&#39;: ngh.sum(), &#39;unique_property_types&#39;: df.groupby(&#39;neighbourhood&#39;).property_type.nunique() }).reset_index() plt.scatter(ngh_diversity,ngh_dominance,s=ngh.sum(),label=&#39;Property Type&#39;,cmap=&#39;Oranges&#39;) sns.regplot(ngh_diversity,ngh_dominance,scatter=False) for i,x in enumerate(ngh.columns): plt.annotate(x,(ngh_diversity[i],ngh_dominance[i]),ha=&#39;center&#39;,va=&#39;center&#39;) plt.xlabel(&#39;Diversity&#39;) plt.ylabel(&#39;Dominance&#39;); . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars . ngh_df[ngh_df.neighbourhood.isin([&#39;Downtown&#39;,&#39;Mission District&#39;,&#39;Mission Bay&#39;])] . neighbourhood diversity dominance total_number_of_properties unique_property_types . 12 Downtown | 1.961264 | 0.170480 | 382 | 12 | . 27 Mission Bay | 0.208982 | 0.914010 | 46 | 3 | . 28 Mission District | 1.344584 | 0.354231 | 705 | 11 | . Diversity and Dominance of property types by for each neighbourhood . Downtown - the most diverse, balanced property distribution in terms of price | Mission District - quite diverse, large number of properties | Mission Bay - low number of properties, and focused on few types | . Property Distributions . fig,ax = plt.subplots(1,2,figsize=(35,10)) property_distribution = df.pivot_table(index=&#39;neighbourhood&#39;,columns=&#39;property_type&#39;,values=&#39;id&#39;,aggfunc=&#39;count&#39;,fill_value=0).T property_distribution = property_distribution.apply(lambda x: x / x.sum(),axis=1) sns.heatmap(property_distribution,ax=ax.ravel()[0],cmap=&#39;Oranges&#39;) property_price = df.pivot_table(index=&#39;neighbourhood&#39;,columns=&#39;property_type&#39;,values=&#39;price&#39;,aggfunc=&#39;mean&#39;,fill_value=0) property_price = property_price.apply(lambda x: x / x.sum(),axis=1) sns.heatmap(property_price.T,ax=ax.ravel()[1],cmap=&#39;Oranges&#39;) ax.ravel()[0].set_xlabel(&#39;&#39;) # ax.ravel()[0].set_xticklabels([&#39;&#39;]) ax.ravel()[0].set_title(&#39;Property Distribution by neighbourhood&#39;) ax.ravel()[1].set_title(&#39;Price Distribution by property_type and neighbourhood&#39;) plt.tight_layout() . Outliers: . Bayview: Castle | Hut | . | Fisherman&#39;s Wharf: Boat | . | Nob Hill: In-law | . | Outer Susnet Camper | . | . Average Yearly Availability by Property Type . (df.groupby(&#39;property_type&#39;).availability_365.mean() / 365).plot.bar(rot=30,figsize=(30,7)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8a80832940&gt; . Average Yearly Availability by Neighbourhood . (df.groupby(&#39;neighbourhood&#39;).availability_365.mean() / 365).plot.bar(rot=45,figsize=(30,7)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8a9c7bfda0&gt; . Number of hosts by range of properties . hosts = df.groupby(&#39;host_id&#39;)[&#39;id&#39;].nunique().to_frame().reset_index().rename(columns={&#39;id&#39;:&#39;number_of_properties&#39;}) bins = [0,1,3,5,10,20,30,50,100,150,200,230] f_bins = list() for x in range(1,len(bins)-1): if x == 1: f_bins.append(str(bins[1])) else: f_bins.append(f&#39;{bins[x-1]}-{bins[x]}&#39;) if x == len(bins)-2: f_bins.append(str(bins[-2]) + &#39;+&#39;) hosts[&#39;group&#39;] = pd.cut(hosts.number_of_properties,bins=bins,labels=f_bins) fig,ax =plt.subplots(1,2,figsize=(20,6)) hosts.group.value_counts()[:3].plot.bar(rot=0,ax=ax.ravel()[0]) hosts.group.value_counts()[3:].plot.bar(rot=0,ax=ax.ravel()[1]) ax.ravel()[0].set_title(&#39;Number of owners by properties cut groups&#39;) plt.tight_layout() . Let&#39;s get into pricing . Property Pricing . Using the mean values filtered by basic property specs . filtering_columns = [&#39;neighbourhood&#39;,&#39;property_type&#39;,&#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;beds&#39;] mean_prices_group = df.groupby(filtering_columns)[&#39;price&#39;].mean().reset_index() . Now let&#39;s pick a random property and see the differences . smpl = df.sample(1).copy() print(&#39;Sample specs&#39;) print(smpl[filtering_columns + [&#39;price&#39;]].T) print(&#39; nMean Price by same property specs&#39;) print(smpl[filtering_columns].merge(mean_prices_group,on=filtering_columns,how=&#39;left&#39;).T) . Sample specs 583 neighbourhood Lower Haight property_type Apartment bathrooms 2 bedrooms 2 beds 2 price 216 Mean Price by same property specs 0 neighbourhood Lower Haight property_type Apartment bathrooms 2 bedrooms 2 beds 2 price 357.5 . XGBoost . x_cols = [&#39;property_type&#39;, &#39;accommodates&#39;, &#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;beds&#39;, &#39;weekly_price&#39;, &#39;monthly_price&#39;, &#39;security_deposit&#39;, &#39;cleaning_fee&#39;, &#39;number_of_reviews&#39;, &#39;review_scores_rating&#39;, &#39;cancellation_policy&#39;, &#39;neighbourhood&#39;, &#39;availability_365&#39;] # copy the main dataframe xdf = df[x_cols + [&#39;price&#39;]].copy() # fill missing values xdf.loc[(xdf.neighbourhood.isnull()) &amp; (xdf.property_type == &#39;Boat&#39;),&#39;neighbourhood&#39;] = &#39;Here is a boat&#39; xdf = xdf.fillna(0) #label encoding label_encoding = dict() to_label = [&#39;property_type&#39;,&#39;cancellation_policy&#39;,&#39;neighbourhood&#39;] for x in to_label: l = xdf[x].unique() label_encoding[x] = dict(zip( l, list(range(len(l))) )) xdf[x] = xdf[x].map(label_encoding[x].get) xdf[:3] . property_type accommodates bathrooms bedrooms beds weekly_price monthly_price security_deposit cleaning_fee number_of_reviews review_scores_rating cancellation_policy neighbourhood availability_365 price . 0 0 | 3 | 1.0 | 1.0 | 2.0 | 1120.0 | 4200.0 | 100.0 | 100.0 | 241 | 97.0 | 0 | 0 | 143 | 170.0 | . 1 0 | 5 | 1.0 | 2.0 | 3.0 | 1600.0 | 5500.0 | 0.0 | 100.0 | 111 | 98.0 | 1 | 1 | 0 | 235.0 | . 2 0 | 2 | 4.0 | 1.0 | 1.0 | 485.0 | 1685.0 | 200.0 | 50.0 | 19 | 84.0 | 1 | 2 | 365 | 65.0 | . import xgboost as xgb . xgr = xgb.XGBRegressor(objective=&#39;reg:gamma&#39;).fit(xdf[x_cols],xdf[&#39;price&#39;]) . xgb.plot_importance(xgr) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8aa396ef28&gt; . smpl = xdf.sample(1).copy() print(&#39;Sample specs&#39;) print(smpl[x_cols + [&#39;price&#39;]].T) print(f&#39; nXGBoost Gamma Regressor Prediced Price {xgr.predict(smpl[x_cols])[0]}&#39;) . Sample specs 7587 property_type 0.0 accommodates 2.0 bathrooms 1.0 bedrooms 1.0 beds 1.0 weekly_price 0.0 monthly_price 0.0 security_deposit 200.0 cleaning_fee 100.0 number_of_reviews 0.0 review_scores_rating 0.0 cancellation_policy 2.0 neighbourhood 43.0 availability_365 83.0 price 86.0 XGBoost Gamma Regressor Prediced Price 150.90704345703125 . def predict_price(df,xgr): c_df = df.copy() for j in to_label: c_df[j] = c_df[j].map(label_encoding[j].get) return xgr.predict(c_df) . Portfolio Simulations . Estimate number of nights per year for each listing . Source: tule2236/Airbnb-Dynamic-Pricing-Optimization . As found in the Overview of the Airbnb Community in San Francisco published by Airbnb, the average length of stay per guest is 4.2 nights. We assumed each listing has 4.2 days as an average lengths of stay per booking. Since we were not able to find a clear number for the ratio of guests making a booking who leave a review for Airbnb, we assumed the review rate to be equal to 0.5, which will be used as a constant throughout the estimation. To prevent artificially high results, we also assumed the maximum occupancy rate cannot exceed 0.95, meaning even the busiest of listings will have several nights a month in which they go unrented. With these assumptions and constants, we generated the formulation of estimated occupancy rate shown below: . . def estimate_nights_per_year(review_per_month,yearly_availability): av_nights = 4.2 review_rate = 0.5 max_occupancy_rate = 0.95 bookings_per_month = review_per_month / review_rate est_occupancy = min( (( bookings_per_month * av_nights ) / 30),max_occupancy_rate) return est_occupancy * yearly_availability . df[&#39;estimated_nights_per_year&#39;] = df.apply(lambda x : estimate_nights_per_year(x.number_of_reviews,x.availability_365),axis=1) . Average estimated nights per year by neighbourhood and property type . plt.figure(figsize=(40,15)) sns.heatmap(df.pivot_table(index=&#39;neighbourhood&#39;,columns=&#39;property_type&#39;,values=&#39;estimated_nights_per_year&#39;,aggfunc=&#39;mean&#39;,fill_value=0)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8aa3b19b38&gt; . Monte Carlo . Since we don&#39;t have property prices, we assume that after 20 years of activity, the property paid the price for itself. . . 1 Month data . main_columns = [&#39;neighbourhood&#39;,&#39;property_type&#39;, &#39;accommodates&#39;, &#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;beds&#39;] copy_df = df.copy() # estimate number of nights per year copy_df[&#39;estimated_nights_per_year&#39;] = copy_df.apply(lambda x : estimate_nights_per_year(x.number_of_reviews,x.availability_365),axis=1) # get optimized prices copy_df[&#39;optimized_price&#39;] = predict_price(copy_df[x_cols],xgr) # groupb by main columns mdf = copy_df.groupby(main_columns).agg({ &#39;availability_365&#39;: &#39;mean&#39;, &#39;price&#39;: [&#39;mean&#39;,&#39;std&#39;], &#39;number_of_reviews&#39;:&#39;mean&#39;, &#39;estimated_nights_per_year&#39;:&#39;mean&#39;, &#39;optimized_price&#39;: [&#39;mean&#39;,&#39;std&#39;] }).reset_index().dropna() # join multi-indexes together mdf.columns = [ &#39;_&#39;.join(x) if x[1] != &#39;&#39; else x[0] for x in mdf.columns ] # calculate return per year mdf[&#39;estimated_return_per_year&#39;] = mdf[&#39;price_mean&#39;] * mdf[&#39;estimated_nights_per_year_mean&#39;] # calculate optimized return per year mdf[&#39;estimated_optimized_return_per_year&#39;] = mdf[&#39;optimized_price_mean&#39;] * mdf[&#39;estimated_nights_per_year_mean&#39;] # generate property price mdf[&#39;estimated_property_price&#39;] = mdf[&#39;price_mean&#39;] * (365 * 20) # format property price mdf[&#39;estimated_property_price_M&#39;] = (mdf[&#39;estimated_property_price&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) mdf[:3] . neighbourhood property_type accommodates bathrooms bedrooms beds availability_365_mean price_mean price_std number_of_reviews_mean estimated_nights_per_year_mean optimized_price_mean optimized_price_std estimated_return_per_year estimated_optimized_return_per_year estimated_property_price estimated_property_price_M . 2 Alamo Square | Apartment | 1 | 3.0 | 1.0 | 1.0 | 364.25 | 48.0 | 6.000000 | 3.25 | 86.6875 | 57.929253 | 4.668282 | 4161.00 | 5021.742087 | 350400.0 | $ 0.3504 | . 3 Alamo Square | Apartment | 2 | 1.0 | 1.0 | 1.0 | 317.50 | 152.0 | 32.526912 | 26.50 | 128.2500 | 154.892731 | 17.912474 | 19494.00 | 19864.992714 | 1109600.0 | $ 1.1096 | . 6 Alamo Square | Apartment | 4 | 1.0 | 1.0 | 2.0 | 159.50 | 150.0 | 98.994949 | 9.50 | 151.5250 | 166.732971 | 5.983315 | 22728.75 | 25264.213460 | 1095000.0 | $ 1.0950 | . Portfolios . investment = 1e7 mc_portfolios = list() pmax=100 for p in range(pmax): print(f&#39;{p}/{pmax}&#39;,end=&#39; r&#39;) #while we have money, pick up random properties for portfolio local_df = mdf.copy() current_money = investment picked_properties = list() stop_flag = False while stop_flag != True: # pick up random property ch = local_df.sample(1).copy() # if we have money to buy it and we haven&#39;t already bought it then let&#39;s do it if ch.estimated_property_price.values[0] &lt; current_money and ch.index.values[0] not in picked_properties: # add property index to current portfolio list of properties picked_properties.append(ch.index.values[0]) # pay the property price current_money -= ch.estimated_property_price.values[0] # slice the current dataframe to get just affordable properties local_df = local_df[local_df.estimated_property_price &lt; current_money] # if we dont&#39;t have enough money to buy the event the cheapest property or we run out of properties then it&#39;s the time to stop if current_money &lt; local_df.estimated_property_price.min() or len(local_df) &lt; 1: stop_flag = True tmp_portfolio = mdf[mdf.index.isin(picked_properties)].copy() tmp_portfolio[&#39;mdf_id&#39;] = picked_properties tmp_portfolio[&#39;p&#39;] = p mc_portfolios.append(tmp_portfolio) mc_portfolios = pd.concat(mc_portfolios).reset_index(drop=True) mc_portfolios[:2] . . neighbourhood property_type accommodates bathrooms bedrooms beds availability_365_mean price_mean price_std number_of_reviews_mean estimated_nights_per_year_mean optimized_price_mean optimized_price_std estimated_return_per_year estimated_optimized_return_per_year estimated_property_price estimated_property_price_M mdf_id p . 0 Bernal Heights | Condominium | 4 | 1.5 | 2.0 | 2.0 | 17.5 | 244.50 | 106.773124 | 50.50 | 16.625 | 245.411896 | 0.479673 | 4064.8125 | 4079.972767 | 1784850.0 | $ 1.7849 | 2726 | 0 | . 1 Chinatown | Apartment | 1 | 1.5 | 1.0 | 1.0 | 349.5 | 65.25 | 13.817260 | 2.75 | 240.510 | 92.636261 | 2.107101 | 15693.2775 | 22279.947130 | 476325.0 | $ 0.4763 | 187 | 0 | . portfolio_results = mc_portfolios.groupby(&#39;p&#39;).agg({ &#39;estimated_return_per_year&#39;: &#39;sum&#39;, &#39;estimated_property_price&#39;: &#39;sum&#39;, &#39;estimated_optimized_return_per_year&#39;: &#39;sum&#39;, &#39;mdf_id&#39;: &#39;count&#39;, }).reset_index() portfolio_results[&#39;estimated_property_price_M&#39;] = (portfolio_results[&#39;estimated_property_price&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) portfolio_results[&#39;estimated_return_per_year_M&#39;] = (portfolio_results[&#39;estimated_return_per_year&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) portfolio_results[&#39;estimated_optimized_return_per_year_M&#39;] = (portfolio_results[&#39;estimated_optimized_return_per_year&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) portfolio_results[&#39;time_to_return&#39;] = portfolio_results[&#39;estimated_property_price&#39;] / portfolio_results[&#39;estimated_return_per_year&#39;] portfolio_results[&#39;time_to_return_optimized&#39;] = portfolio_results[&#39;estimated_property_price&#39;] / portfolio_results[&#39;estimated_optimized_return_per_year&#39;] portfolio_results[&#39;profit&#39;] = (portfolio_results[&#39;time_to_return&#39;] * portfolio_results[&#39;estimated_optimized_return_per_year&#39;]) - (portfolio_results[&#39;time_to_return&#39;] * portfolio_results[&#39;estimated_return_per_year&#39;]) portfolio_results[&#39;profit_of_investment&#39;] = portfolio_results[&#39;profit&#39;] / portfolio_results[&#39;estimated_property_price&#39;] portfolio_results[&#39;profit_M&#39;] = (portfolio_results[&#39;profit&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) . Portfolios with minimal return time . portfolio_results.sort_values(by=&#39;time_to_return_optimized&#39;,ascending=True)[:3] . p estimated_return_per_year estimated_property_price estimated_optimized_return_per_year mdf_id estimated_property_price_M estimated_return_per_year_M estimated_optimized_return_per_year_M time_to_return time_to_return_optimized profit profit_of_investment profit_M . 39 39 | 222350.258407 | 9.966736e+06 | 339534.248750 | 13 | $ 9.9667 | $ 0.2224 | $ 0.3395 | 44.824486 | 29.354140 | 5.252712e+06 | 0.527024 | $ 5.2527 | . 66 66 | 219921.792321 | 9.931917e+06 | 309218.686489 | 7 | $ 9.9319 | $ 0.2199 | $ 0.3092 | 45.161129 | 32.119393 | 4.032749e+06 | 0.406039 | $ 4.0327 | . 35 35 | 189177.550040 | 9.990919e+06 | 296431.938938 | 8 | $ 9.9909 | $ 0.1892 | $ 0.2964 | 52.812393 | 33.703922 | 5.664361e+06 | 0.566951 | $ 5.6644 | . Time is money . Profit vs Time to return . sns.regplot(portfolio_results[&#39;time_to_return&#39;],portfolio_results[&#39;profit_of_investment&#39;]) for x in portfolio_results.itertuples(): plt.annotate(x.p,(x.time_to_return, x.profit_of_investment),ha=&#39;center&#39;,va=&#39;center&#39;) plt.xlabel(&#39;Time to return&#39;) plt.ylabel(&#39;Profit from investment %&#39;) plt.tight_layout() . Portfolios by time and profit . fig,ax = plt.subplots(1,2,figsize=(30,10)) sns.regplot(portfolio_results[&#39;time_to_return&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[0]) sns.regplot(portfolio_results[&#39;time_to_return_optimized&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[1]) for x in portfolio_results.itertuples(): ax[0].annotate(x.p,(x.time_to_return, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[1].annotate(x.p,(x.time_to_return_optimized, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[0].set_xlabel(&#39;Time to return&#39;) ax[0].set_ylabel(&#39;Total Investment&#39;) ax[0].set_title(&#39;Original Prices&#39;) ax[1].set_xlabel(&#39;Time to return&#39;) ax[1].set_ylabel(&#39;Total Investment&#39;) ax[1].set_title(&#39;Optimized Prices&#39;) plt.tight_layout() . Portfolios by yearly return and total investment . fig,ax = plt.subplots(1,2,figsize=(30,10)) sns.regplot(portfolio_results[&#39;estimated_return_per_year&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[0]) sns.regplot(portfolio_results[&#39;estimated_optimized_return_per_year&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[1]) for x in portfolio_results.itertuples(): ax[0].annotate(x.p,(x.estimated_return_per_year, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[1].annotate(x.p,(x.estimated_optimized_return_per_year, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[0].set_xlabel(&#39;Return per Year&#39;) ax[0].set_ylabel(&#39;Total Investment&#39;) ax[0].set_title(&#39;Original Prices&#39;) ax[1].set_xlabel(&#39;Return per Year&#39;) ax[1].set_ylabel(&#39;Total Investment&#39;) ax[1].set_title(&#39;Optimized Prices&#39;) plt.tight_layout() . 1 year data . grp = dfs.groupby(&#39;id&#39;).agg({ &#39;host_id&#39;:&#39;first&#39;, &#39;neighbourhood&#39;:&#39;first&#39;, &#39;property_type&#39;:&#39;first&#39;, &#39;accommodates&#39;: &#39;mean&#39;, &#39;bathrooms&#39;:&#39;mean&#39;, &#39;bedrooms&#39;: &#39;mean&#39;, &#39;beds&#39;:&#39;mean&#39;, &#39;price&#39;:&#39;mean&#39;, &#39;number_of_reviews&#39;:&#39;mean&#39;, &#39;availability_365&#39;:&#39;mean&#39;, &#39;review_scores_rating&#39;: &#39;mean&#39;, &#39;monthly_price&#39;: &#39;mean&#39;, &#39;security_deposit&#39;: &#39;mean&#39;, &#39;cleaning_fee&#39;: &#39;mean&#39;, &#39;cancellation_policy&#39;: &#39;first&#39;, &#39;weekly_price&#39;: &#39;mean&#39; }).reset_index() grp[:3] . id host_id neighbourhood property_type accommodates bathrooms bedrooms beds price number_of_reviews availability_365 review_scores_rating monthly_price security_deposit cleaning_fee cancellation_policy weekly_price . 0 958 | 1169 | Duboce Triangle | Apartment | 3.0 | 1.0 | 1.0 | 2.0 | 170.0 | 224.769231 | 90.153846 | 97.000 | 4200.0 | 100.0 | 100.0 | moderate | 1120.0 | . 1 3850 | 4921 | Inner Sunset | House | 2.0 | 1.0 | 1.0 | 1.0 | 99.0 | 159.250000 | 66.250000 | 94.375 | 0.0 | 0.0 | 10.0 | strict_14_with_grace_period | 0.0 | . 2 5858 | 8904 | Bernal Heights | Apartment | 5.0 | 1.0 | 2.0 | 3.0 | 235.0 | 111.000000 | 0.076923 | 98.000 | 5500.0 | 0.0 | 100.0 | strict_14_with_grace_period | 1600.0 | . location group . main_columns = [&#39;neighbourhood&#39;,&#39;property_type&#39;, &#39;accommodates&#39;, &#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;beds&#39;] copy_df = grp.copy() copy_df[&#39;estimated_nights_per_year&#39;] = copy_df.apply(lambda x : estimate_nights_per_year(x.number_of_reviews,x.availability_365),axis=1) copy_df[&#39;optimized_price&#39;] = predict_price(copy_df[x_cols],xgr) mdf = copy_df.groupby(main_columns).agg({ &#39;availability_365&#39;: &#39;mean&#39;, &#39;price&#39;: [&#39;mean&#39;,&#39;std&#39;], &#39;number_of_reviews&#39;:&#39;mean&#39;, &#39;estimated_nights_per_year&#39;:[&#39;mean&#39;,&#39;std&#39;], &#39;optimized_price&#39;: [&#39;mean&#39;,&#39;std&#39;] }).reset_index().dropna() mdf.columns = [ &#39;_&#39;.join(x) if x[1] != &#39;&#39; else x[0] for x in mdf.columns ] returns = list() optimized_returns = list() for x in mdf.itertuples(): # generate random number of nights using the mean and std of estimated nights per year random_nights = np.abs(np.random.normal(loc=x.estimated_nights_per_year_mean,scale=x.estimated_nights_per_year_std)) # generate random prices with the size of the random nights random_prices = np.random.normal(loc=x.price_mean,scale=x.price_std,size=int(random_nights)) # add the yearly return to our list returns.append(random_prices.sum()) # for the same number of random nights, calculate the optimized yearly return random_optim_prices = np.random.normal(loc=x.optimized_price_mean,scale=x.optimized_price_std,size=int(random_nights)) optimized_returns.append(random_optim_prices.sum()) mdf[&#39;estimated_return_per_year&#39;] = returns mdf[&#39;estimated_optimized_return_per_year&#39;] = optimized_returns mdf[&#39;estimated_property_price&#39;] = mdf[&#39;price_mean&#39;] * (365 * 20) mdf[&#39;estimated_property_price_M&#39;] = (mdf[&#39;estimated_property_price&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) mdf[:3] . neighbourhood property_type accommodates bathrooms bedrooms beds availability_365_mean price_mean price_std number_of_reviews_mean estimated_nights_per_year_mean estimated_nights_per_year_std optimized_price_mean optimized_price_std estimated_return_per_year estimated_optimized_return_per_year estimated_property_price estimated_property_price_M . 2 Alamo Square | Apartment | 1.0 | 3.0 | 1.0 | 1.0 | 348.685897 | 47.000000 | 4.898979 | 2.191026 | 75.109436 | 127.222971 | 60.804733 | 4.177582 | 11603.875568 | 14897.434087 | 3.431000e+05 | $ 0.3431 | . 3 Alamo Square | Apartment | 2.0 | 1.0 | 1.0 | 1.0 | 199.173789 | 135.333333 | 36.909800 | 13.307692 | 72.541026 | 125.644742 | 156.995514 | 13.179280 | 21458.311016 | 25192.637399 | 9.879333e+05 | $ 0.9879 | . 9 Alamo Square | Apartment | 4.0 | 1.0 | 1.0 | 2.0 | 159.062500 | 156.250000 | 90.156115 | 9.375000 | 151.109375 | 213.700928 | 166.732971 | 5.983315 | 2647.978633 | 2840.720439 | 1.140625e+06 | $ 1.1406 | . investment = 1e7 mc_portfolios = list() pmax=100 for p in range(pmax): print(f&#39;{p}/{pmax}&#39;,end=&#39; r&#39;) #while we have money, pick up random properties for portfolio local_df = mdf.copy() current_money = investment picked_properties = list() stop_flag = False while stop_flag != True: # pick up random property ch = local_df.sample(1).copy() # if we have money to buy it and we haven&#39;t already bought it then let&#39;s do it if ch.estimated_property_price.values[0] &lt; current_money and ch.index.values[0] not in picked_properties: # add property index to current portfolio list of properties picked_properties.append(ch.index.values[0]) # pay the property price current_money -= ch.estimated_property_price.values[0] # slice the current dataframe to get just affordable properties local_df = local_df[local_df.estimated_property_price &lt; current_money] # if we dont&#39;t have enough money to buy even the cheapest property or we run out of properties then it&#39;s the time to stop if current_money &lt; local_df.estimated_property_price.min() or len(local_df) &lt; 1: stop_flag = True tmp_portfolio = mdf[mdf.index.isin(picked_properties)].copy() tmp_portfolio[&#39;mdf_id&#39;] = picked_properties tmp_portfolio[&#39;p&#39;] = p mc_portfolios.append(tmp_portfolio) mc_portfolios = pd.concat(mc_portfolios).reset_index(drop=True) mc_portfolios[:2] . . neighbourhood property_type accommodates bathrooms bedrooms beds availability_365_mean price_mean price_std number_of_reviews_mean estimated_nights_per_year_mean estimated_nights_per_year_std optimized_price_mean optimized_price_std estimated_return_per_year estimated_optimized_return_per_year estimated_property_price estimated_property_price_M mdf_id p . 0 Bayview | House | 2.0 | 2.0 | 1.0 | 1.0 | 235.420994 | 84.28125 | 47.190994 | 3.540064 | 161.430898 | 111.271046 | 139.239120 | 19.414377 | 20207.811891 | 35149.639528 | 615253.125 | $ 0.6153 | 2932 | 0 | . 1 Bayview | House | 8.0 | 3.0 | 4.0 | 4.0 | 25.307692 | 262.50000 | 53.033009 | 14.300000 | 24.042308 | 32.657455 | 549.040161 | 8.945264 | 10060.290051 | 21961.733177 | 1916250.000 | $ 1.9163 | 213 | 0 | . portfolio_results = mc_portfolios.groupby(&#39;p&#39;).agg({ &#39;estimated_return_per_year&#39;: &#39;sum&#39;, &#39;estimated_property_price&#39;: &#39;sum&#39;, &#39;estimated_optimized_return_per_year&#39;: &#39;sum&#39;, &#39;mdf_id&#39;: &#39;count&#39;, }).reset_index() portfolio_results[&#39;estimated_property_price_M&#39;] = (portfolio_results[&#39;estimated_property_price&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) portfolio_results[&#39;estimated_return_per_year_M&#39;] = (portfolio_results[&#39;estimated_return_per_year&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) portfolio_results[&#39;estimated_optimized_return_per_year_M&#39;] = (portfolio_results[&#39;estimated_optimized_return_per_year&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) portfolio_results[&#39;time_to_return&#39;] = portfolio_results[&#39;estimated_property_price&#39;] / portfolio_results[&#39;estimated_return_per_year&#39;] portfolio_results[&#39;time_to_return_optimized&#39;] = portfolio_results[&#39;estimated_property_price&#39;] / portfolio_results[&#39;estimated_optimized_return_per_year&#39;] portfolio_results[&#39;profit&#39;] = (portfolio_results[&#39;time_to_return&#39;] * portfolio_results[&#39;estimated_optimized_return_per_year&#39;]) - (portfolio_results[&#39;time_to_return&#39;] * portfolio_results[&#39;estimated_return_per_year&#39;]) portfolio_results[&#39;profit_of_investment&#39;] = portfolio_results[&#39;profit&#39;] / portfolio_results[&#39;estimated_property_price&#39;] portfolio_results[&#39;profit_M&#39;] = (portfolio_results[&#39;profit&#39;] / 1e6).map(lambda x: f&#39;$ {x:.4f}&#39;) . Portfolios with minimal return time . portfolio_results.sort_values(by=&#39;time_to_return_optimized&#39;,ascending=True)[:3] . p estimated_return_per_year estimated_property_price estimated_optimized_return_per_year mdf_id estimated_property_price_M estimated_return_per_year_M estimated_optimized_return_per_year_M time_to_return time_to_return_optimized profit profit_of_investment profit_M . 65 65 | 413167.932968 | 9.904386e+06 | 411843.015193 | 7 | $ 9.9044 | $ 0.4132 | $ 0.4118 | 23.971816 | 24.048934 | -3.176069e+04 | -0.003207 | $ -0.0318 | . 20 20 | 199503.023973 | 9.993643e+06 | 348561.432544 | 10 | $ 9.9936 | $ 0.1995 | $ 0.3486 | 50.092689 | 28.671109 | 7.466737e+06 | 0.747149 | $ 7.4667 | . 82 82 | 262761.351114 | 9.936821e+06 | 326939.359423 | 10 | $ 9.9368 | $ 0.2628 | $ 0.3269 | 37.816904 | 30.393468 | 2.427014e+06 | 0.244244 | $ 2.4270 | . sns.regplot(portfolio_results[&#39;time_to_return&#39;],portfolio_results[&#39;profit_of_investment&#39;]) for x in portfolio_results.itertuples(): plt.annotate(x.p,(x.time_to_return, x.profit_of_investment),ha=&#39;center&#39;,va=&#39;center&#39;) plt.xlabel(&#39;Time to return&#39;) plt.ylabel(&#39;Profit from investment %&#39;) plt.tight_layout() . Portfolios by time to return and total investment . fig,ax = plt.subplots(1,2,figsize=(30,10)) sns.regplot(portfolio_results[&#39;time_to_return&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[0]) sns.regplot(portfolio_results[&#39;time_to_return_optimized&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[1]) for x in portfolio_results.itertuples(): ax[0].annotate(x.p,(x.time_to_return, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[1].annotate(x.p,(x.time_to_return_optimized, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[0].set_xlabel(&#39;Time to return&#39;) ax[0].set_ylabel(&#39;Total Investment&#39;) ax[0].set_title(&#39;Original Prices&#39;) ax[1].set_xlabel(&#39;Time to return&#39;) ax[1].set_ylabel(&#39;Total Investment&#39;) ax[1].set_title(&#39;Optimized Prices&#39;) plt.tight_layout() . Huge return time . Most probaby because there are some properties with just few days per year availability . portfolio_results.sort_values(by=&#39;time_to_return_optimized&#39;,ascending=False)[:3] . p estimated_return_per_year estimated_property_price estimated_optimized_return_per_year mdf_id estimated_property_price_M estimated_return_per_year_M estimated_optimized_return_per_year_M time_to_return time_to_return_optimized profit profit_of_investment profit_M . 79 79 | 21630.442249 | 9.866244e+06 | 25863.355133 | 6 | $ 9.8662 | $ 0.0216 | $ 0.0259 | 456.127712 | 381.475802 | 1.930749e+06 | 0.195692 | $ 1.9307 | . 72 72 | 40881.997953 | 9.842167e+06 | 40613.729134 | 8 | $ 9.8422 | $ 0.0409 | $ 0.0406 | 240.745749 | 242.335965 | -6.458458e+04 | -0.006562 | $ -0.0646 | . 21 21 | 36208.247995 | 9.991117e+06 | 42171.958689 | 7 | $ 9.9911 | $ 0.0362 | $ 0.0422 | 275.934828 | 236.913745 | 1.645595e+06 | 0.164706 | $ 1.6456 | . Portfolios by yearly return and total investment . fig,ax = plt.subplots(1,2,figsize=(30,10)) sns.regplot(portfolio_results[&#39;estimated_return_per_year&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[0]) sns.regplot(portfolio_results[&#39;estimated_optimized_return_per_year&#39;],portfolio_results[&#39;estimated_property_price&#39;],ax=ax[1]) for x in portfolio_results.itertuples(): ax[0].annotate(x.p,(x.estimated_return_per_year, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[1].annotate(x.p,(x.estimated_optimized_return_per_year, x.estimated_property_price),ha=&#39;center&#39;,va=&#39;center&#39;) ax[0].set_xlabel(&#39;Return per Year&#39;) ax[0].set_ylabel(&#39;Total Investment&#39;) ax[0].set_title(&#39;Original Prices&#39;) ax[1].set_xlabel(&#39;Return per Year&#39;) ax[1].set_ylabel(&#39;Total Investment&#39;) ax[1].set_title(&#39;Optimized Prices&#39;) plt.tight_layout() . Next Iterations: . Find External Data for properties prices | Create a class for simulation maybe more models | . | Find a way to messure the predicted prices, if possible | Maybe try binomial distribution to estimate number of nights per listing | Fill the missing gaps: more features for pricing model | more ways to estimate the number of nights per year | add risk rates | . | . Sources: . bpostance/training.data_science | dino-rodriguez/optibnb | CarolineBarret/Modelling-Airbnb-Prices | .",
            "url": "https://cristianexer.github.io/blog/2020/10/09/AirbnbPricingOptimisationV1.html",
            "relUrl": "/2020/10/09/AirbnbPricingOptimisationV1.html",
            "date": " • Oct 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Daniel Cristian Fat . Agnostic data scientist with prior background in software development, currently working in the insurance sector and delivering high-performance data-driven solutions. . Experience . Data Scientist at Beazley Group 2021 - present | Junior Data Scientist at Beazley Group 2020 - 2021 | Software Developer at Accommodation.co.uk 2018 - 2019 | . Education . MSc Computer Science - University of Lincoln | BSc Computer Science - 1st Class - “1 Decembrie 1918” University of Alba Iulia | .",
          "url": "https://cristianexer.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cristianexer.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}